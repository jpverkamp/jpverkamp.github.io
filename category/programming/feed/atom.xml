<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>jverkamp.com</title><link href="http://blog.jverkamp.com" /><link rel="self" href="http://blog.jverkamp.com/feed/" /><updated>2015-12-01T00:00:00Z</updated><author><name>JP Verkamp</name></author><id>urn:uuid:bdbdd0f8-f9c2-fda3-168c-52092e959085</id><entry><title>Iterating the GitHub API (For users sans MFA)</title><link href="http://blog.jverkamp.com/2015/12/01/iterating-the-github-api" /><id>urn:uuid:0bd52a91-eb2f-62af-b7a6-425164f823a7</id><updated>2015-12-01T00:00:00Z</updated><summary type="html"><![CDATA[<p>Today I found myself auditing an organization's users to see which have <a href="https://en.wikipedia.org/wiki/multifactor_authentication">multifactor authentication</a> enabled<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>. Since we have a not insignificant number of users, I wanted to write a quick script to automate it. Down the rabbit hole I go... and now I have a clean way of iterating across paginated GitHub API responses.</p>
]]></summary><content type="html"><![CDATA[<p>Today I found myself auditing an organization's users to see which have <a href="https://en.wikipedia.org/wiki/multifactor_authentication">multifactor authentication</a> enabled<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>. Since we have a not insignificant number of users, I wanted to write a quick script to automate it. Down the rabbit hole I go... and now I have a clean way of iterating across paginated GitHub API responses.</p>
<!--more-->
<p>First, the full script:</p>
<pre class="python"><code>#!/usr/bin/env python3

import requests
import os

try:
    token = os.environ['GITHUB_TOKEN']
except:
    print('$GITHUB_TOKEN must be set with proper permission')
    sys.exit(0)

headers = {'Authorization': 'token {}'.format(token)}

def api_iterator(endpoint):
    url = 'https://api.github.com' + endpoint

    while True:
        response = requests.get(url, headers = headers)
        yield from response.json()

        if 'next' in response.links:
            url = response.links['next']['url']
        else:
            return</code></pre>
<p>The core of this script once again leans against the excellent <a href="http://docs.python-requests.org/en/latest/">Requests</a> library for Python. It makes making simple requests and parsing the <a href="http://www.w3.org/wiki/LinkHeader">HTTP Link Header</a> trivial. Also, <code>yield from</code> is pretty cool.</p>
<p>Basically, we use these instructions to <a href="https://help.github.com/articles/creating-an-access-token-for-command-line-use/">create a GitHub access token</a>. You'll need at least the organization rights, I don't have an exact list. Unfortunately, it doesn't look like there is a way to do this with a username, password, and MFA token. I tried a few variations but it kept claiming that I wasn't an owner of the organization. So it goes.</p>
<p>Now, if we wanted to use this for my original goal of finding users without MFA, you need the <code>/orgs/:organization/members</code> endpoint, with a specific filter:</p>
<pre class="python"><code>endpoint = '/orgs/{}/members?filter=2fa_disabled'.format(organization)
for user in api_iterator(endpoint):
    print(user['login'])</code></pre>
<p>Alternatively, you can use it just as easily to get all of your repositories (similar to what I did for my post on <a href="http://blog.jverkamp.com/2015/09/08/backing-up-github-repositories">backing up GitHub repositories</a>):</p>
<pre class="python"><code>for repo in api_iterator('/user/repos'):
    print(repo['url'])</code></pre>
<p>Cool. Hope it's helpful!</p>
<p>The full source for the MFA version is also available on GitHub: <a href="https://github.com/jpverkamp/small-projects/blob/master/blog/missing-mfa.py">missing-mfa.py</a></p>]]></content></entry><entry><title>Finding EC2 instances by tag</title><link href="http://blog.jverkamp.com/2015/10/30/finding-ec2-instances-by-tag" /><id>urn:uuid:dcd71490-1abf-0503-d116-70fb4eeb29cd</id><updated>2015-10-30T00:00:00Z</updated><summary type="html"><![CDATA[<p>Another script similar to my previous post about <a href="http://blog.jverkamp.com/2015/07/22/finding-aws-iam-users-by-access-key">Finding AWS IAM users by access key</a>. This time, we want to do much the same thing for EC2 instances by tag.</p>
]]></summary><content type="html"><![CDATA[<p>Another script similar to my previous post about <a href="http://blog.jverkamp.com/2015/07/22/finding-aws-iam-users-by-access-key">Finding AWS IAM users by access key</a>. This time, we want to do much the same thing for EC2 instances by tag.</p>
<!--more-->
<pre class="python"><code>#!/usr/bin/env python3

import argparse
import boto.ec2
import json
import re
import sys

parser = argparse.ArgumentParser(description = 'Find out information about EC2 instances')
parser.add_argument('--region', default = 'us-west-2', help = 'The AWS region to search')
parser.add_argument('--limit', type = int, default = 1, help = 'How many results to report, 0 will return all')
parser.add_argument('--output', default = 'private_ip_address', help = 'The output to fetch for each instance; json will output a json object will all of the outputs')
parser.add_argument('filters', nargs = argparse.REMAINDER, help = 'Regular expressions to apply to each tag')
args = parser.parse_args()

ec2 = boto.ec2.connect_to_region('us-west-2')

if not args.filters:
    raise Exception('You must specify at least one instance')

def include_instance(instance):
    if instance.state != 'running':
        return

    if not args.filters:
        return instance

    for filter in args.filters:
        for tag_key in instance.tags:
            tag_value = instance.tags[tag_key]
            tag = '{}={}'.format(tag_key, tag_value).lower()

            if re.search(filter, tag_value):
                return instance

def filter():
    for reservation in ec2.get_all_instances():
        for instance in reservation.instances:
            if include_instance(instance):
                yield instance

for i, instance in enumerate(filter(), 1):
    if args.output == 'json':
        print(json.dumps(instance.__dict__, default = str))
    else:
        print(getattr(instance, args.output))

    if args.limit and i &gt;= args.limit:
        break</code></pre>
<p>It's original goal was to get a single IP from a group of servers with a specific tag so that I could log in and poke around:</p>
<pre class="bash"><code>$ ssh &grave;ec2 prod-server&grave;</code></pre>
<p>There are a few other uses though, especially when combined with other tools such as <a href="https://stedolan.github.io/jq/"><code>jq</code></a>.</p>
<pre class="bash"><code>ec2 --limit 0 --output json | jq '.instance_type' | sort | uniq -c | sort -nr

  27 "t2.micro"
   5 "m3.medium"
   1 "c4.xlarge"</code></pre>
<p>That's neat.</p>
<p>Much as <code>who-iam</code> it's a bit slow, but it still works great.</p>
<p>If you'd like to download the script, it's available in my <a href="https://github.com/jpverkamp/dotfiles">dotfiles</a>: <a href="https://github.com/jpverkamp/dotfiles/blob/master/bin/ec2">ec2</a></p>]]></content></entry><entry><title>Takuzu solver</title><link href="http://blog.jverkamp.com/2015/10/29/takuzu-solver" /><id>urn:uuid:db1688ed-6c5a-fd87-ce8b-71c39075285b</id><updated>2015-10-29T00:00:00Z</updated><summary type="html"><![CDATA[<p>Based on a <a href="">/r/dailyprogrammer</a> puzzle: <a href="https://www.reddit.com/r/dailyprogrammer/comments/3pwf17/20151023_challenge_237_hard_takuzu_solver/">Takuzu solver</a>.</p>
<p>Basically, Takuzu is a logic puzzle similar to Sudoku. You are given a grid partially filled with 0s and 1s. You have to fill in the rest of the grid according to three simple rules:</p>
<ul>
    <li>You cannot have more than three of the same number in a line</li>
    <li>Each column must have an equal number of 0s and 1s<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span></li>
    <li>No two rows or no two columns can be identical</li>
</ul>
<p>Thus, if you have a puzzle like this:</p>
<pre class="text"><code>0.01.1
0....1
..00..
..00..
1....0
10.0.0</code></pre>
<p>One valid solution (most puzzles should have only a single valid answer, but that doesn't always seem to be the case):</p>
<pre class="text"><code>010101
001101
110010
010011
101100
101010</code></pre>
<p>Let's do it!</p>
]]></summary><content type="html"><![CDATA[<p>Based on a <a href="">/r/dailyprogrammer</a> puzzle: <a href="https://www.reddit.com/r/dailyprogrammer/comments/3pwf17/20151023_challenge_237_hard_takuzu_solver/">Takuzu solver</a>.</p>
<p>Basically, Takuzu is a logic puzzle similar to Sudoku. You are given a grid partially filled with 0s and 1s. You have to fill in the rest of the grid according to three simple rules:</p>
<ul>
    <li>You cannot have more than three of the same number in a line</li>
    <li>Each column must have an equal number of 0s and 1s<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span></li>
    <li>No two rows or no two columns can be identical</li>
</ul>
<p>Thus, if you have a puzzle like this:</p>
<pre class="text"><code>0.01.1
0....1
..00..
..00..
1....0
10.0.0</code></pre>
<p>One valid solution (most puzzles should have only a single valid answer, but that doesn't always seem to be the case):</p>
<pre class="text"><code>010101
001101
110010
010011
101100
101010</code></pre>
<p>Let's do it!</p>
<!--more-->
<p>First, we need some sort of structure to represent and read in a Takuzu board. I think I'm going to over-engineer a little bit here, since I think it will be helpful in the long run.</p>
<p>My basic idea is to represent it in a way without mutation, that is to say once I've created a board, that board will never change. That will make it easier to write a backtracking solution, since when we need to back up to a previous state, we just throw away any of the derivative boards.</p>
<p>Taking that a level further, let's represent the Takuzu board as a base board with an essentially a 'changelog' on top of it, storing any differences from the boards 'under' it. Something like this:</p>
<pre class="text"><code>....    ..1.    ....    ..1.
0.0. {- .... {- .... == 0.0.
..0.    ....    ....    ..0.
...1    ..1.    .0..    .011</code></pre>
<p>In that diagram, the board on the far left is the first layer, then the second is the next layer up, and the third is the final layer. The fourth image is the virtual board that any user of the program would actually see.</p>
<p>So how do we turn that into code?</p>
<pre class="python"><code>class Takuzu(object):

    def __init__(self, filename = None, parent = None):
        '''
        Represent a Takuzu puzzle (a grid of 0, 1, and .)

        If filename is set, load from file.
        If parent is set, extend that Takuzu puzzle.

        If neither or both is set, that is an error.
        '''

        if not (filename or parent) or (filename and parent):
            raise Exception('Set exactly one of filename and parent')

        self.size = 0
        self.tiles = collections.defaultdict(lambda : False)
        self.parent = False

        if parent:
            self.size = parent.size
            self.parent = parent

        elif filename:
            with open(filename, 'r') as fin:
                for row, line in enumerate(fin):
                    for col, char in enumerate(line.strip()):
                        if char in '01':
                            self.tiles[row, col] = char

                        self.size = col + 1

    def get(self, row = None, col = None):
        '''
        Access a tile in the current puzzle, return False for unset values

        If the current puzzle doesn't have a value set, recur to parents.
        If either row or col is set to None, return that entire row or column.
        If neither is set, return nested lists containing all current values.
        '''

        # Note: We need the ugly != None to correctly deal with row or col = 0.
        # Sometimes truthiness is annoying.

        if row != None and col != None:
            return self.tiles[row, col] or (self.parent and self.parent.get(row, col))
        elif row != None:
            return [self.get(row, col) for col in range(self.size)]
        elif col != None:
            return [self.get(row, col) for row in range(self.size)]
        else:
            return [
                [self.get(row, col) for col in range(self.size)]
                for row in range(self.size)
            ]

    def set(self, row, col, val):
        '''
        Create a child Takuzu object with the specific value set.

        If either row or column is set to None, fill any empty elements in that entire row
        with the given value.
        '''

        child = Takuzu(parent = self)

        if row != None and col != None:
            child.tiles[row, col] = val
        elif row != None:
            for col in range(self.size):
                if not child.get(row, col):
                    child.tiles[row, col] = val
        elif col != None:
            for row in range(self.size):
                if not child.get(row, col):
                    child.tiles[row, col] = val
        else:
            raise Exception('Must set at least one of row and column')

        return child</code></pre>
<p>The interesting bits here are the <code>get</code> and <code>set</code> methods. <code>get</code>, as mentioned, assumes the layered structure above. It will start on the topmost layer (the actual object the program has a reference to) and try to look up the given point. If that fails (we're using a <code>collections.defaultdict</code>, so every reference will either by <code>'0'</code>, <code>'1'</code>, or <code>False</code>), fall back to the next layer up (the <code>parent</code>) until we either find one or run out of <code>parent</code> nodes.</p>
<p>Similarly, <code>set</code> doesn't actually change the current Takuzu object. Instead, it creates a new object with the current one as its parent, setting the new value in the child. This means that any values that were previously set are transparently available in the child, but we can at any point backtrack so long as we keep a reference to the now parent object around.</p>
<p>In addition, I've gone ahead and built in a bit of functionality that I know we're going to need into <code>get</code> and <code>set</code>. In either case, if you only specify either the <code>row</code> or <code>col</code> and leave the other empty (<code>None</code>), then we will return that entire row or column (or <code>set</code> any empty values). That's easy enough to code and it has the advantage of making it easy to compare rows to one another (for the third requirement above).</p>
<p>Okay, up next, we'll probably want a few more helper functions in this class in order to actually tell when we've solved one of these puzzles so the algorithms we eventually write can actually terminate:</p>
<pre class="python"><code>class Takuzu(object):
    ...

    def __eq__(self, other):
        '''Check if two Takuzu puzzles are equal.'''

        for row in range(self.size):
            for col in range(self.size):
                if self.get(row, col) != other.get(row, col):
                    return False

        return True

    def __str__(self):
        '''Return a string representation the same as can be read from a file.'''

        out = ''

        for row in range(self.size):
            for col in range(self.size):
                out += str(self.get(row, col) or '.')
            out += '\n'

        return out

    def is_full(self):
        '''If all values have been filled in.'''

        for row in range(self.size):
            for col in range(self.size):
                if not self.get(row, col):
                    return False

        return True

    def is_valid(self):
        '''Test if the current is still possibly a valid solution. If it also is_full,
        the board is solved.'''

        # Cannot have three identical numbers in a line
        # Ignore unset pieces
        for row in range(self.size):
            for col in range(self.size):
                if not self.get(row, col):
                    continue

                if self.get(row - 1, col) == self.get(row, col) == self.get(row + 1, col):
                    return False

                if self.get(row, col - 1) == self.get(row, col) == self.get(row, col + 1):
                    return False

        # All rows and columns must have no more than the maximum (size/2) number of 0s or 1s
        for index in range(self.size):
            if (
                self.get(index, None).count('0') &gt; self.size / 2
                or self.get(index, None).count('1') &gt; self.size / 2
                or self.get(None, index).count('0') &gt; self.size / 2
                or self.get(None, index).count('1') &gt; self.size / 2
            ):
                return False

        # No two rows or columns can be equal (ignore rows/columns that contain unset values)
        # all(...) on a row or column will be true iff all values are set
        for first_index in range(self.size):
            for second_index in range(first_index):
                if (
                    all(self.get(first_index, None))
                    and all(self.get(None, first_index))
                    and (
                        self.get(first_index, None) == self.get(second_index, None)
                        or self.get(None, first_index) == self.get(None, second_index)
                    )
                ):
                    return False

        # Whee passed all three conditions!
        return True

    def is_solved(self):
        '''Return True iff this puzzle is solved.'''

        return self.is_full() and self.is_valid()</code></pre>
<p><code>__eq__</code> and <code>__str__</code> are 'magic' methods in Python that will define equality and converting this object to a string respectively. This will be nice to make sure we don't investigate the same board more than once later.</p>
<p>After that, we have <code>is_full</code>, <code>is_valid</code>, and <code>is_solved</code><span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>. In the first case, we're checking if a puzzle has everything filled in. That way we know if we can stop recurring one way or the other.</p>
<p><code>is_valid</code> is actually a relatively new method. Before that, I could only check if a puzzle <code>is_solved</code>, but this way we can eliminate entire branches of the search space much earlier. For example, as soon as a backtracking solution places the third <code>0</code> in a row, it can stop looking down that path since <code>is_valid</code> will return <code>False</code>. Finally, <code>is_solved</code>. It used to have most of the <code>is_valid</code> code, but once that method existed, a puzzle <code>is_solved</code> simply if both it <code>is_full</code> and it <code>is_valid</code>. Easy enough.</p>
<p>So how do we actually solve a puzzle with this?</p>
<p>Let's start with the simple (relatively speaking, since we've done it before) backtracking solution. Given everything that we have in the Takuzu class, the actual solver is actually really simple:</p>
<pre class="python"><code>def solve(takuzu):
    '''Solve a puzzle using backtracking (also a fall back for the human solver).'''

    queue = [takuzu]

    while queue:
        takuzu = queue.pop()

        # Solved, we're done!
        if takuzu.is_solved():
            return takuzu

        # If we don't have a valid solution, stop looking on this branch
        if not takuzu.is_valid():
            continue

        # Otherwise, find one empty spot and try both possiblities
        def enqueue():
            for row in range(takuzu.size):
                for col in range(takuzu.size):
                    if not takuzu.get(row, col):
                        for value in '01':
                            queue.insert(takuzu.set(row, col, value), 0)
                        return
        enqueue()

    return False</code></pre>
<p>Basically, we keep a stack of solutions, which will allow us to perform a <a href="https://en.wikipedia.org/wiki/depth-first_search">depth-first search</a><span class="footnote"><sup><a href="#footnote-3">[3]</a></sup></span>.</p>
<p>Basically, create a branch, trying first a <code>0</code> in the first empty spot. Looking down that path, if we find a solution, we're done. If we don't try a <code>1</code> instead. That's really it. And it's actually relatively fast.</p>
<p>Given the input:</p>
<pre class="test"><code>0..1.0
0.11..
......
......
1..1..
.....0</code></pre>
<p>We can solve it easily enough:</p>
<pre class="bash"><code>$ python3 takuzu.py --method backtracker sample_6x6.takuzu

010110
001101
110010
011001
100101
101010

Solved in 0.15 seconds</code></pre>
<p>Nice. (You can check out the <a href="https://github.com/jpverkamp/takuzu">full source</a> if you'd like to see how I'm handling the command line parameters along with a few other goodies.<span class="footnote"><sup><a href="#footnote-4">[4]</a></sup></span>)</p>
<p>Unfortunately, as puzzles get a bit larger, that runtime isn't so great:</p>
<pre class="bash"><code>$ python3 takuzu.py --method backtracker sample_8x8.takuzu

10011010
11001100
01100101
10110010
01011001
01001101
10100110
00110011

Solved in 47.03 seconds</code></pre>
<p>Yeah...</p>
<p>We can do better.</p>
<p>How about instead of trying to solve the puzzle like a computer (brute forcing it), let's apply some heuristics more like a human would solve the puzzle. For example, if we see a pair of 0s next to each other, we know the tile on either side of it is a 1 (likewise for a pair of 0s separated by a single space and said space):</p>
<pre class="python"><code>def __third_of_a_kind__(takuzu):
    '''If adding a value would make three in a row, add the other.'''

    for row in range(takuzu.size):
        for col in range(takuzu.size):
            if takuzu.get(row, col):
                continue

            for ((offset1_row, offset1_col), (offset2_row, offset2_col)) in [
                # Two already in a line
                (( 0,  1), ( 0,  2)),
                (( 0, -1), ( 0, -2)),
                (( 1,  0), ( 2,  0)),
                ((-1,  0), (-2,  0)),
                # Two with a hole in between
                (( 0,  1), ( 0, -1)),
                (( 1,  0), (-1,  0)),
            ]:

                val1 = takuzu.get(row + offset1_row, col + offset1_col)
                val2 = takuzu.get(row + offset2_row, col + offset2_col)

                if val1 and val2 and val1 == val2:
                    return takuzu.set(row, col, invert(val1))

    return False</code></pre>
<p>Basically, for each empty tile in the current puzzle, compare each of six pairs. Either those in the four cardinal directions or the pair on either side horizontally or vertically.</p>
<p>Next rule, let's look for rows where we already have the necessary number of 0s and only need 1s. We can just fill those out:</p>
<pre class="python"><code>def __fill_rows__(takuzu):
    '''If we can figure out how many 0s and 1s we need for each and any row/col needs
    only 0s or 1s, add them'''

    # Try to fill any rows that have all of the needed 0s/1s but not the other
    for index in range(takuzu.size):
        for row, col in [(index, None), (None, index)]:
            for value in '01':
                # Have enough of 'value'
                if takuzu.get(row, col).count(value) == takuzu.size / 2:
                    # Not enough of the other one
                    if takuzu.get(row, col).count(invert(value)) &lt; takuzu.size / 2:
                        return takuzu.set(row, col, invert(value))

    return False</code></pre>
<p>It's neat how short that code is.</p>
<p>Finally (for the moment at least), let's write a slightly more complicated method. This time, let's take a single row or column and generate all the possible ways we can fill it out. Then remove those that would either lead to a duplicate or three in a row. If we have only exactly one row left, then we're golden. That's our row:</p>
<pre class="python"><code>def __fill_by_duplicates__(takuzu):
    '''Fill a puzzle by checking if any rows/cols are near enough to done that only one
    possibility is left.'''

    def row_or_col(which, index):
        if which == 'row':
            return takuzu.get(index, None)
        else:
            return takuzu.get(None, index)

    for major in ('row', 'col'):
        # Completed rows/cols have no Nones (so are 'all')
        completed = filter(all, [
            row_or_col(major, index)
            for index in range(takuzu.size)
        ])

        for index in range(takuzu.size):
            current = row_or_col(major, index)

            # Already a complete row/col, skip it
            if all(current):
                continue

            # Generate all posibilities, removing any that we already see
            possibilities = [
                option
                for option in permute_nones(current)
                if (
                    option not in completed
                    and option.count('0') == takuzu.size / 2
                    and option.count('1') == takuzu.size / 2
                    and '000' not in ''.join(option)
                    and '111' not in ''.join(option)
                )
            ]

            # If we have exactly one, set that one
            if len(possibilities) == 1:
                for other_index, value in enumerate(possibilities[0]):
                    if major == 'row':
                        takuzu = takuzu.set(index, other_index, value)
                    else:
                        takuzu = takuzu.set(other_index, index, value)

                return takuzu

    # Never found one
    return False</code></pre>
<p>I'm not nearly as happy with this rule as I am with the other two. Mostly because of the difference between rows and columns, the code is a little strange. The core of it is the <code>permute_nones</code> helper, which will take a list containing some number of <code>None</code> entries and fill them each with either a <code>0</code> or <code>1</code>, generating all possibilities:</p>
<pre class="python"><code>def permute_nones(ls):
    '''Helper function to generate all permutations from filling in 0s and 1s into a list'''

    if ls == []:
        yield []
    elif ls[0]:
        for recur in permute_nones(ls[1:]):
            yield [ls[0]] + recur
    else:
        for value in '01':
            for recur in permute_nones(ls[1:]):
                yield [value] + recur</code></pre>
<p>And that's all of my rules from now. The basic algorithm will be to apply each of those three rules in turn (since even after one has stopped working, another may 'unblock it'). If we get to the point where none of those three rules work, fall back to the backtracker we discussed above:</p>
<pre class="python"><code>RULES = [
    __third_of_a_kind__,
    __fill_rows__,
    __fill_by_duplicates__,
    solvers.backtracker.solve,
]

def solve(takuzu):
    '''Solve a Takuzu puzzle much as a human would: by applying a series of logical rules.'''

    while True:
        updated = False

        # If we've already solved it, return
        if takuzu.is_solved():
            return takuzu

        # Try to apply each rule in turn; if any rule works start over
        for rule in RULES:
            next_takuzu = rule(takuzu)

            if next_takuzu:
                takuzu = next_takuzu
                updated = True
                break

        # If we didn't apply any rule this iteration, done trying
        if not updated:
            break

    return takuzu</code></pre>
<p>Since we don't have to use the much slower (runtime <span>\(O(2^n)\)</span>) backtracking solution for the entire puzzle, this should run significantly faster:</p>
<pre class="python"><code>$ python3 takuzu.py --method human sample_6x6.takuzu

010110
001101
110010
011001
100101
101010

Solved in 0.13 seconds

$ python3 takuzu.py --method human sample_8x8.takuzu

10011010
11001100
01100101
10110010
01011001
01001101
10100110
00110011

Solved in 37.23 seconds</code></pre>
<p>Okay. So 0.13 seconds isn't <em>that</em> much faster than 0.15 seconds, and 37.23 seconds is only a bit faster than 47.03 seconds. For easier puzzles (those where a human wouldn't have to guess), you'll get a better improvement. These were considered 'hard'.</p>
<p>We can do better though.</p>
<p>Right now, we fall back to a pure backtracking solution rather than the faster human rules if we ever fail to advance. What if we combined the two models? Use the human solution until you get stuck; then advance exactly one step with the backtracking model; then switch back to the human model. If that branch fails, you reset back to the backtracking step, the human steps can be skipped for this.</p>
<p>Let's try it:</p>
<pre class="python"><code>RULES = copy.copy(solvers.human.RULES)
RULES.remove(solvers.backtracker.solve)

def solve(takuzu):
    '''
    Solve a puzzle using a hybrid model.

    Start with the human solver.
    Every time you get stuck, guess at a spot.
    Switch back to the human solver (backtracking to step 2 on failures).
    '''

    queue = [takuzu]

    while queue:
        takuzu = queue.pop()

        # Solved, we're done!
        if takuzu.is_solved():
            return takuzu

        # If we don't have a valid solution, stop looking on this branch
        if not takuzu.is_valid():
            continue

        # Try to advance using the human rules until they all fail
        while True:
            updated = False
            for rule in RULES:
                next_takuzu = rule(takuzu)

                if next_takuzu:
                    takuzu = next_takuzu
                    updated = True
                    break

            if not updated:
                break

        # Solved, we're done!
        if takuzu.is_solved():
            return takuzu

        # Once they've failed, find one empty spot and try both possibilities
        def enqueue():
            for row in range(takuzu.size):
                for col in range(takuzu.size):
                    if not takuzu.get(row, col):
                        for value in '01':
                            queue.append(takuzu.set(row, col, value))
                        return
        enqueue()

    return False</code></pre>
<p>So how much does that buy us?</p>
<pre class="python"><code>$ python3 takuzu.py --method hybrid sample_6x6.takuzu

010110
001101
110010
011001
100101
101010

Solved in 0.05 seconds

$ python3 takuzu.py --method hybrid sample_8x8.takuzu

10011010
11001100
01100101
10110010
01011001
01001101
10100110
00110011

Solved in 0.67 seconds</code></pre>
<p>Hah! That's more like it!</p>
<p>For now, that's it<span class="footnote"><sup><a href="#footnote-5">[5]</a></sup></span>. If you'd like to see how I structured the code (it's big enough to spread into multiple files), how I parsed command line parameters, or how I dynamically load the various solvers, you can see the entire code on GitHub: <a href="https://github.com/jpverkamp/takuzu">jpverkamp/takuzu</a>.</p>
<p>I like puzzles. Perhaps I'll try <a href="https://en.wikipedia.org/wiki/Suduko">Suduko</a> next. Or maybe <a href="https://en.wikipedia.org/wiki/Hashiwokakero">Hashi puzzles</a><span class="footnote"><sup><a href="#footnote-6">[6]</a></sup></span>. Onwards!<span class="footnote"><sup><a href="#footnote-7">[7]</a></sup></span></p>]]></content></entry><entry><title>Mandelbrot</title><link href="http://blog.jverkamp.com/2015/09/14/mandelbrot" /><id>urn:uuid:466b7de5-1cb6-2bfa-8b63-e1e10e296799</id><updated>2015-09-14T00:00:00Z</updated><summary type="html"><![CDATA[<p>Perhaps the best known fractal of all: the <a href="https://en.wikipedia.org/wiki/Mandelbrot_set">Mandelbrot set</a>.</p>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_default_400x300_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_default_400x300_hot-and-cold.png" /></a></p>
<p>Since I was already working on Python code that would render an image given a function (for a future post), I figured that I might as well render fractals with it.</p>
]]></summary><content type="html"><![CDATA[<p>Perhaps the best known fractal of all: the <a href="https://en.wikipedia.org/wiki/Mandelbrot_set">Mandelbrot set</a>.</p>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_default_400x300_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_default_400x300_hot-and-cold.png" /></a></p>
<p>Since I was already working on Python code that would render an image given a function (for a future post), I figured that I might as well render fractals with it.</p>
<!--more-->
<p>The basic idea is simple. Use <a href="https://python-pillow.github.io/">pillow</a> (the successor <a href="http://www.pythonware.com/products/pil/">PIL</a>), create an empty image of a given size. Then, call a given function for each point in that image, passing the <code>x</code> and <code>y</code> coordinates of the function as parameters. Basically, the <code><a href="http://docs.racket-lang.org/search/index.html?q=build-flomap*">build-flomap*</a></code> function I use all the time in Racket.</p>
<p>It turns out, that's actually really straight forward:</p>
<pre class="python"><code>def generate_image(width, height, generator):
    '''
    Generate an RGB image using a generator function.

    width, height -- the size of the generated image
    generator -- a function that takes (x, y) and returns (r, g, b)
    '''

    # Generate the data as a row-major list of (r, g, b)
    data = [generator(x, y) for y in range(height) for x in range(width)]

    # Pack that into a Pillow image and return it
    img = PIL.Image.new('RGB', (width, height))
    img.putdata(data)
    return img</code></pre>
<p>I like that you can have multiple <code>for</code> statements in a generator like that. It's very similiar to the different forms of Racket's <code><a href="http://docs.racket-lang.org/search/index.html?q=for">for</a></code><span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>, but I always seem to forget that I can do that in Python.</p>
<p>One downside of this is that it's relatively slow (at least on the big multi-core machines we have now). Luckily, we can use the <code><a href="https://docs.python.org/2/library/multiprocessing.html">multiprocessing</a></code> module to speed things up:</p>
<pre class="python"><code>def generate_image(width, height, generator, threads = 1):
    '''
    Generate an RGB image using a generator function.

    width, height -- the size of the generated image
    generator -- a function that takes (x, y) and returns (r, g, b)
    threads -- if != 1, use multiprocessing to spawn this many processes
    '''

    # Generate the data as a row-major list of (r, g, b)
    if threads == 1:
        data = [generator(x, y) for y in range(height) for x in range(width)]
    else:
        pool = multiprocessing.Pool(threads)
        data = pool.starmap(generator, [(x, y) for y in range(height) for x in range(width)])

    # Pack that into a Pillow image and return it
    img = PIL.Image.new('RGB', (width, height))
    img.putdata(data)
    return img</code></pre>
<p>By using <code>multiprocessing</code> rather than <code>threading</code>, we are actually spawning multiple Python processes, so we get a true multithreaded speedup. Since this program is almost entirely CPU bound, <code>threading</code> (with Python's <a href="https://en.wikipedia.org/wiki/global_interpreter lock">global interpreter lock</a>) wouldn't actually be any faster.</p>
<p>An aside: Using <code>starmap</code> allows us to pass multiple parameters to the function we are mapping over. This was only introduced in Python 2.6 / 3.3, so make sure you have a sufficiently new version<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>.</p>
<p>With that, we can make some pretty pictures like I'm sure I've shown off before<span class="footnote"><sup><a href="#footnote-3">[3]</a></sup></span>.</p>
<pre class="python"><code>generate_image(
    400,
    300,
    lambda x, y: (
        (x * y) % 256,
        (x + y) % 256,
        max(x, y) % 256
    )
).save('sample.png')</code></pre>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/sample.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/sample.png" /></a></p>
<p>Yes, I realize that's not the most Pythonic code in the world. And because the body of a Python <code>lambda</code> has to be an expression, you cannot write nearly as complicated functions as you could in Racket. It's perfectly valid though. :)</p>
<p>Okay, so we have a way to generate images, let's use it to generate Mandelbrot sets. The basic idea of the Mandelbrot set is surprisingly simple<span class="footnote"><sup><a href="#footnote-4">[4]</a></sup></span>:</p>
<p>Given a complex number <span>\( \mathbb{C} \)</span>, the <a href="https://en.wikipedia.org/wiki/complex_number">complex</a> <a href="https://en.wikipedia.org/wiki/quadratic_polynomial">quadratic polynomial</a>:</p>
<div>$$\mathbb{Z}_n+1 = \mathbb{Z}_n^2 + \mathbb{C}$$</div>
<p>Either does or does not escape to infinity. If the result remains bounded as <span>\( n \to \infty \)</span>, the number is part of the Mandelbrot set. If not, it's not. Because Python has built in support for complex numbers, this code is fairly elegant:</p>
<pre class="python"><code>def make_mandelbrot_generator(width, height, center, size, max_iterations = 256):
        '''
        A generator that makes generate_image compatible mandelbrot generators.

        width, height -- the size of the resulting image (used for scale)
        center -- the focus point of the image
        size -- the size of the larger dimension
        max_iterations -- the scale to check before exploding, used for coloring
        '''

        # Scale the size so that is the size of the larger dimension
        if width &gt;= height:
            size_x = size
            size_y = size * height / width
        else:
            size_x = size * width / height
            size_y = size

        # Convert to a bounding box
        min_x = center[0] - size_x / 2
        max_x = center[0] + size_x / 2
        min_y = center[1] - size_y / 2
        max_y = center[1] + size_y / 2

        def generator(x, y):
            # Scale to the mandlebrot frame; convert to a complex number
            x = (x / width) * (max_x - min_x) + min_x
            y = (y / height) * (max_y - min_y) + min_y
            c = x + y * 1j

            # Iterate until we escape to infinity or run out of iterations
            # For our purposes, we can consider infinity = 2
            z = 0
            for iteration in range(max_iterations):
                z = z * z + c

                # Size is r of polar coordinates
                (r, phi) = cmath.polar(z)
                if r &gt; 2:
                    break

            g = int(256 * iteration / max_iterations)
            return (g, g, g)

        return generator</code></pre>
<p>I've chosen here to make a function that returns the actual color generator primarily so that we would have access to the <code>width</code> and <code>height</code> within the main function.</p>
<p>Amusingly, it's been proven that if the magnitude of <span>\(\mathbb{Z}_n\)</span> crosses 2, it will go to infinity. Since <code>r</code> is the magnitude in the <a href="https://en.wikipedia.org/wiki/polar_coordinate system">polar coordinate system</a> (<code>r</code>,<code>ϕ</code>), we can use that as an escape hatch and even as a basic way to color the output.</p>
<p>One side note: using the <code>multiprocessing</code> module, we have to be able to <code><a href="https://docs.python.org/2/library/pickle.html">pickle</a></code> any variables to the function called. Functions defined in the global scope can be pickled, but functions used directly as parameters to other functions cannot; don't ask me why.</p>
<p>So if <code>threads</code> is not 1, this does not work:</p>
<pre class="python"><code>generate_image(
    400,
    300,
    make_mandelbrot_generator(400, 300, (-0.5, 0), 3),
    threads = 4
)</code></pre>
<p>But this does:</p>
<pre class="python"><code>generator = make_mandelbrot_generator(400, 300, (-0.5, 0), 3),
generate_image(400, 300, generator, threads = 4)</code></pre>
<p>Weird.</p>
<p>Anyways, what do we get when we try it out?</p>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_default_400x300_grayscale.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_default_400x300_grayscale.png" /></a></p>
<p>Beautiful<span class="footnote"><sup><a href="#footnote-5">[5]</a></sup></span>!</p>
<p>We need some color. Let's introduce one more parameter to the <code>make_mandelbrot_generator</code> function: <code>coloring</code>. Basically, a function that takes in a number in the range <code>[0, 1]</code> (which we're already computing; that is <code>iteration / max_iterations</code>) and return an RGB color. That way, we can have some more interesting colorations.</p>
<p>For example, the grayscale coloring function from earlier:</p>
<pre class="python"><code>def grayscale(v):
    '''Simple grayscale value.'''

    g = int(256 * v)
    return (g, g, g)</code></pre>
<p>Or how about instead, we render something in blue and red. Start at black, then fade up the blue channel, crossfade to red in the next third, and fade back to black in the last:</p>
<pre class="python"><code>def hot_and_cold(v):
    '''Scale from black to blue to red and back to black.'''

    r = g = b = 0

    if v &lt; 1/3:
        v = 3 * v
        b = int(256 * v)
    elif v &lt; 2/3:
        v = 3 * (v - 1/3)
        r = int(256 * v)
        b = int(256 * (1 - v))
    else:
        v = 3 * (v - 2/3)
        r = int(256 * (1 - v))

    return (r, g, b)</code></pre>
<p>Let's render that one instead:</p>
<pre class="python"><code>generator = make_mandelbrot_generator(400, 300, (-0.5, 0), 3),
generate_image(400, 300, generator, threads = 4, coloring = hot_and_cold)</code></pre>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_default_400x300_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_default_400x300_hot-and-cold.png" /></a></p>
<p>Excellent. We have a simple Mandelbrot generator. It's not exactly what I set out to do for this post (really only the <code>generate_image</code> function is), but I think it's pretty cool.</p>
<p>As a bonus round, I made something of a basic testing framework:</p>
<pre class="python"><code>THREAD_COUNT = max(1, multiprocessing.cpu_count() - 1)

SIZES = [
    (400, 300),
    (1920, 1080)
]

COLORINGS = [
    ('grayscale', grayscale),
    ('hot-and-cold', hot_and_cold),
]

IMAGES = [
    ('default', (-0.5, 0), 3),
    # http://www.nahee.com/Derbyshire/manguide.html
    ('seahorse-valley', (-0.75, 0.1), 0.05),
    ('triple-spiral-valley', (0.088, 0.654), 0.25),
    ('quad-spiral-valley', (0.274, 0.482), 0.005),
    ('double-scepter-valley', (-0.1, 0.8383), 0.005),
    ('mini-mandelbrot', (-1.75, 0), 0.1),

]

for width, height in SIZES:
    for image_name, center, size in IMAGES:
        for coloring_name, coloring in COLORINGS:
            filename = os.path.join('{width}x{height}', 'mandelbrot_{name}_{width}x{height}_{coloring}.png')
            filename = filename.format(
                name = image_name,
                width = width,
                height = height,
                coloring = coloring_name,
            )
            generator = make_mandelbrot_generator(width, height, center, size, coloring = coloring)

            start = time.time()
            img = generate_image(
                width,
                height,
                generator,
                threads = THREAD_COUNT
            )
            end = time.time()

            if not os.path.exists(os.path.dirname(filename)):
                os.makedirs(os.path.dirname(filename))
            img.save(filename)

            print('{} generated in {} seconds with {} threads'.format(
                filename,
                end - start,
                THREAD_COUNT
            ))</code></pre>
<p><code>multiprocessing.cpu_count() - 1</code> means that I leave one processor for other work (I was having issues with my computer freazing, <code>multiprocessing</code> is good at that). Other than that, generate a bunch of images and shove them into directories by size.</p>
<p>Here are a few examples from <a href="http://www.nahee.com/Derbyshire/manguide.html">nahee.com</a>:</p>
<h3>Seahorse Valley<h3>
<a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_seahorse-valley_400x300_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_seahorse-valley_400x300_hot-and-cold.png" /></a>

<h3>Double Scepter Valley</h3>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_double-scepter-valley_400x300_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_double-scepter-valley_400x300_hot-and-cold.png" /></a></p>
<h3>Triple Spiral Valley</h3>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_triple-spiral-valley_400x300_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_triple-spiral-valley_400x300_hot-and-cold.png" /></a></p>
<h3>Quad Spiral Valley</h3>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_quad-spiral-valley_400x300_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_quad-spiral-valley_400x300_hot-and-cold.png" /></a></p>
<h3>Mini Mandelbrot</h3>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_mini-mandelbrot_400x300_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_mini-mandelbrot_400x300_hot-and-cold.png" /></a></p>
<p>Or how about one nice large one (right click, save as):</p>
<p><a href="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_seahorse-valley_1920x1080_hot-and-cold.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/09/14/mandelbrot/mandelbrot_seahorse-valley_1920x1080_hot-and-cold.png" /></a></p>
<p>So much detail!</p>
<p>Enjoy!</p>]]></content></entry><entry><title>Backing up GitHub repositories</title><link href="http://blog.jverkamp.com/2015/09/08/backing-up-github-repositories" /><id>urn:uuid:1441231b-25d9-0022-7eb4-23a4966e0f50</id><updated>2015-09-08T00:00:00Z</updated><summary type="html"><![CDATA[<p>The newest chapter in my quest to collect entirely too much data / back up All The Things!: GitHub.</p>
<p>Basically, I want to back up all of my own personal GitHub repositories along with any from organizations that I am involved with. Strictly speaking, this is a little strange, since it's unlikely that GitHub is going anywhere soon and, if it does, we are likely to have fair warning. But still, it's nice to have a local copy just in case GitHub is down.</p>
]]></summary><content type="html"><![CDATA[<p>The newest chapter in my quest to collect entirely too much data / back up All The Things!: GitHub.</p>
<p>Basically, I want to back up all of my own personal GitHub repositories along with any from organizations that I am involved with. Strictly speaking, this is a little strange, since it's unlikely that GitHub is going anywhere soon and, if it does, we are likely to have fair warning. But still, it's nice to have a local copy just in case GitHub is down.</p>
<!--more-->
<p>The code is actually really straight forward. Most of the heavy lifting is being done by the <a href="https://github.com/copitux/python-github3">pygithub3</a> library. One caveat: it's a little strange to install. In my case, I had to first install <code>pygithub</code> and then <code>pygithub3</code> in order to satisfy a missing dependency (<a href="https://github.com/copitux/python-github3/issues/41">previously reported</a>).</p>
<p>After that, you can authenticate to GitHub, get a list of repositories, and download them all:</p>
<pre class="python"><code>#!/usr/bin/env python3

import pygithub3
import os

# Load a list of repos we don't want to download / update
ignored = set()
if os.path.exists('ignore.txt'):
    with open('ignore.txt', 'r') as fin:
        for line in fin:
            ignored.add(line.strip())

# Connect to github (if you have MFA, your password must be a token)
gh = pygithub3.Github(os.environ['GITHUB_USERNAME'], os.environ['GITHUB_PASSWORD'])

# Loop over repos for the specified user, this will include their organization's repos as well
for repo in gh.get_user().get_repos():

    remote_path = repo.ssh_url
    size = repo.size
    owner = repo.owner.login
    name = repo.name
    name_with_owner = '{owner}/{name}'.format(owner = owner, name = name)
    print(name_with_owner)

    # Check if the repo is in the ignore list
    if name in ignored or name_with_owner in ignored:
        print('... skipping')
        continue

    # Build up a list of commands that will be run for the given repo
    local_path = os.path.join('repos', owner, name)
    cmds = ['mkdir -p repos/{owner}; cd repos/{owner}'.format(owner = owner)]

    # Already exists, update it
    if os.path.exists(local_path):
        print('... updating')
        cmds += [
            'cd {name}'.format(name = name),
            'git pull --rebase --prune',                 # Update to the most recent master
            'git submodule update --init --recursive',   # Update submodules
        ]
    # Doesn't exist yet, clone it
    else:
        print('... cloning')
        cmds += [
            'git clone {url}'.format(url = remote_path), # Download a new clean copy using repo name as directory
            'cd {name}'.format(name = name),
            'git submodule update --init --recursive',   # Download and update submodules
        ]

    # Run each command specified above, bailing out if any failed (&&)
    cmds = ' && '.join(cmds)
    os.system(cmds)
    print()</code></pre>
<p>Basically, we're going to run a sequence of commands depending on if the repo was previously checked out or not. If it hasn't been (this is the first time), we want to run:</p>
<ul>
    <li><code>git clone {url}</code></li>
    <li><code>cd {name}</code></li>
    <li><code>git submodule update --init --recursive</code></li>
</ul>
<p>This will get the initial version and any submodules. I'm not sure that I'm going to keep the submodule code around, but in interest of keeping everything in a runnable state even if GitHub were to vanish tomorrow, it seemed a good idea.</p>
<p>Alternatively, if we want to update a previously cloned repo:</p>
<ul>
    <li><code>cd {name}</code></li>
    <li><code>git pull --rebase --prune</code></li>
    <li><code>git submodule update --init --recursive</code></li>
</ul>
<p>This is much the same, it just pulls any new code first.</p>
<p>And that's it. It took a little while to pull all of my repositories down (I have about 15 GB all told, counting various private repositories from work and my university days (which I still help to maintain)), but that's only necessary the first time. After that, it's much quicker.</p>]]></content></entry><entry><title>Adjacency Matrix Generator</title><link href="http://blog.jverkamp.com/2015/08/24/adjacency-matrix-generator" /><id>urn:uuid:136b3438-1761-5142-adc5-95a8b5586bca</id><updated>2015-08-24T00:00:00Z</updated><summary type="html"><![CDATA[<p>Been a while since I've actually tackled one of the <a href="http://blog.jverkamp.com/category/programming/by-source/daily-programmer">Daily Programmer</a> challenges, so let's try one out. From <a href="https://www.reddit.com/r/dailyprogrammer/comments/3h0uki/20150814_challenge_227_hard_adjacency_matrix/)">a week and a half ago</a>, we are challeneged to make an adjacency matrix generator, turning a graphical representation of a graph into an <a href="https://en.wikipedia.org/wiki/adjacency_matrix">adjacency matrix</a>.</p>
<p>Input:</p>
<pre class="text"><code>a-----b
|\   / \
| \ /   \
|  /     e
| / \   /
|/   \ /
c-----d</code></pre>
<p>Output:</p>
<pre class="text"><code>01110
10101
11010
10101
01010</code></pre>
]]></summary><content type="html"><![CDATA[<p>Been a while since I've actually tackled one of the <a href="http://blog.jverkamp.com/category/programming/by-source/daily-programmer">Daily Programmer</a> challenges, so let's try one out. From <a href="https://www.reddit.com/r/dailyprogrammer/comments/3h0uki/20150814_challenge_227_hard_adjacency_matrix/)">a week and a half ago</a>, we are challeneged to make an adjacency matrix generator, turning a graphical representation of a graph into an <a href="https://en.wikipedia.org/wiki/adjacency_matrix">adjacency matrix</a>.</p>
<p>Input:</p>
<pre class="text"><code>a-----b
|\   / \
| \ /   \
|  /     e
| / \   /
|/   \ /
c-----d</code></pre>
<p>Output:</p>
<pre class="text"><code>01110
10101
11010
10101
01010</code></pre>
<!--more-->
<p>Specifically, we're working under a few conditions:</p>
<ul>
    <li>Nodes<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span> will be represented by a lowercase letter <code>a</code> to <code>z</code> (there will never be more than 26 nodes)</li>
    <li>Edges are either horizontally, vertically, or diagonally at 45 degrees and will be represented by <code>|-/\</code></li>
    <li>All edges will have a node at each end</li>
    <li>Any two nodes have at most one edge between them</li>
    <li>If edges need to bend, a virtual node marked by a <code>#</code> will be inserted (see examples later)</li>
    <li>Edges can overlap; there will always be at least one directed edge adjacent to each node</li>
</ul>
<p>My general idea for a solution takes place in three parts:</p>
<ol>
    <li>Load the grid into a quickly accessible form</li>
    <li>From each node, try each of the 8 cardinal directions looking for an edge; on an edge follow it until you hit either:
        <ul>
            <li>Another node: record the edge</li>
            <li>A virtual node: recursively check that edge instead</li>
        </ul>
    </li>
    <li>Print out the edges found above</li>
</ol>
<p>Let's do it. First, read in the edges:</p>
<pre class="python"><code>from collections import defaultdict as ddict

# Points will be stored as (row, col) tuples
# The grid is a mapping of points to the character in that location
# Nodes nodes store their point for fast iteration
# The final solution will be a mapping of nodes to a set of other nodes adjacent to them
grid = ddict(lambda : ' ')
nodes = {}
adjacency = ddict(set)

# Read the rest of the grid
for row, line in enumerate(sys.stdin):
    for col, char in enumerate(line):

        pt = (row, col)
        grid[pt] = char

        if is_node(char):
            nodes[char] = (row, col)</code></pre>
<p>Straight forward. I love <code>defaultdict</code>, since it means we don't have to worry about looking for points off the edge of the grid. If it's out of bounds, it will just return empty space.</p>
<p>Next, the 'seeking' function, that will follow an edge until a node (either real or virtual):</p>
<pre class="python"><code># List of possible edges, ordered row, col, edge type
possible_edges = (
    (-1, -1, '\\'), (-1, 0, '|'), (-1, 1, '/'),
    ( 0, -1, '-'),                ( 0, 1, '-'),
    ( 1, -1, '/'),  ( 1, 0, '|'), ( 1, 1, '\\')
)

def neighbors(src_pt, previous_deltas = None):
    '''Given a point, yield any neighboring nodes'''

    src = grid[src_pt]
    row, col = src_pt

    for row_delta, col_delta, edge_type in possible_edges:

        dst_pt = (row + row_delta, col + col_delta)
        dst = grid[dst_pt]

        # Don't go back the way we came
        if (-row_delta, -col_delta) == previous_deltas:
            continue

        # A valid leaving edge, follow it until a node or a #
        elif dst == edge_type:
            for i in naturals(2):
                dst_pt = (row + i * row_delta, col + i * col_delta)
                dst = grid[dst_pt]

                # Found the target node, add
                if is_node(dst):
                    yield dst
                    break

                # Found a connector, continue on the other exit point
                elif dst == '#':
                    yield from neighbors(dst_pt, (row_delta, col_delta))
                    break</code></pre>
<p>Basically, we have eight <code>possible_edges</code>, each of which has a delta along the row and column, along with the character that has to start and end it (the rest could be under something else, we don't really care, since we're making the assumption that the input is well formed).</p>
<p>I did use a couple of helper functions here. They should be fairly obvious:</p>
<pre class="python"><code>def is_node(c):
    return c in 'abcdefghijklmnopqrstuvwxyz'

def is_edge(c):
    return c in '|-/\\'

def naturals(i = 0):
    while True:
        yield i
            i += 1</code></pre>
<p>That includes one trick I've picked up from my <a href="http://blog.jverkamp.com/category/programming/by-language/racket">Racket</a> posts. <code>naturals</code> is an infinite list (a generator) of all natural numbers, starting at the given point. It's useful in this case since we don't know how far we're going to run along the edge, just that it will end eventually.</p>
<p>Finally, we need functions to iterate over all of the nodes as starting points and then to print out the results:</p>
<pre class="python"><code># Start at each node and expand all edges
# Note: This will find each edge twice, so it goes
for (src, pt) in nodes.items():
    for dst in neighbors(pt):
        adjacency[src].add(dst)
        adjacency[dst].add(src)

# Print an adjacency matrix in sorted node order
for src in sorted(nodes):
    for dst in sorted(nodes):
        if dst in adjacency[src]:
            sys.stdout.write('1')
        else:
            sys.stdout.write('0')
    sys.stdout.write('\n')</code></pre>
<p>As noted, this will find each edge twice, but so it goes. To account for that, we would have to use up a bit more memory storing the state of where we've been. As it is, we have a mostly functional solution, which I like.</p>
<p>And that's it. Let's see a few of the test cases from the <a href="https://www.reddit.com/r/dailyprogrammer/comments/3h0uki/20150814_challenge_227_hard_adjacency_matrix/)">original post</a>:</p>
<pre class="bash"><code>$ cat 1.matrix; python3 matrix-reader.py &lt; 1.matrix

7
a-----b
|\   / \
| \ /   \
|  /     e
| / \   /
|/   \ /
c-----d

01110
10101
11010
10101
01010

$ cat 3.matrix; python3 matrix-reader.py &lt; 3.matrix

7
a  b--c
|    /
|   /
d  e--f
 \    |
  \   |
g--h--#

00010000
00100000
01001000
10000001
00100100
00001001
00000001
00010110

9
   #--#
   | /        #
   |a--------/-\-#
  #--\-c----d   /
   \  \|     \ / \
   |\  b      #   #
   | #  \        /
   |/    #------#
   #

0111
1011
1101
1110</code></pre>
<p>Shiny!</p>
<p>I wonder how hard it would be to program the inverse: take an adjacency matrix as input and generate one of these graphical matrices as output? Even better, generate an 'optimal' graphic, with the smallest possible area.</p>
<p>We'll see.</p>
<p>The full code is available on GitHub: <a href="https://github.com/jpverkamp/small-projects/blob/master/blog/matrix-reader.py">matrix-reader.py</a></p>]]></content></entry><entry><title>Setting up Postfix and OpenDKIM</title><link href="http://blog.jverkamp.com/2015/08/10/setting-up-postfix-and-opendkim" /><id>urn:uuid:65ecefd0-9ca8-1ba1-d32a-ae3857191285</id><updated>2015-08-10T00:00:00Z</updated><summary type="html"><![CDATA[<p>Last week, I was presented with a fairly interesting challenge: add DKIM (via <a href="http://www.opendkim.org/">OpenDKIM</a>) support to our mail servers (running <a href="http://www.postfix.org/">Postfix</a>). Given that I've never actually worked on a mail server before, it sounded fun. <img alt="smile" class="emoji" src="/emoji/smile.svg" /></p>
]]></summary><content type="html"><![CDATA[<p>Last week, I was presented with a fairly interesting challenge: add DKIM (via <a href="http://www.opendkim.org/">OpenDKIM</a>) support to our mail servers (running <a href="http://www.postfix.org/">Postfix</a>). Given that I've never actually worked on a mail server before, it sounded fun. <img alt="smile" class="emoji" src="/emoji/smile.svg" /></p>
<!--more-->
<p>First, a bit of background on what exactly DKIM is:</p>
<blockquote>
    DomainKeys Identified Mail (DKIM) is an email validation system designed to detect email spoofing by providing a mechanism to allow receiving mail exchangers to check that incoming mail from a domain is authorized by that domain's administrators and that the email (including attachments) has not been modified during transport. A digital signature included with the message can be validated by the recipient using the signer's public key published in the DNS.
    -- Wikipedia: <a href="https://en.wikipedia.org/wiki/DKIM">DKIM</a>
</blockquote>
<p>Sounds nice. So what in the world does that mean?</p>
<h2>A bit of background</h2>
<p>Starting in the details, we have <a href="https://en.wikipedia.org/wiki/public_key cryptography">public key cryptography</a>. The basic idea of public key cryptography is that you have some sort of algorithm with two keys: one of which can be used to encrypt things and can be made public and another separate piece of information which can be used to decrypt things and should remain private. That way, you can publish your public key, well, publically. Then anyone that wants to send you a message can do so, knowing that only you (since only you possess the private key) can read it.</p>
<p>If we take this a step further, we can swap the roles of the public and private key<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>. Instead of encrypting with the public key, we will use the private key, requiring the <em>public</em> key for decryption. This sounds mad, since the public key is, by definition public. So what's the point of a message that only you can write but anyone can read?</p>
<p>Well, that's exactly the point: <em>only you</em> can write it. This is what's called a digital signature. Since only your private key could have encoded the message and since only you have the private key, this allows anyone to read your message and be safe in the knowledge that you wrote it.</p>
<p>This is exactly what DKIM does.</p>
<p>By creating an public/private key pair, publishing the public key to a DNS record, and using the private key to sign messages, you are allowing any receiver to verify that <em>you</em> were person who sent them. Don't get me wrong, there are still a few problems with this approach (we'll get to them later), but it's a cool idea.</p>
<h2>Implementation</h2>
<p>So how do you actually implement it in practice?</p>
<p>Given that I don't have any particular previous experience with mail servers, the answer involves a lot of Google. Here are the steps that worked for me.</p>
<h4>1 - Have a previously configured server correctly delivering mail via <a href="http://www.postfix.org/">Postfix</a></h4>
<h4>2 - Install <a href="http://www.opendkim.org/">OpenDKIM</a></h4>
<pre class="bash"><code>sudo apt-get install -y opendkim opendkim-tools</code></pre>
<h4>3 - Generate a new keypair</h4>
<pre class="bash"><code>sudo opendkim-genkey -s mail -d example.com</code></pre>
<p>The <code>-s</code> argument specifies a selector, which allows us to have multiple keypairs specified on the same domain (if we wanted to), while the <code>-d</code> is the domain we are signing for. In this case, we are generating a record for <code>mail._domainkey.example.com</code>.</p>
<p>This will generate two files. The first is <code>mail.private</code>, which we will move to <code>/etc/opendkim/keys/example.com.private</code> (to match the <code>KeyFile</code> later). Also, we need to make sure that the file has correctly narrow Unix permissions, or OpenDKIM will refuse to use it (for our own safety):</p>
<pre class="bash"><code>sudo chmod 0400 /etc/opendkim/keys/example.com.private
sudo chown opendkim:opendkim /etc/opendkim/keys/example.com.private
sudo adduser postfix opendkim</code></pre>
<p>The second file is <code>mail.txt</code>, which is a DNS record for the above domain. Set it up so this returns successfully:</p>
<pre class="bash"><code>$ dig mail._domainkey.example.com TXT

...
;; ANSWER SECTION:
mail._domainkey.example.com. 599	IN TXT "v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQC5N3lnvvrYgPCRSoqn+awTpE+iGYcKBPpo8HHbcFfCIIV10Hwo4PhCoGZSaKVHOjDm4yefKXhQjM7iKzEPuBatE7O47hAx1CJpNuIdLxhILSbEmbMxJrJAG0HZVn8z6EAoOHZNaPHmK2h4UUrjOG8zA5BHfzJf7tGwI+K619fFUwIDAQAB" ; ----- DKIM key mail for example.com
...</code></pre>
<p>That's (almost) all we have to do to configure the public half of the key pair. (We still have to use the same selector later in the <code>KeyFile</code>, but that's it.)</p>
<h4>4 - Create an OpenDKIM configuration file (<code>/etc/opendkim.conf</code>)</h4>
<pre class="bash"><code>Canonicalization        relaxed/relaxed
ExternalIgnoreList      refile:/etc/opendkim/TrustedHosts
InternalHosts           refile:/etc/opendkim/TrustedHosts
KeyTable                refile:/etc/opendkim/KeyTable
LogWhy                  Yes
MinimumKeyBits          1024
Mode                    sv
PidFile                 /var/run/opendkim/opendkim.pid
SigningTable            refile:/etc/opendkim/SigningTable
Socket                  inet:8891@localhost
Syslog                  Yes
SyslogSuccess           Yes
UMask                   022
UserID                  opendkim:opendkim</code></pre>
<p>What in the world do those all mean? Well, (based somewhat on documentation and somewhat on expirimentation):</p>
<ul>
    <li><code>Canonicalization</code> - controls whether further mail servers can edit the message contents with destroying the signature. <code>relaxed</code> allows more modification, <code>simple</code> is more strict. <code>relaxed</code> seems to be the more common option.</li>
    <li><code>ExternalIgnoreList</code> - hosts that we trust to send us mail; do not validate their DKIM signatures (if present). Specifying <code>refile</code> means that we can use wildcards.</li>
    <li><code>InternalHosts</code> - hosts that will relay mail through us; if we see an unsigned message coming from one of them, sign it before forwarding it along (This is important! I'll get to why in a bit)</li>
    <li><code>KeyTable</code> - a list of private keys we can use to sign messages (included later)</li>
    <li><code>LogWhy</code> - log errors to <code>/var/log/mail.log</code> (useful for debugging)</li>
    <li><code>MinimumKeyBits</code> - flag an error if we try to specify a private key shorter (less secure) than this</li>
    <li><code>Mode</code> - various options, <code>sv</code> is fairly common and means <code>s</code>ign outgoing messages and <code>v</code>erify incoming ones</li>
    <li><code>PidFile</code> - where to store the <a href="https://en.wikipedia.org/wiki/PID_file">PID file</a></li>
    <li><code>SigningTable</code> - specify which key from the <code>KeyTable</code> should be used for a given message</li>
    <li><code>Socket</code> - how Postfix and OpenDKIM communicate</li>
    <li><code>Syslog</code>, <code>SyslogSuccess</code> - log to <a href="https://en.wikipedia.org/wiki/syslog">syslog</a> as well as <code>mail.log</code>; log successes as well as failures</li>
    <li><code>UMask</code> - allows other Linux users (such as Postfix's) to talk to OpenDKIM</li>
    <li><code>UserID</code> - the user that OpenDKIM runs as</li>
</ul>
<p>Fairly straight forward.</p>
<h4>5 - Next, another copy of the socket definition in <code>/etc/default/opendkim</code> (I'm actually not sure why this one is necessary)</h4>
<pre class="bash"><code>SOCKET="inet:8891@localhost"</code></pre>
<p>Next, specify our keys in <code>/etc/opendkim/KeyTable</code>:</p>
<pre class="bash"><code>example.com example.com:mail:/etc/opendkim/keys/example.com.private</code></pre>
<p>The first entry is a name for the key. It can be anything and doesn't necessary have to match the domain, just so long as the corresponding entry in <code>SigningTable</code> matches. After that, you have the domain where your public key is hosted, the selector on that domain, and where the private key is locally located. So in the above example, <code>example.com:mail</code> corresponds to a key at <code>mail._domainkey.example.com</code>.</p>
<p>And what emails to sign in <code>/etc/opendkim/SigningTable</code>:</p>
<pre class="bash"><code>*@example.com example.com
*@*.example.com example.com
*@*.example.org example.com</code></pre>
<p>This one is a bit more complicated, since for my particular case, I had emails coming from two different domains, along with subdomains in both cases (the first field). They can (and in this case) all use the same key, since the second entry matches the value specified in <code>KeyTable</code>.</p>
<p>Note again: Since we're using wildcards here, we have to specify <code>refile</code> up above in <code>opendkim.conf</code>.</p>
<h4>6 - Next, <code>/etc/opendkim/TrustedHosts</code></h4>
<p>This file will be used to specify both incoming message we trust and outgoing message we will sign (although it doesn't have to do both).</p>
<pre class="bash"><code>127.0.0.1
10.77.0.0/16
example.com
*.example.com
*.example.org
*.*.example.org</code></pre>
<p>Entries here can be either IP addresses, <a href="https://en.wikipedia.org/wiki/CIDR">CIDR</a> style IP ranges, or hostnames (including wildcards, since this is a <code>refile</code>), all of which I use above. This actually took a bit to figure out, since originally I was signing email directly from the box (successfully), but when I attempted to actually send a signed email from the product, it didn't work (since the frontends relay mail to the mail servers).</p>
<h4>7 - Okay, that's enough to configure OpenDKIM. Next, we need to tell Postfix to talk to it</h4>
<p>This one is relatively straight forward as well. Just add a few lines to the bottom of <code>/etc/postfix/main.cf</code>:</p>
<pre class="bash"><code># DKIM
milter_default_action = accept
milter_protocol = 2
smtpd_milters = inet:localhost:8891
non_smtpd_milters = inet:localhost:8891</code></pre>
<p>Essentially, milter is a protocol Postfix uses for plugins. We want to configure all mail traffic (both from smptd and not) to go to OpenDKIM, via the port we specified (twice) earlier. Shiny.</p>
<h4>8 - Finally, restart both OpenDKIM and Postfix, so they can take advantage of their new settings</h4>
<pre class="bash"><code>sudo service opendkim restart
sudo service postfix restart</code></pre>
<p>This should only take a few seconds.</p>
<p>And that's it. We can send a test email:</p>
<pre class="bash"><code>echo "test email" | mail -s &lt;code&gt;hostname&lt;/code&gt; me@example.com</code></pre>
<p>That will send an email with the <code>hostname</code> of the current machine to the specified address, useful if you're working with multiple different machines and need to know which are up to date.</p>
<p>Check the message headers and you should see a block that looks like this:</p>
<pre class="bash"><code>DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=example.com; s=mail;
        t=1439063676; bh=RnRlt9pNo9HfghopMAD1V157IZOFVrE6piv9xdXYFNs=;
        h=To:Subject:Reply-To:From:Date;
        b=LekoRRQgOX97WUHP/ELtl/yhMzZsiCPr8kqaYUpZER5sYZ1dyAwgOKKvuI3mL3IMo
         4Y90NDGv+CHm2ZmAaGmFOgGnDPsDxLE+ptleVBP/cQny9grftwA8Emc3MKS6aJ/w5P
         E1bh2wFE8LiRTl/wcof6JL0MyeoR8R63FCKMgnA8=</code></pre>
<p>Bam.</p>
<h2>A few caveats</h2>
<p>So, what is DKIM actually used for?</p>
<p>The original claim is that it's an <em>email validation system designed to detect email spoofing</em>. Which is all well and good, but there's one big problem: adaptation.</p>
<p>Numbers are a little hard to come by, but one site that I found is builtWith trends: <a href="https://trends.builtwith.com/mx/DKIM">DKIM Usage Statistics</a>. Their reported coverage notes:</p>
<ul>
    <li>Quantcast Top 10k - 14 of 10,000</li>
    <li>Quantcast Top 100k - 104 of 100,000</li>
    <li>Quantcast Top Million - 585 of 865,105</li>
    <li>Entire Internet - 24,064 of 328,844,222</li>
</ul>
<p>That's less than 0.1% in any category. Not so great.</p>
<p>This is a problem, not because it means that mail servers aren't using it, but rather because there is little reason for mail <em>clients</em> to support it. It's easy enough to verify a DKIM signature if present, but they're so rarely present, that most will not go through that effort.</p>
<p>Furthmore, the header does not sign the message headers and (unless you are using SMTP over TLS), it is trivial to remove. If the DKIM header is not present, the message is trivial to modify with the recipient none the wiser. This could be offset--for some business models--by requiring messages to contain a DKIM header and rejecting those that don't.</p>
<p>Still, it's an interesting technology and there's no particular harm (other than a small amount of extra CPU effort to do the signing) in implementing it.</p>]]></content></entry><entry><title>Finding AWS IAM users by access key</title><link href="http://blog.jverkamp.com/2015/07/22/finding-aws-iam-users-by-access-key" /><id>urn:uuid:4b3770fb-d5b2-cf75-6c83-24a7895c99fa</id><updated>2015-07-22T00:00:00Z</updated><summary type="html"><![CDATA[<p>Every once in a while<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>, I find myself with an <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSGettingStartedGuide/AWSCredentials.html">AWS access key</a> and need to figure out who in the world it belongs to. Unfortunately, so far as I've been able to find, there's no way to directly do this in either the <a href="https://aws.amazon.com/console/">AWS console</a> or with the <a href="https://aws.amazon.com/cli/">AWS api</a>.</p>
]]></summary><content type="html"><![CDATA[<p>Every once in a while<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>, I find myself with an <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSGettingStartedGuide/AWSCredentials.html">AWS access key</a> and need to figure out who in the world it belongs to. Unfortunately, so far as I've been able to find, there's no way to directly do this in either the <a href="https://aws.amazon.com/console/">AWS console</a> or with the <a href="https://aws.amazon.com/cli/">AWS api</a>.</p>
<!--more-->
<p>Luckily, <a href="https://aws.amazon.com/cli/">boto</a>:</p>
<pre class="python"><code>#!/usr/bin/env python3

import boto.iam.connection
import pprint
import sys

if len(sys.argv) == 1:
    print('Usage: who-iam [access-key ...]')
    sys.exit(0)

conn = boto.iam.connection.IAMConnection()
users = conn.get_all_users()

for user in users['list_users_response']['list_users_result']['users']:
    keys = conn.get_all_access_keys(user['user_name'])
    for key in keys['list_access_keys_response']['list_access_keys_result']['access_key_metadata']:
        for target in sys.argv[1:]:
            if key['access_key_id'] == target:
                print(key['access_key_id'], user['user_name'])</code></pre>
<p>Check it out (keys changed on the off chance that actually matters):</p>
<pre class="bash"><code>$ who-iam AKIAIOSWISKB6EXAMPLE AKIAIOSGWISN7EXAMPLE
AKIAIOSWISKB6EXAMPLE luke
AKIAIOSGWISN7EXAMPLE han</code></pre>
<p>It's rather slow (since it has to make <code>O(n)</code> requests and doesn't short circuit), but this should be something you do rarely enough that it doesn't matter.</p>
<p>If you'd like to download the script, it's available in my <a href="https://github.com/jpverkamp/dotfiles">dotfiles</a>: <a href="https://github.com/jpverkamp/dotfiles/blob/master/bin/who-iam">who-iam</a></p>]]></content></entry><entry><title>Configuring Websockets behind an AWS ELB</title><link href="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb" /><id>urn:uuid:7dcd0d64-5bf4-e7c8-1c43-b21d96bd35e0</id><updated>2015-07-20T00:00:00Z</updated><summary type="html"><![CDATA[<p>Recently at work, we were trying to get an application that uses <a href="https://en.wikipedia.org/wiki/websockets">websockets</a> working on an <a href="https://aws.amazon.com/">AWS</a> instance behind an <a href="https://aws.amazon.com/elasticloadbalancing/">ELB (load balancer)</a> and <a href="http://nginx.org/">nginx</a> on the instance.</p>
<p>If you're either not using a secure connection or handling the cryptography on the instance (either in nginx or Flask), it works right out of the box. But if you want the ELB to handle TLS termination it doesn't work nearly as well... Luckily, after a bit of fiddling, I got it working.</p>
]]></summary><content type="html"><![CDATA[<p>Recently at work, we were trying to get an application that uses <a href="https://en.wikipedia.org/wiki/websockets">websockets</a> working on an <a href="https://aws.amazon.com/">AWS</a> instance behind an <a href="https://aws.amazon.com/elasticloadbalancing/">ELB (load balancer)</a> and <a href="http://nginx.org/">nginx</a> on the instance.</p>
<p>If you're either not using a secure connection or handling the cryptography on the instance (either in nginx or Flask), it works right out of the box. But if you want the ELB to handle TLS termination it doesn't work nearly as well... Luckily, after a bit of fiddling, I got it working.</p>
<!--more-->
<p>First, we have a basic application. For my purposes, I wrote a quick Websocket chat app: <a href="https://github.com/jpverkamp/ws-chat">ws-chat</a>. The particular implementation details aren't as important. We'll start with the nginx config file:</p>
<pre class="nginx"><code>upstream webserver {
    server 127.0.0.1:8000;
}

upstream wsserver {
    server 127.0.0.1:9000;
}

server {
    listen 80 proxy_protocol;

    location / {
        if ($http_x_forwarded_proto = "http") {
            return 301 https://$host$request_uri;
        }

        proxy_pass http://webserver;
    }

    location /ws/ {
        proxy_pass http://wsserver;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}</code></pre>
<p>Straight forward enough. We have two backend services: a <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/web-server.py">web server</a> running on port 8000 (a simple Flask server that just servers a single <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/templates/index.html">HTML page</a>) and the <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/ws-server.py">websocket backend</a> running on port 9000. Alternatively, these could be the same codebase. The important parts are that you allow the Websocket <code>upgrade</code> header to pass through to establish the connection and that you tell nginx to listen using the <code>proxy_protocol</code>, an extra header that passes through extra information:</p>
<pre class="text"><code>PROXY_STRING + single space + INET_PROTOCOL + single space + CLIENT_IP + single space + PROXY_IP + single space + CLIENT_PORT + single space + PROXY_PORT + "\r\n"</code></pre>
<p>This seems like it wouldn't be necessary, except that without <code>proxy_protocol</code> AWS ELBs seem to strip something important to the connection.</p>
<p>Next, we need to configure the load balancer. One complication here is that telling the load balancer to forward HTTPS traffic to HTTP will not work for the websockets. Instead, you have to configure it to forward TCP (SSL) to TCP. This will still work for HTTP/HTTPS traffic (as HTTP is just a specific protocol over TCP and HTTPS is just HTTP with a TLS layer), but it will also allow the non-HTTP websocket traffic to pass through successfully. Something like this:</p>
<p><a href="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb/configure-elb.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb/configure-elb.png" /></a></p>
<p>(Don't forget to set the certificate :))</p>
<p>Finally, you have to configure the ELB also to speak proxy protocol. This part is slightly more annoying, since (at least now), there's no way to configure this through the AWS console. You have to use the <a href="https://aws.amazon.com/cli/">AWS CLI</a>.</p>
<p>First, create the new policy (assuming you have an environment variable <code>ELB_NAME</code> defined):</p>
<pre class="bash"><code>aws elb create-load-balancer-policy \
    --load-balancer-name $ELB_NAME \
    --policy-name $ELB_NAME-proxy-protocol \
    --policy-type-name ProxyProtocolPolicyType \\
    --policy-attributes AttributeName=ProxyProtocol,AttributeValue=True</code></pre>
<p>Then, attach it to the load balancer. You will have to run this once for each port that the instance is listening on:</p>
<pre class="bash"><code>aws elb set-load-balancer-policies-for-backend-server \
    --load-balancer-name $ELB_NAME \
    --instance-port 80 \
    --policy-names $ELB_NAME-proxy-protocol</code></pre>
<p>Make sure that you're using <code>https://</code> for the web traffic and <code>wss://</code> for the websocket and you're golden. Encrypted websockets behind an AWS ELB. Now if only they would expose the proxy protocol options in the console...</p>]]></content></entry><entry><title>Automagically storing Python objects in Redis</title><link href="http://blog.jverkamp.com/2015/07/16/automagically-storing-python-objects-in-redis" /><id>urn:uuid:e83d21ba-8555-bfba-b05f-ceefc0808723</id><updated>2015-07-16T00:00:00Z</updated><summary type="html"><![CDATA[<p>When you're starting out on a simple web application, eventually<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span> you will reach the point where you need to store some form of persistant data. Basically, you have three options<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>:</p>
<ul>
    <li>Store the information in flat files on the file system</li>
    <li>Store the information in a database (<a href="https://www.mysql.com/">MySQL</a>, <a href="https://www.sqlite.org/">SQLite</a> etc)</li>
    <li>Store the information in a key/value store (<a href="https://www.mongodb.org/">mongoDB</a>, <a href="http://redis.io/">reddis</a>)</li>
</ul>
<p>There are all manner of pros and cons to each, in particular how easy they are to get started in, how well they fit the data you are using, and how well they will scale horizontally (adding more machines rather than bigger ones).</p>
]]></summary><content type="html"><![CDATA[<p>When you're starting out on a simple web application, eventually<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span> you will reach the point where you need to store some form of persistant data. Basically, you have three options<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>:</p>
<ul>
    <li>Store the information in flat files on the file system</li>
    <li>Store the information in a database (<a href="https://www.mysql.com/">MySQL</a>, <a href="https://www.sqlite.org/">SQLite</a> etc)</li>
    <li>Store the information in a key/value store (<a href="https://www.mongodb.org/">mongoDB</a>, <a href="http://redis.io/">reddis</a>)</li>
</ul>
<p>There are all manner of pros and cons to each, in particular how easy they are to get started in, how well they fit the data you are using, and how well they will scale horizontally (adding more machines rather than bigger ones).</p>
<!--more-->
<p>For the project that I was working on (I'll post about it eventually), I didn't have terribly many different kinds of data to store, so it would be easy enough to start with anything. I started with a simple file system backing, with one json file per object that I was storing. That worked well enough, but I didn't particularly care for having to write all of the code myself to join / find child objects. I wanted something a little more powerful.</p>
<p>Next, I considered using a database with an <a href="https://en.wikipedia.org/wiki/Object-relational_mapping">ORM</a> layer. That would let me define everything as Python objects and let the library handle all of the mappings to the actual database. That way, I could write a bare minimum of code for my actual models. Unfortunately, the data wasn't particularly well structured for a relational database, being more hierarchical in struture. It's entirely possible to represent hierarchical data in a relational database, it's just not what they are best suited for.</p>
<p>Finally, I came to Redis. I've used Redis in a few projects at work and come to really like it. It works great as a simple key/value store and even better when you start taking advatage of some of their other data structures. In particular, Redis hashes and lists map nicely to Python dicts and lists. So that's what I ended up doing: Writing a pair of base classes (<code>RedisDict</code> and <code>RedisList</code>) which to the programmer act just like a Python dict or list, but actually store all of their data transparently in Redis.</p>
<p>Let's get to it.</p>
<p>First, there is a bit of shared code that both <code>RedisDict</code> and <code>RedisList</code> share, which we can factor out into a base class for the two of them: <code>RedisObject</code>.</p>
<pre class="python"><code>import base64
import json
import redis
import os

class RedisObject(object):
    '''
    A base object backed by redis.
    Genrally, use RedisDict or RedisList rather than this directly.
    '''

    def __init__(self, id = None):
        '''Create or load a RedisObject.'''

        self.redis = redis.StrictRedis(host = 'redis', decode_responses = True)

        if id:
            self.id = id
        else:
            self.id = base64.urlsafe_b64encode(os.urandom(9)).decode('utf-8')

        if ':' not in self.id:
            self.id = self.__class__.__name__ + ':' + self.id

    def __bool__(self):
        '''Test if an object currently exists'''

        return self.redis.exists(self.id)

    def __eq__(self, other):
        '''Tests if two redis objects are equal (they have the same key)'''

        return self.id == other.id

    def __str__(self):
        '''Return this object as a string for testing purposes.'''

        return self.id

    def delete(self):
        '''Delete this object from redis'''

        self.redis.delete(self.id)

    @staticmethod
    def decode_value(type, value):
        '''Decode a value if it is non-None, otherwise, decode with no arguments.'''

        if value == None:
            return type()
        else:
            return type(value)

    @staticmethod
    def encode_value(value):
        '''Encode a value using json.dumps, with default = str'''

        return str(value)</code></pre>
<p>Most of that should be pretty straight forward. The basic idea is that any <code>RedisObject</code>, be it a <code>RedisDict</code> or a <code>RedisList</code> has an ID. This will be used as the key that Redis stores the object under. In particular, I've used a neat (in my opinion) trick to generate random alphanumeric identifiers:</p>
<pre class="python"><code>&gt;&gt;&gt; base64.urlsafe_b64encode(os.urandom(9)).decode('utf-8')
u'UQTfwq8XLZr6'</code></pre>
<p>Neat. Alternatively, if you want to use a specific value for an ID (such as a user's email address), you can just specify that instead. Next, the <code>__bool__</code> method will make a <code>RedisObject</code> 'truthy'. You can use Python's <code>if</code> to tell if an object acutally exists or not. Finally, <code>delete</code>. I wanted to use <code>__del__</code> originally, but that actually gets called when an object is garbage collected, which doesn't quite work for this usage<span class="footnote"><sup><a href="#footnote-3">[3]</a></sup></span></p>
<p>Finally, static helper functions <code>decode_value</code> and <code>encode_value</code>. These will be used in a bit, since Redis only stores strings. Thus a <code>RedisObject</code> stores the type of each value and needs to know how to read/write that in a systematic way. For that, I'm using Python's <code>json</code> encoding, falling back to <code>str</code> (and thus the <code>__str__</code> magic function on objects). This will deal nicely with most default Python objects and can be easily extended for all manner of more interesting ones if you'd like (I've done it for <code>RedisObject</code>s).</p>
<p>One oddity that you've probably noticed is that I've hard coded the Reids IP to connect to. I'm using <a href="https://github.com/docker/compose">docker-compose</a> to run my project, which sets up hostnames automagically within the various containers.</p>
<p>Next, <code>RedisDict</code>:</p>
<pre class="python"><code>import json
import redis

from lib.RedisObject import RedisObject

class RedisDict(RedisObject):
    '''An equivalent to dict where all keys/values are stored in Redis.'''

    def __init__(self, id = None, fields = {}, defaults = None):
        '''
        Create a new RedisObject
        id: If specified, use this as the redis ID, otherwise generate a random ID.
        fields: A map of field name to construtor used to read values from redis.
            Objects will be written with json.dumps with default = str, so override __str__ for custom objects.
            This should generally be set by the subobject's constructor.
        defaults: A map of field name to values to store when constructing the object.
        '''

        RedisObject.__init__(self, id)

        self.fields = fields

        if defaults:
            for key, val in defaults.items():
                self[key] = val

    def __getitem__(self, key):
        '''
        Load a field from this redis object.
        Keys that were not specified in self.fields will raise an exception.
        Keys that have not been set (either in defaults or __setitem__) will return the default for their type (if set)
        '''

        if key == 'id':
            return self.id

        if not key in self.fields:
            raise KeyError('{} not found in {}'.format(key, self))

        return RedisObject.decode_value(self.fields[key], self.redis.hget(self.id, key))

    def __setitem__(self, key, val):
        '''
        Store a value in this redis object.
        Keys that were not specified in self.fields will raise an exception.
        Keys will be stored with json.dumps with a default of str, so override __str__ for custom objects.
        '''

        if not key in self.fields:
            raise KeyError('{} not found in {}'.format(key, self))

        self.redis.hset(self.id, key, RedisObject.encode_value(val))

    def __iter__(self):
        '''Return (key, val) pairs for all values stored in this RedisDict.'''

        yield ('id', self.id.rsplit(':', 1)[-1])

        for key in self.fields:</code></pre>
<p>Basically, there are three interesting parts to this code: <code>__init__</code> stores the fields that this object has (and should be set by the constructors in subclasses) and can also be used as as a constructor for new objects. <code>__get/setitem__</code> will load/store items via Redis. Given the <code>encode/decode_value</code> functions in <code>RedisObject</code>, this is actually really straight forward.</p>
<p>So how would you use something like this?</p>
<p>Here's is most of the <code>User</code> class from the project I am working on:</p>
<pre class="python"><code>import bcrypt

import lib
import models
import utils

class User(lib.RedisDict):
    '''A user. Duh.'''

    def __init__(self, id = None, email = None, **defaults):

        # Use email as id, if specified
        if email:
            id = email
            defaults['email'] = email

        lib.RedisDict.__init__(
            self,
            id = email,
            fields = {
                'name': str,
                'email': str,
                'password': str,
                'friends': lib.RedisList.as_child(self, 'friends', models.User),
            },
            defaults = defaults
        )

    def __setitem__(self, key, val):
        '''Override the behavior if user is trying to change the password'''

        if key == 'password':
            val = bcrypt.hashpw(
                val.encode('utf-8'),
                bcrypt.gensalt()
            ).decode('utf-8')

        lib.RedisDict.__setitem__(self, key, val)

    def verifyPassword(self, testPassword):
        '''Verify if a given password is correct'''

        hashedTestPassword = bcrypt.hashpw(
            testPassword.encode('utf-8'),
            self['password'].encode('utf-8')
        ).decode('utf-8')

        return hashedTestPassword == self['password']</code></pre>
<p>A <code>User</code> will have four fields: a <code>name</code>, an <code>email</code>, a <code>password</code>, and a list of <code>friends</code> (we'll get to how that works in a bit). Then, I've added some custom code to automatically store passwords using <a href="https://en.wikipedia.org/wiki/bcrypt">bcrypt</a><span class="footnote"><sup><a href="#footnote-4">[4]</a></sup></span>. You can use it just like you would a <code>dict</code>:</p>
<pre class="python"><code>&gt;&gt;&gt; han = User(
...     name = 'Luke Skywalker',
...     email = 'luke@rebel-alliance.io',
...     password = 'TheForce',
... )
...

&gt;&gt;&gt; print(luke['name'])
Luke Skywalker

&gt;&gt;&gt; luke.verifyPassword('password')
False

&gt;&gt;&gt; luke.verifyPassword('TheForce')
True

&gt;&gt;&gt; han = User(
...     name = 'Han Solo',
...     email = 'han@rebel-alliance.io',
...     password = 'LetTheWookieWin',
... )
...

&gt;&gt;&gt; luke['friends'].append(han)

&gt;&gt;&gt; han['friends'].append(luke)

&gt;&gt;&gt; print(luke['friends'][0]['name'])
'Han Solo'</code></pre>
<p>Then we can go into the <code>redis-cli</code> to verify that everything saved correctly:</p>
<pre class="bash"><code>127.0.0.1:6379&gt; keys *
1) "User:han@rebel-alliance.io:friends"
2) "User:han@rebel-alliance.io"
3) "User:luke@rebel-alliance.io:friends"
4) "User:luke@rebel-alliance.io"

127.0.0.1:6379&gt; hgetall User:luke@rebel-alliance.io
1) "name"
2) "Luke Skywalker"
3) "email"
4) "luke@rebel-alliance.io"
5) "password"
6) "$2b$12$XQ1zDvl5PLS6g.K64H27xewPQMnkELa3LvzFSyay8p9kz0XXHVOFq"

127.0.0.1:6379&gt; lrange User:luke@rebel-alliance.io:friends 0 -1
1) "User:han@rebel-alliance.io"</code></pre>
<p>There are two entires for each, since technically the <code>friends</code> list is a <code>RedisList</code>. Originally, I was storing these as JSON encoded lists, but as they got larger, this started to get a little unweildy.</p>
<p>Another plus is that since all of the objects are backed by Redis, you get automatic persistance. Stop Python completely, start it back up, and you can just load the same objects again (remember, for these objects, I'm using the <code>email</code> as the ID):</p>
<pre class="python"><code>&gt;&gt;&gt; luke = User('luke@rebel-alliance.io')

&gt;&gt;&gt; print(luke['friends'][0]['name'])
'Han Solo'</code></pre>
<p>Very cool.</p>
<p>So, speaking of <code>RedisList</code>, how does that work? Mostly the same as <code>RedisDict</code> (although I had a few more functions to implement):</p>
<pre class="python"><code>import json
import redis

from lib.RedisObject import RedisObject

class RedisList(RedisObject):
    '''An equivalent to list where all items are stored in Redis.'''

    def __init__(self, id = None, item_type = str, items = None):
        '''
        Create a new RedisList
        id: If specified, use this as the redis ID, otherwise generate a random ID.
        item_type: The constructor to use when reading items from redis.
        values: Default values to store during construction.
        '''

        RedisObject.__init__(self, id)

        self.item_type = item_type

        if items:
            for item in items:
                self.append(item)

    @classmethod
    def as_child(cls, parent, tag, item_type):
        '''Alternative callable constructor that instead defines this as a child object'''

        def helper(_ = None):
            return cls(parent.id + ':' + tag, item_type)

        return helper

    def __getitem__(self, index):
        '''
        Load an item by index where index is either an int or a slice
        Warning: this is O(n))
        '''

        if isinstance(index, slice):
            if slice.step != 1:
                raise NotImplemented('Cannot specify a step to a RedisObject slice')

            return [
                RedisObject.decode_value(self.item_type, el)
                for el in self.redis.lrange(self.id, slice.start, slice.end)
            ]
        else:
            return RedisObject.decode_value(self.item_type, self.redis.lindex(self.id, index))

    def __setitem__(self, index, val):
        '''Update an item by index
        Warning: this is O(n)
        '''

        self.redis.lset(self.id, index, RedisObject.encode_value(val))

    def __len__(self):
        '''Return the size of the list.'''

        return self.redis.llen(self.id)

    def __delitem__(self, index):
        '''Delete an item from a RedisList by index. (warning: this is O(n))'''

        self.redis.lset(self.id, index, '__DELETED__')
        self.redis.lrem(self.id, 1, '__DELETED__')

    def __iter__(self):
        '''Iterate over all items in this list.'''

        for el in self.redis.lrange(self.id, 0, -1):
            yield RedisObject.decode_value(self.item_type, el)

    def lpop(self):
        '''Remove and return a value from the left (low) end of the list.'''

        return RedisObject.decode_value(self.item_type, self.redis.lpop(self.id))

    def rpop(self):
        '''Remove a value from the right (high) end of the list.'''

        return RedisObject.decode_value(self.item_type, self.redis.rpop(self.id))

    def lpush(self, val):
        '''Add an item to the left (low) end of the list.'''

        self.redis.lpush(self.id, RedisObject.encode_value(val))

    def rpush(self, val):
        '''Add an item to the right (high) end of the list.'''

        self.redis.rpush(self.id, RedisObject.encode_value(val))

    def append(self, val):
        self.rpush(val)</code></pre>
<p>Basically, I'm mapping a lot of the default Python <code>list</code> functionality to Redis lists and vice versa<span class="footnote"><sup><a href="#footnote-5">[5]</a></sup></span>. It's a little odd and some things aren't as efficient as I'd like (you only get <code>O(1)</code> access to the beginning and end of the list), but so it goes. It works great, as you saw in the <code>friends</code> example above.</p>
<p>The one interesting function is <code>as_child</code>. Since either a <code>RedisDict</code> or a <code>RedisList</code> expect a 'constructor-like' function as the data type, I need something that will correctly store a <code>RedisList</code> inside of a <code>RedisDict</code> while generating a human readable ID (with <code>:friends</code> appended in the example above). I love <a href="https://en.wikipedia.org/wiki/higher_order functions">higher order functions</a>.</p>
<p>And... that's it. Eventually, I think I'll look into publishing this as a library to <code>pip</code> or the like. But since I've never done that before and this post is already a little on the long sice, we'll leave that for another day. All of the code is included in the post, so you can copy and paste it into your project if you'd like to try it out before I publish it. Once I have, I'll edit this post.</p>]]></content></entry></feed>