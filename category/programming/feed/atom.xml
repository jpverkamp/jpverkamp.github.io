<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>jverkamp.com</title><link href="http://blog.jverkamp.com" /><link rel="self" href="http://blog.jverkamp.com/feed/" /><updated>2015-07-20T00:00:00Z</updated><author><name>JP Verkamp</name></author><id>urn:uuid:bdbdd0f8-f9c2-fda3-168c-52092e959085</id><entry><title>Configuring Websockets behind an AWS ELB</title><link href="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb" /><id>urn:uuid:7dcd0d64-5bf4-e7c8-1c43-b21d96bd35e0</id><updated>2015-07-20T00:00:00Z</updated><summary type="html"><![CDATA[<p>Recently at work, we were trying to get an application that uses <a href="https://en.wikipedia.org/wiki/websockets">websockets</a> working on an <a href="https://aws.amazon.com/">AWS</a> instance behind an <a href="https://aws.amazon.com/elasticloadbalancing/">ELB (load balancer)</a> and <a href="http://nginx.org/">nginx</a> on the instance.</p>
<p>If you're either not using a secure connection or handling the cryptography on the instance (either in nginx or Flask), it works right out of the box. But if you want the ELB to handle TLS termination it doesn't work nearly as well... Luckily, after a bit of fiddling, I got it working.</p>
]]></summary><content type="html"><![CDATA[<p>Recently at work, we were trying to get an application that uses <a href="https://en.wikipedia.org/wiki/websockets">websockets</a> working on an <a href="https://aws.amazon.com/">AWS</a> instance behind an <a href="https://aws.amazon.com/elasticloadbalancing/">ELB (load balancer)</a> and <a href="http://nginx.org/">nginx</a> on the instance.</p>
<p>If you're either not using a secure connection or handling the cryptography on the instance (either in nginx or Flask), it works right out of the box. But if you want the ELB to handle TLS termination it doesn't work nearly as well... Luckily, after a bit of fiddling, I got it working.</p>
<!--more-->
<p>First, we have a basic application. For my purposes, I wrote a quick Websocket chat app: <a href="https://github.com/jpverkamp/ws-chat">ws-chat</a>. The particular implementation details aren't as important. We'll start with the nginx config file:</p>
<pre class="nginx"><code>upstream webserver {
    server 127.0.0.1:8000;
}

upstream wsserver {
    server 127.0.0.1:9000;
}

server {
    listen 80 proxy_protocol;

    location / {
        if ($http_x_forwarded_proto = "http") {
            return 301 https://$host$request_uri;
        }

        proxy_pass http://webserver;
    }

    location /ws/ {
        proxy_pass http://wsserver;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}</code></pre>
<p>Straight forward enough. We have two backend services: a <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/web-server.py">web server</a> running on port 8000 (a simple Flask server that just servers a single <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/templates/index.html">HTML page</a>) and the <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/ws-server.py">websocket backend</a> running on port 9000. Alternatively, these could be the same codebase. The important parts are that you allow the Websocket <code>upgrade</code> header to pass through to establish the connection and that you tell nginx to listen using the <code>proxy_protocol</code>, an extra header that passes through extra information:</p>
<pre class="text"><code>PROXY_STRING + single space + INET_PROTOCOL + single space + CLIENT_IP + single space + PROXY_IP + single space + CLIENT_PORT + single space + PROXY_PORT + "\r\n"</code></pre>
<p>This seems like it wouldn't be necessary, except that without <code>proxy_protocol</code> AWS ELBs seem to strip something important to the connection.</p>
<p>Next, we need to configure the load balancer. One complication here is that telling the load balancer to forward HTTPS traffic to HTTP will not work for the websockets. Instead, you have to configure it to forward TCP (SSL) to TCP. This will still work for HTTP/HTTPS traffic (as HTTP is just a specific protocol over TCP and HTTPS is just HTTP with a TLS layer), but it will also allow the non-HTTP websocket traffic to pass through successfully. Something like this:</p>
<p><a href="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb/configure-elb.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb/configure-elb.png" /></a></p>
<p>(Don't forget to set the certificate :))</p>
<p>Finally, you have to configure the ELB also to speak proxy protocol. This part is slightly more annoying, since (at least now), there's no way to configure this through the AWS console. You have to use the <a href="https://aws.amazon.com/cli/">AWS CLI</a>.</p>
<p>First, create the new policy (assuming you have an environment variable <code>ELB_NAME</code> defined):</p>
<pre class="bash"><code>aws elb create-load-balancer-policy \
    --load-balancer-name $ELB_NAME \
    --policy-name $ELB_NAME-proxy-protocol \
    --policy-type-name ProxyProtocolPolicyType \\
    --policy-attributes AttributeName=ProxyProtocol,AttributeValue=True</code></pre>
<p>Then, attach it to the load balancer. You will have to run this once for each port that the instance is listening on:</p>
<pre class="bash"><code>aws elb set-load-balancer-policies-for-backend-server \
    --load-balancer-name $ELB_NAME \
    --instance-port 80 \
    --policy-names $ELB_NAME-proxy-protocol</code></pre>
<p>Make sure that you're using <code>https://</code> for the web traffic and <code>wss://</code> for the websocket and you're golden. Encrypted websockets behind an AWS ELB. Now if only they would expose the proxy protocol options in the console...</p>]]></content></entry><entry><title>Automagically storing Python objects in Redis</title><link href="http://blog.jverkamp.com/2015/07/16/automagically-storing-python-objects-in-redis" /><id>urn:uuid:e83d21ba-8555-bfba-b05f-ceefc0808723</id><updated>2015-07-16T00:00:00Z</updated><summary type="html"><![CDATA[<p>When you're starting out on a simple web application, eventually<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span> you will reach the point where you need to store some form of persistant data. Basically, you have three options<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>:</p>
<ul>
    <li>Store the information in flat files on the file system</li>
    <li>Store the information in a database (<a href="https://www.mysql.com/">MySQL</a>, <a href="https://www.sqlite.org/">SQLite</a> etc)</li>
    <li>Store the information in a key/value store (<a href="https://www.mongodb.org/">mongoDB</a>, <a href="http://redis.io/">reddis</a>)</li>
</ul>
<p>There are all manner of pros and cons to each, in particular how easy they are to get started in, how well they fit the data you are using, and how well they will scale horizontally (adding more machines rather than bigger ones).</p>
]]></summary><content type="html"><![CDATA[<p>When you're starting out on a simple web application, eventually<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span> you will reach the point where you need to store some form of persistant data. Basically, you have three options<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>:</p>
<ul>
    <li>Store the information in flat files on the file system</li>
    <li>Store the information in a database (<a href="https://www.mysql.com/">MySQL</a>, <a href="https://www.sqlite.org/">SQLite</a> etc)</li>
    <li>Store the information in a key/value store (<a href="https://www.mongodb.org/">mongoDB</a>, <a href="http://redis.io/">reddis</a>)</li>
</ul>
<p>There are all manner of pros and cons to each, in particular how easy they are to get started in, how well they fit the data you are using, and how well they will scale horizontally (adding more machines rather than bigger ones).</p>
<!--more-->
<p>For the project that I was working on (I'll post about it eventually), I didn't have terribly many different kinds of data to store, so it would be easy enough to start with anything. I started with a simple file system backing, with one json file per object that I was storing. That worked well enough, but I didn't particularly care for having to write all of the code myself to join / find child objects. I wanted something a little more powerful.</p>
<p>Next, I considered using a database with an <a href="https://en.wikipedia.org/wiki/Object-relational_mapping">ORM</a> layer. That would let me define everything as Python objects and let the library handle all of the mappings to the actual database. That way, I could write a bare minimum of code for my actual models. Unfortunately, the data wasn't particularly well structured for a relational database, being more hierarchical in struture. It's entirely possible to represent hierarchical data in a relational database, it's just not what they are best suited for.</p>
<p>Finally, I came to Redis. I've used Redis in a few projects at work and come to really like it. It works great as a simple key/value store and even better when you start taking advatage of some of their other data structures. In particular, Redis hashes and lists map nicely to Python dicts and lists. So that's what I ended up doing: Writing a pair of base classes (<code>RedisDict</code> and <code>RedisList</code>) which to the programmer act just like a Python dict or list, but actually store all of their data transparently in Redis.</p>
<p>Let's get to it.</p>
<p>First, there is a bit of shared code that both <code>RedisDict</code> and <code>RedisList</code> share, which we can factor out into a base class for the two of them: <code>RedisObject</code>.</p>
<pre class="python"><code>import base64
import json
import redis
import os

class RedisObject(object):
    '''
    A base object backed by redis.
    Genrally, use RedisDict or RedisList rather than this directly.
    '''

    def __init__(self, id = None):
        '''Create or load a RedisObject.'''

        self.redis = redis.StrictRedis(host = 'redis', decode_responses = True)

        if id:
            self.id = id
        else:
            self.id = base64.urlsafe_b64encode(os.urandom(9)).decode('utf-8')

        if ':' not in self.id:
            self.id = self.__class__.__name__ + ':' + self.id

    def __bool__(self):
        '''Test if an object currently exists'''

        return self.redis.exists(self.id)

    def __eq__(self, other):
        '''Tests if two redis objects are equal (they have the same key)'''

        return self.id == other.id

    def __str__(self):
        '''Return this object as a string for testing purposes.'''

        return self.id

    def delete(self):
        '''Delete this object from redis'''

        self.redis.delete(self.id)

    @staticmethod
    def decode_value(type, value):
        '''Decode a value if it is non-None, otherwise, decode with no arguments.'''

        if value == None:
            return type()
        else:
            return type(value)

    @staticmethod
    def encode_value(value):
        '''Encode a value using json.dumps, with default = str'''

        return str(value)</code></pre>
<p>Most of that should be pretty straight forward. The basic idea is that any <code>RedisObject</code>, be it a <code>RedisDict</code> or a <code>RedisList</code> has an ID. This will be used as the key that Redis stores the object under. In particular, I've used a neat (in my opinion) trick to generate random alphanumeric identifiers:</p>
<pre class="python"><code>&gt;&gt;&gt; base64.urlsafe_b64encode(os.urandom(9)).decode('utf-8')
u'UQTfwq8XLZr6'</code></pre>
<p>Neat. Alternatively, if you want to use a specific value for an ID (such as a user's email address), you can just specify that instead. Next, the <code>__bool__</code> method will make a <code>RedisObject</code> 'truthy'. You can use Python's <code>if</code> to tell if an object acutally exists or not. Finally, <code>delete</code>. I wanted to use <code>__del__</code> originally, but that actually gets called when an object is garbage collected, which doesn't quite work for this usage<span class="footnote"><sup><a href="#footnote-3">[3]</a></sup></span></p>
<p>Finally, static helper functions <code>decode_value</code> and <code>encode_value</code>. These will be used in a bit, since Redis only stores strings. Thus a <code>RedisObject</code> stores the type of each value and needs to know how to read/write that in a systematic way. For that, I'm using Python's <code>json</code> encoding, falling back to <code>str</code> (and thus the <code>__str__</code> magic function on objects). This will deal nicely with most default Python objects and can be easily extended for all manner of more interesting ones if you'd like (I've done it for <code>RedisObject</code>s).</p>
<p>One oddity that you've probably noticed is that I've hard coded the Reids IP to connect to. I'm using <a href="https://github.com/docker/compose">docker-compose</a> to run my project, which sets up hostnames automagically within the various containers.</p>
<p>Next, <code>RedisDict</code>:</p>
<pre class="python"><code>import json
import redis

from lib.RedisObject import RedisObject

class RedisDict(RedisObject):
    '''An equivalent to dict where all keys/values are stored in Redis.'''

    def __init__(self, id = None, fields = {}, defaults = None):
        '''
        Create a new RedisObject
        id: If specified, use this as the redis ID, otherwise generate a random ID.
        fields: A map of field name to construtor used to read values from redis.
            Objects will be written with json.dumps with default = str, so override __str__ for custom objects.
            This should generally be set by the subobject's constructor.
        defaults: A map of field name to values to store when constructing the object.
        '''

        RedisObject.__init__(self, id)

        self.fields = fields

        if defaults:
            for key, val in defaults.items():
                self[key] = val

    def __getitem__(self, key):
        '''
        Load a field from this redis object.
        Keys that were not specified in self.fields will raise an exception.
        Keys that have not been set (either in defaults or __setitem__) will return the default for their type (if set)
        '''

        if key == 'id':
            return self.id

        if not key in self.fields:
            raise KeyError('{} not found in {}'.format(key, self))

        return RedisObject.decode_value(self.fields[key], self.redis.hget(self.id, key))

    def __setitem__(self, key, val):
        '''
        Store a value in this redis object.
        Keys that were not specified in self.fields will raise an exception.
        Keys will be stored with json.dumps with a default of str, so override __str__ for custom objects.
        '''

        if not key in self.fields:
            raise KeyError('{} not found in {}'.format(key, self))

        self.redis.hset(self.id, key, RedisObject.encode_value(val))

    def __iter__(self):
        '''Return (key, val) pairs for all values stored in this RedisDict.'''

        yield ('id', self.id.rsplit(':', 1)[-1])

        for key in self.fields:</code></pre>
<p>Basically, there are three interesting parts to this code: <code>__init__</code> stores the fields that this object has (and should be set by the constructors in subclasses) and can also be used as as a constructor for new objects. <code>__get/setitem__</code> will load/store items via Redis. Given the <code>encode/decode_value</code> functions in <code>RedisObject</code>, this is actually really straight forward.</p>
<p>So how would you use something like this?</p>
<p>Here's is most of the <code>User</code> class from the project I am working on:</p>
<pre class="python"><code>import bcrypt

import lib
import models
import utils

class User(lib.RedisDict):
    '''A user. Duh.'''

    def __init__(self, id = None, email = None, **defaults):

        # Use email as id, if specified
        if email:
            id = email
            defaults['email'] = email

        lib.RedisDict.__init__(
            self,
            id = email,
            fields = {
                'name': str,
                'email': str,
                'password': str,
                'friends': lib.RedisList.as_child(self, 'friends', models.User),
            },
            defaults = defaults
        )

    def __setitem__(self, key, val):
        '''Override the behavior if user is trying to change the password'''

        if key == 'password':
            val = bcrypt.hashpw(
                val.encode('utf-8'),
                bcrypt.gensalt()
            ).decode('utf-8')

        lib.RedisDict.__setitem__(self, key, val)

    def verifyPassword(self, testPassword):
        '''Verify if a given password is correct'''

        hashedTestPassword = bcrypt.hashpw(
            testPassword.encode('utf-8'),
            self['password'].encode('utf-8')
        ).decode('utf-8')

        return hashedTestPassword == self['password']</code></pre>
<p>A <code>User</code> will have four fields: a <code>name</code>, an <code>email</code>, a <code>password</code>, and a list of <code>friends</code> (we'll get to how that works in a bit). Then, I've added some custom code to automatically store passwords using <a href="https://en.wikipedia.org/wiki/bcrypt">bcrypt</a><span class="footnote"><sup><a href="#footnote-4">[4]</a></sup></span>. You can use it just like you would a <code>dict</code>:</p>
<pre class="python"><code>&gt;&gt;&gt; han = User(
...     name = 'Luke Skywalker',
...     email = 'luke@rebel-alliance.io',
...     password = 'TheForce',
... )
...

&gt;&gt;&gt; print(luke['name'])
Luke Skywalker

&gt;&gt;&gt; luke.verifyPassword('password')
False

&gt;&gt;&gt; luke.verifyPassword('TheForce')
True

&gt;&gt;&gt; han = User(
...     name = 'Han Solo',
...     email = 'han@rebel-alliance.io',
...     password = 'LetTheWookieWin',
... )
...

&gt;&gt;&gt; luke['friends'].append(han)

&gt;&gt;&gt; han['friends'].append(luke)

&gt;&gt;&gt; print(luke['friends'][0]['name'])
'Han Solo'</code></pre>
<p>Then we can go into the <code>redis-cli</code> to verify that everything saved correctly:</p>
<pre class="bash"><code>127.0.0.1:6379&gt; keys *
1) "User:han@rebel-alliance.io:friends"
2) "User:han@rebel-alliance.io"
3) "User:luke@rebel-alliance.io:friends"
4) "User:luke@rebel-alliance.io"

127.0.0.1:6379&gt; hgetall User:luke@rebel-alliance.io
1) "name"
2) "Luke Skywalker"
3) "email"
4) "luke@rebel-alliance.io"
5) "password"
6) "$2b$12$XQ1zDvl5PLS6g.K64H27xewPQMnkELa3LvzFSyay8p9kz0XXHVOFq"

127.0.0.1:6379&gt; lrange User:luke@rebel-alliance.io:friends 0 -1
1) "User:han@rebel-alliance.io"</code></pre>
<p>There are two entires for each, since technically the <code>friends</code> list is a <code>RedisList</code>. Originally, I was storing these as JSON encoded lists, but as they got larger, this started to get a little unweildy.</p>
<p>Another plus is that since all of the objects are backed by Redis, you get automatic persistance. Stop Python completely, start it back up, and you can just load the same objects again (remember, for these objects, I'm using the <code>email</code> as the ID):</p>
<pre class="python"><code>&gt;&gt;&gt; luke = User('luke@rebel-alliance.io')

&gt;&gt;&gt; print(luke['friends'][0]['name'])
'Han Solo'</code></pre>
<p>Very cool.</p>
<p>So, speaking of <code>RedisList</code>, how does that work? Mostly the same as <code>RedisDict</code> (although I had a few more functions to implement):</p>
<pre class="python"><code>import json
import redis

from lib.RedisObject import RedisObject

class RedisList(RedisObject):
    '''An equivalent to list where all items are stored in Redis.'''

    def __init__(self, id = None, item_type = str, items = None):
        '''
        Create a new RedisList
        id: If specified, use this as the redis ID, otherwise generate a random ID.
        item_type: The constructor to use when reading items from redis.
        values: Default values to store during construction.
        '''

        RedisObject.__init__(self, id)

        self.item_type = item_type

        if items:
            for item in items:
                self.append(item)

    @classmethod
    def as_child(cls, parent, tag, item_type):
        '''Alternative callable constructor that instead defines this as a child object'''

        def helper(_ = None):
            return cls(parent.id + ':' + tag, item_type)

        return helper

    def __getitem__(self, index):
        '''
        Load an item by index where index is either an int or a slice
        Warning: this is O(n))
        '''

        if isinstance(index, slice):
            if slice.step != 1:
                raise NotImplemented('Cannot specify a step to a RedisObject slice')

            return [
                RedisObject.decode_value(self.item_type, el)
                for el in self.redis.lrange(self.id, slice.start, slice.end)
            ]
        else:
            return RedisObject.decode_value(self.item_type, self.redis.lindex(self.id, index))

    def __setitem__(self, index, val):
        '''Update an item by index
        Warning: this is O(n)
        '''

        self.redis.lset(self.id, index, RedisObject.encode_value(val))

    def __len__(self):
        '''Return the size of the list.'''

        return self.redis.llen(self.id)

    def __delitem__(self, index):
        '''Delete an item from a RedisList by index. (warning: this is O(n))'''

        self.redis.lset(self.id, index, '__DELETED__')
        self.redis.lrem(self.id, 1, '__DELETED__')

    def __iter__(self):
        '''Iterate over all items in this list.'''

        for el in self.redis.lrange(self.id, 0, -1):
            yield RedisObject.decode_value(self.item_type, el)

    def lpop(self):
        '''Remove and return a value from the left (low) end of the list.'''

        return RedisObject.decode_value(self.item_type, self.redis.lpop(self.id))

    def rpop(self):
        '''Remove a value from the right (high) end of the list.'''

        return RedisObject.decode_value(self.item_type, self.redis.rpop(self.id))

    def lpush(self, val):
        '''Add an item to the left (low) end of the list.'''

        self.redis.lpush(self.id, RedisObject.encode_value(val))

    def rpush(self, val):
        '''Add an item to the right (high) end of the list.'''

        self.redis.rpush(self.id, RedisObject.encode_value(val))

    def append(self, val):
        self.rpush(val)</code></pre>
<p>Basically, I'm mapping a lot of the default Python <code>list</code> functionality to Redis lists and vice versa<span class="footnote"><sup><a href="#footnote-5">[5]</a></sup></span>. It's a little odd and some things aren't as efficient as I'd like (you only get <code>O(1)</code> access to the beginning and end of the list), but so it goes. It works great, as you saw in the <code>friends</code> example above.</p>
<p>The one interesting function is <code>as_child</code>. Since either a <code>RedisDict</code> or a <code>RedisList</code> expect a 'constructor-like' function as the data type, I need something that will correctly store a <code>RedisList</code> inside of a <code>RedisDict</code> while generating a human readable ID (with <code>:friends</code> appended in the example above). I love <a href="https://en.wikipedia.org/wiki/higher_order functions">higher order functions</a>.</p>
<p>And... that's it. Eventually, I think I'll look into publishing this as a library to <code>pip</code> or the like. But since I've never done that before and this post is already a little on the long sice, we'll leave that for another day. All of the code is included in the post, so you can copy and paste it into your project if you'd like to try it out before I publish it. Once I have, I'll edit this post.</p>]]></content></entry><entry><title>Backing up Moves Data</title><link href="http://blog.jverkamp.com/2015/07/10/backing-up-moves-data" /><id>urn:uuid:2eed2118-1b61-a7ab-1b50-35fe8d9c2baa</id><updated>2015-07-10T00:00:00Z</updated><summary type="html"><![CDATA[<p>Another <a href="http://blog.jverkamp.com/category/programming/by-topic/backups">backup post</a>, this time I'm going to back up my data from the <a href="https://www.moves-app.com/">Moves App</a> (step counter + GPS tracker). Theoretically, it should be possible to get this same data from the app as part of my <a href="http://blog.jverkamp.com/category/programming/by-project/ios-backup">iOS Backup</a> series, but the data there is in a strange binary format. Much easier to use their API.</p>
]]></summary><content type="html"><![CDATA[<p>Another <a href="http://blog.jverkamp.com/category/programming/by-topic/backups">backup post</a>, this time I'm going to back up my data from the <a href="https://www.moves-app.com/">Moves App</a> (step counter + GPS tracker). Theoretically, it should be possible to get this same data from the app as part of my <a href="http://blog.jverkamp.com/category/programming/by-project/ios-backup">iOS Backup</a> series, but the data there is in a strange binary format. Much easier to use their API.</p>
<!--more-->
<p>The first step will be to make a few helper methods. As I often do with web scripts, I'll be using <a href="http://blog.jverkamp.com/category/programming/by-language/python">Python</a> and the excellent <a href="http://docs.python-requests.org/en/latest/">Requests</a> library. First things first, we have to get an <code>access_token</code> using an <a href="https://en.wikipedia.org/wiki/OAuth">OAuth</a> handshake. It's a little complicated since our app is designed to run from the command line, yet needs to interact with the user on initial set up, but luckily that only has to be done once:</p>
<pre class="python"><code># Request a new access token

if not 'access_token' in config:
    url = 'https://api.moves-app.com/oauth/v1/authorize?response_type=code&client_id={client_id}&scope={scope}'.format(
        client_id = config['client_id'],
        scope = 'activity location'
    )
    print('Opening URL in browser...')
    webbrowser.open(url)
    code = raw_input('Please follow prompts and enter code: ')

    response = requests.post('https://api.moves-app.com/oauth/v1/access_token?grant_type=authorization_code&code={code}&client_id={client_id}&client_secret={client_secret}&redirect_uri={redirect_uri}'.format(
        code = code,
        client_id = config['client_id'],
        client_secret = config['client_secret'],
        redirect_uri = 'http://localhost/',
    ))
    js = response.json()
    print(js)

    config['access_token'] = js['access_token']
    config['refresh_token'] = js['refresh_token']
    config['user_id'] = js['user_id']

    with open('config.yaml', 'w') as fout:
        yaml.safe_dump(config, fout, default_flow_style=False)</code></pre>
<p>Basically, we have to have two values to start the handshake: <code>client_id</code> and <code>client_secret</code>. I've put those in a separate file (<code>config.yaml</code>) so that we don't have secrets in a repository. From there, we make a request to a given endpoint (see above), which opens in a browser. The user then gets an eight digit code which they enter in the app on the phone, prompting the web browser in turn to redirect with a <code>code</code> parameter. This part is a little ugly and I could make it much nicer by running a temporary single endpoint server, but since this only needs to be done once, I didn't bother.</p>
<p>After that, we take the <code>code</code> we just got, along with the <code>client_id</code> and <code>client_secret</code> and get the initial <code>access_token</code> and a <code>refresh_token</code> we can periodically use to prove we're still the same person.</p>
<p>Next, a little bit of framework. We'll wrap the default <code>requests</code> object to automatically provide an <code>access_token</code> to any <code>GET</code> or <code>POST</code> requests I want to make to the API, now that I've gotten one:</p>
<pre class="python"><code>def makeMethod(f):
    def run(url, **kwargs):

        if 'access_token' in config:
            headers = {'Authorization': 'Bearer {access_token}'.format(access_token = config['access_token'])}
        else:
            headers = {}

        url = 'https://api.moves-app.com/api/1.1' + url.format(**kwargs)

        if 'data' in kwargs:
            return f(url, data = kwargs['data'], headers = headers)
        else:
            return f(url, headers = headers)

    return run

get = makeMethod(requests.get)
post = makeMethod(requests.post)</code></pre>
<p>With that, we can just always use that <code>refresh_token</code> we got above every time we run the script. This is definitely over kill, but it saves a little bit of logic telling when we have to refresh the code or not and doesn't really cost anything more than a single extra request:</p>
<pre class="python"><code># Perform a refresh on the access token just as a matter of course

response = requests.post('https://api.moves-app.com/oauth/v1/access_token', data = {
    'grant_type': 'refresh_token',
    'refresh_token': config['refresh_token'],
    'client_id': config['client_id'],
    'client_secret': config['client_secret']
})
js = response.json()

config['access_token'] = js['access_token']
config['refresh_token'] = js['refresh_token']
config['user_id'] = js['user_id']

with open('config.yaml', 'w') as fout:
    yaml.safe_dump(config, fout, default_flow_style=False)</code></pre>
<p>Next, fetch my user profile:</p>
<pre class="python"><code># Load the user profile to see how far back data goes

user_profile = get('/user/profile').json()</code></pre>
<p>The most interesting bit of information here is <code>.profile.firstDate</code>, which tells us when we first started using Moves. We can then loop from that date forward in time, grabbing any days we are missing. Since sometimes previous days aren't completely done processing the next morning, I'll also always re-download the last week's worth of data no matter what.</p>
<pre class="python"><code># Loop through all missing files, or force load anything less than a week ago

date = datetime.datetime.strptime(user_profile['profile']['firstDate'], '%Y%m%d')
today = datetime.datetime.now()
oneWeekAgo = today - datetime.timedelta(days = 7)

while date &lt; today:
    dir = os.path.join('data', date.strftime('%Y'), date.strftime('%m'))
    filename = os.path.join(dir, date.strftime('%d') + '.json')

    if not date &gt; oneWeekAgo and os.path.exists(filename):
        date += datetime.timedelta(days = 1)
        continue

    if not os.path.exists(dir):
        os.makedirs(dir)

    print(filename)

    response = get('/user/storyline/daily/{date}?trackPoints=true', date = date.strftime('%Y%m%d'))

    if response.status_code != 200:
        print('Bad response, stopping')
        print(response.text)
        sys.exit(0)

    if int(response.headers['x-ratelimit-minuteremaining']) &lt; 1:
        print('Rate limited, waiting one minute before continuing')
        time.sleep(60)

    if int(response.headers['x-ratelimit-hourremaining']) &lt; 1:
        print('Rate limited, wait one hour and try again')
        time.sleep(3600)

    with codecs.open(filename, 'w', 'utf-8') as fout:
        fout.write(response.text)

    date += datetime.timedelta(days = 1)</code></pre>
<p>There is a neat bit in there with the <code>x-ratelimit-minuteremaining</code> and <code>x-ratelimit-hourremaining</code>. If we're downloading the entire history for the first time, you're going to get rate limited. So in this case, we'll wait a minute or an hour until the rate limit has expired.</p>
<p>And that's it. In the end, I end up with a pile of files, one for each day, each with exactly where I was on that day. I can use that data for all sorts of interesting analytics, like how far I walk in the average week, what my area of influence is, or even to combine with my <a href="http://blog.jverkamp.com/category/photography">photography</a> so that I can geotag my pictures. It's a lot of fun.</p>
<p>So, yes. I am something of a digital hoarder. But on the flip side, storage space is cheap and data is interesting. Perhaps I'll get a post or two out of making pretty pretty pictures out of where all I've been!</p>
<p>If you'd like to see / download the entire script for my Moves backup (or any of my other non-iOS backups, those are <a href="http://blog.jverkamp.com/category/programming/by-project/ios-backup">here</a>), you can do so here: <a href="https://github.com/jpverkamp/backup">jpverkamp/backup on GitHub</a></p>]]></content></entry><entry><title>Scraping Kindle Highlights</title><link href="http://blog.jverkamp.com/2015/07/02/scraping-kindle-highlights" /><id>urn:uuid:607a86a7-2b7b-24ef-d68a-fed6db77517e</id><updated>2015-07-02T00:00:00Z</updated><summary type="html"><![CDATA[<p>As part of an ongoing effort to <a href="http://blog.jverkamp.com/category/programming/by-topic/backups">backup all the things</a>, combined with a rather agressive <a href="http://blog.jverkamp.com/2015/01/01/2015-reading-list">2015 Reading List</a>, I wanted to the ability to back up any sections that I've highlighted on my Kindle. Unfortunately, Amazon doesn't seem to have an API to do that, but why should that stop me?</p>
<p>Using a combination of <a href="http://blog.jverkamp.com/category/programming/by-language/python">Python</a> and the Python libraries <a href="http://docs.python-requests.org/en/latest/">Requests</a> and <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a><span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>, it's entirely possible to write a Python script that will log into Amazon, get a list of all of the books on your account, and download the highlights for each.</p>
<p>Let's do it!</p>
]]></summary><content type="html"><![CDATA[<p>As part of an ongoing effort to <a href="http://blog.jverkamp.com/category/programming/by-topic/backups">backup all the things</a>, combined with a rather agressive <a href="http://blog.jverkamp.com/2015/01/01/2015-reading-list">2015 Reading List</a>, I wanted to the ability to back up any sections that I've highlighted on my Kindle. Unfortunately, Amazon doesn't seem to have an API to do that, but why should that stop me?</p>
<p>Using a combination of <a href="http://blog.jverkamp.com/category/programming/by-language/python">Python</a> and the Python libraries <a href="http://docs.python-requests.org/en/latest/">Requests</a> and <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a><span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>, it's entirely possible to write a Python script that will log into Amazon, get a list of all of the books on your account, and download the highlights for each.</p>
<p>Let's do it!</p>
<!--more-->
<p>First, we are going to be using a Requests <code>session</code>. This will keep track of any cookies that Amazon decides to send us so that we know that we're logged in.</p>
<pre class="python"><code>session = requests.Session()</code></pre>
<p>After that, the next thing we need to do is to use requests to log into Amazon. Loading up the login page (<code>https://kindle.amazon.com/login</code>), we see that the <code>form</code> target is a <code>POST</code> request to <code>https://www.amazon.com/ap/signin</code>, specifying the fields <code>email</code> and <code>password</code>. Something like this:</p>
<pre class="python"><code>signin_data = {}

signin_data[u'email'] = os.environ['AMAZON_USERNAME']
signin_data[u'password'] = os.environ['AMAZON_PASSWORD']

response = session.post('https://www.amazon.com/ap/signin', data = signin_data)</code></pre>
<p>I'm reading my Amazon username and password from the environment. In general, that means I can have a simple file like this:</p>
<pre class="bash"><code>export AMAZON_USERNAME="me@example.com"
export AMAZON_PASSWORD="correct horse battery staple"</code></pre>
<p>Then I can source that script before running my program:</p>
<pre class="bash"><code>. ./env.conf && python3 kindle-highlights-backups.py</code></pre>
<p>That should work, but unfortunately it doesn't. It looks like Amazon is sending a small pile of hidden fields. Theoretically, I could look at the page and hard code them, but where's the fun in that? Instead, let's use Requests to grab the login page and BeautifulSoup to parse out all of the fiels we're going to send:</p>
<pre class="python"><code># Log in to Amazon, we have to get the real login page to bypass CSRF
print('Logging in...')
response = session.get('https://kindle.amazon.com/login')
soup = bs4.BeautifulSoup(response.text)

signin_data = {}
signin_form = soup.find('form', {'name': 'signIn'})
for field in signin_form.find_all('input'):
    try:
        signin_data[field['name']] = field['value']
    except:
        pass

signin_data[u'email'] = os.environ['AMAZON_USERNAME']
signin_data[u'password'] = os.environ['AMAZON_PASSWORD']

response = session.post('https://www.amazon.com/ap/signin', data = signin_data)
if response.status_code != 200:
    print('Failed to login: {0} {1}'.format(response.status_code, response.reason))
    sys.exit(0)</code></pre>
<p>... Still doesn't work. I'm getting a page back that says I need to enable cookies, which I most definitely have enabled (that's why I created the <code>session</code>). A bit of Google-fu later, and I find out that Amazon will only allow connections from semi-reasonable <a href="https://en.wikipedia.org/wiki/User_Agents">User Agents</a>. Let's set it to a recent Chrome build on Windows 8.1:</p>
<pre class="python"><code>session = requests.Session()
session.headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.61 Safari/537.36'
}</code></pre>
<p>Ah ha! That works. Finally logged in. Next, we know that we can get a list of your current books by going to <code>https://kindle.amazon.com/your_reading/0/0/0</code>. The last three numbers are:</p>
<ul>
    <li>mode (all, read, reading)</li>
    <li>starting index / page (increments in 25)</li>
    <li>all books (0) versus kindle only (1)</li>
</ul>
<p>So let's write a loop to keep fetching pages of these books, 25 at a time:</p>
<pre class="python"><code># Iterate through pages of books, 25 at a time
# Note: The last three parts of the URL are:
#   - mode (all, read, reading)
#   - starting index / page (increments in 25)
#   - all books (0) versus kindle only (1)
print('Getting books...')
book_page = 0
while True:
    time.sleep(0.5) # Half a second between pages

    response = session.get('https://kindle.amazon.com/your_reading/0/{book_page}/0'.format(book_page = book_page))
    soup = bs4.BeautifulSoup(response.text)
    found_book = False

    ...

    if found_book:
        book_page += 25
    else:
        break</code></pre>
<p>Within that page, there are a series of <code>td</code> elements linking to books, each with the class <code>titleAndAuthor</code>. That's the beauty of BeautifulSoup:</p>
<pre class="python"><code>...

# For each page of books, find all of the individual book links
# The last part of each URL is Amazon's internal ID for that book
for el in soup.findAll('td', {'class': 'titleAndAuthor'}, recursive = True):
    time.sleep(0.1) # 1/10 of a second between books

    found_book = True

    book_id = el.find('a')['href'].split('/')[-1]
    title = el.find('a').text
    sys.stdout.write(title + ' ... ')

    highlights = []
    cursor = 0

    ...</code></pre>
<p>Luckily, the next part is a little easier to deal with. There is actually an API of sorts of Kindle highlights, once you have a user ID. All you have to do is hit <code>https://kindle.amazon.com/kcw/highlights?asin={book_id}</code> (potentially many times, it's <a href="https://en.wikipedia.org/wiki/paginated">paginated</a>):</p>
<pre class="python"><code>...

# Ask the Amazon API for highlights one page of 10 at a time until we have them all
while True:
    response = session.get('https://kindle.amazon.com/kcw/highlights?asin={book_id}&cursor={cursor}&count=10'.format(
        book_id = book_id,
        cursor = cursor,
    ))
    js = response.json()

    found_highlight = False
    for item in js['items']:
        found_highlight = True
        item['highlight'] = html_unescape(item['highlight'])
        highlights.append(item)

    if found_highlight:
        cursor +=1
    else:
        break

...</code></pre>
<p>One caveat is that we don't want to store HTML entities (like <code>&amp;rsquo;</code>), we want the real characters. This is a little annoying, since the library to parse that has moved around in various Python versions:</p>
<pre class="python"><code># Get a function to unescape html entites
try:
    import html
    html_unescape = html.unescape
except:
    try:
        import html.parser
        html_unescape = html.parser.HTMLParser().unescape
    except:
        import HTMLParser
        html_unescape = HTMLParser.HTMLParser().unescape</code></pre>
<p>Yeah...</p>
<p>Now that we have a list of highlights, let's save them to disk. Generate a filename from the book title, and use the <code>json</code> library to write them out. Make sure that we're writing everything as UTF8 so that any more unusual characters (like more interesting quotes) save correctly:</p>
<pre class="python"><code># Use book title as filename, but strip out 'dangerous' characters
print('{count} highlights found'.format(count = len(highlights)))
if highlights:
    filename = re.sub(r'[\/:*?"&lt;&gt;|"\']', '', title).strip() + '.json'
    path = os.path.join('Kindle Highlights', filename)

    with open(path, 'w', encoding = 'utf8') as fout:
        fout.write(json.dumps(highlights, fout, indent = 4, sort_keys = True, ensure_ascii = False))</code></pre>
<p>And there you have it. A simple(ish) way to download your Kindle highlights.</p>
<p>Unfortunately... that's not all she wrote. After running my script for a few days, it started to fail. Why? Because Amazon detected some strange activity on my account and started displaying a captcha. I can detect it easily enough:</p>
<pre class="python"><code>warning = soup.find('div', {'id': 'message_warning'})
if warning:
    print('Failed to login: {0}'.format(warning.text))
    sys.exit(0)</code></pre>
<p>Put that just after the previous 'Failed to login' block and you'll seem some text to the order of 'please enter these characters to continue'. It's actually not that hard to solve a catcha programmatically... but we'll save that for another post.</p>
<p>And that's it for today. So far I have 308 highlights spread over 20 books and it's only growing. It's fun to go back and read them again.</p>]]></content></entry><entry><title>Generating YouTube user RSS feeds</title><link href="http://blog.jverkamp.com/2015/05/11/generating-youtube-user-rss-feeds" /><id>urn:uuid:a097ad77-6800-c1be-c4de-26a9af4348d9</id><updated>2015-05-11T00:00:00Z</updated><summary type="html"><![CDATA[<p>On 4 March 2014, YouTube deprecated the v2.0 API for YouTube (<a href="https://developers.google.com/youtube/2.0/developers_guide_protocol_deprecated">source</a>). One of the unfortunate side effects was that RSS feeds for user uploads were included in what was deprecated.</p>
<p>Previously, you could get an RSS feed with a link of the form: <code>https://gdata.youtube.com/feeds/base/users/{user}/uploads</code> For the longest time, even after the deprecation, those links still worked, but a couple weeks ago, more and more of the video feeds I was subscribed to started redirecting to <a href="https://www.youtube.com/channel/UCMDQxm7cUx3yXkfeHa5zJIQ/videos">YouTube Help account</a>. As thrilling as that channel is, it's not what I'm looking for.</p>
<p>Let's fix it.</p>
]]></summary><content type="html"><![CDATA[<p>On 4 March 2014, YouTube deprecated the v2.0 API for YouTube (<a href="https://developers.google.com/youtube/2.0/developers_guide_protocol_deprecated">source</a>). One of the unfortunate side effects was that RSS feeds for user uploads were included in what was deprecated.</p>
<p>Previously, you could get an RSS feed with a link of the form: <code>https://gdata.youtube.com/feeds/base/users/{user}/uploads</code> For the longest time, even after the deprecation, those links still worked, but a couple weeks ago, more and more of the video feeds I was subscribed to started redirecting to <a href="https://www.youtube.com/channel/UCMDQxm7cUx3yXkfeHa5zJIQ/videos">YouTube Help account</a>. As thrilling as that channel is, it's not what I'm looking for.</p>
<p>Let's fix it.</p>
<!--more-->
<p>First step, we need a YouTube Data API (v3) key. Unfortunately it doesn't look like they provide un-authenticated use of the API as before. That's easy enough though, just <a href="https://developers.google.com/youtube/registering_an_application">go through the steps to register an application</a>. Next, dig through the APIs. It doesn't look like there is a way to directly get a user's videos, but you can get a list of a users 'uploads' playlist, which functions much the same way.</p>
<p>Using Python's <a href="http://docs.python-requests.org/en/latest/">requests</a> library, we just have to hit the correct endpoint:</p>
<pre class="python"><code># Use the channel to get the 'uploads' playlist id
response = requests.get(
    'https://www.googleapis.com/youtube/v3/channels',
    params = {
        'part': 'contentDetails',
        'forUsername': user,
        'key': API_KEY,
    }
)</code></pre>
<p>One example response:</p>
<pre class="json"><code>{"etag": "\"tbWC5XrSXxe1WOAx6MK9z4hHSU8/6wRGj4eVz7tGNiDQjCMuhP6B4vQ\"",
 "items": [{"contentDetails": {"googlePlusUserId": "112244684143881021368",
                               "relatedPlaylists": {"likes": "LL6nSFpj9HTCZ5t-N3Rm3-HA",
                                                    "uploads": "UU6nSFpj9HTCZ5t-N3Rm3-HA"}},
            "etag": "\"tbWC5XrSXxe1WOAx6MK9z4hHSU8/yMieoczWtu9QiK_MEdJrC0hqmdU\"",
            "id": "UC6nSFpj9HTCZ5t-N3Rm3-HA",
            "kind": "youtube#channel"}],
 "kind": "youtube#channelListResponse",
 "pageInfo": {"resultsPerPage": 5, "totalResults": 1}}</code></pre>
<p>That's some progress. Specifically, you can get the playlist ID for the user uploads:</p>
<pre class="python"><code>playlistId = response.json()['items'][0]['contentDetails']['relatedPlaylists']['uploads']</code></pre>
<p>From there, we can query the API again in order to get the most recent 20 items from that specific playlist:</p>
<pre class="python"><code># Get the most recent 20 videos on the 'uploads' playlist
response = requests.get(
    'https://www.googleapis.com/youtube/v3/playlistItems',
    params = {
        'part': 'snippet',
        'maxResults': 20,
        'playlistId': playlistId,
        'key': API_KEY
    }
)</code></pre>
<p>Some snippets from that output (it's rather large):</p>
<pre class="json"><code>{"etag": "\"tbWC5XrSXxe1WOAx6MK9z4hHSU8/PH2ohKtd9aLBba_d7dVtVUfFle0\"",
 "items": [{"etag": "\"tbWC5XrSXxe1WOAx6MK9z4hHSU8/-LUTubHPIceTTTPMrBW-Qs9KOZQ\"",
            "id": "UUKzDohq4XHVIEl9O19Nd8rKuqCo1VravR",
            "kind": "youtube#playlistItem",
            "snippet": {"channelId": "UC6nSFpj9HTCZ5t-N3Rm3-HA",
                        "channelTitle": "Vsauce",
                        "description": "Vsauce is nominated for a Webby "
                                       "in Science & Education! You can "
                                       ...,
                        "playlistId": "UU6nSFpj9HTCZ5t-N3Rm3-HA",
                        "position": 0,
                        "publishedAt": "2015-04-13T19:26:03.000Z",
                        "resourceId": {"kind": "youtube#video",
                                       "videoId": "f8WsO__XcI0"},
                        "thumbnails": {"default": {"height": 90,
                                                   "url": "https://i.ytimg.com/vi/f8WsO__XcI0/default.jpg",
                                                   "width": 120},
                                       ...},
                       "title": "When Will We Run Out Of Names?"}},
           {"etag": "\"tbWC5XrSXxe1WOAx6MK9z4hHSU8/kCpPYobMQgr7w9gfxaR-_cfkJkc\"",
           ...]}</code></pre>
<p>In that snippets blob, we have everything we need. Specifically, the <code>title</code>, <code>thumbnails</code>, and <code>link</code> (by way of <code>resourceID.videoId</code>). Specifically, we can start to construct an RSS feed using the <a href="https://pypi.python.org/pypi/feedgen/">feedgen</a> library<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>:</p>
<pre class="python"><code># Generate a list of results that can be used as feed items
feed = feedgen.feed.FeedGenerator()
feed.title(user + ' (YRSS)')
feed.author({'name': user + ' (YRSS)'})
feed.id('YRSS:' + user)

for item in response.json()['items']:
    title = item['snippet']['title']
    video_id = item['snippet']['resourceId']['videoId']
    published = item['snippet']['publishedAt']
    thumbnail = item['snippet']['thumbnails']['high']['url']
    video_url = 'https://www.youtube.com/watch?v=' + video_id

    item = feed.add_entry()
    item.title(title)
    item.link(href = video_url)
    item.published(dateutil.parser.parse(published))
    item.updated(dateutil.parser.parse(published))
    item.id(video_id)
    item.content('''
&lt;a href="{url}"&gt;&lt;img src="{img}" /&gt;&lt;/a&gt;
&lt;a href="{url}"&gt;{title}&lt;/a&gt;
'''.format(
        url = video_url,
        img = thumbnail,
        title = title,
    ))</code></pre>
<p>And generate the feed:</p>
<pre class="python"><code>feed.atom_str()</code></pre>
<p>I love how (relatively) elegant that was. You don't have to worry about or even know anything about how the underlying XML will be structured.</p>
<p>Since eventually I want this to be a web service, I used Flask to generate a simple API:</p>
<pre class="python"><code>app = flask.Flask(__name__)

@app.route('/&lt;user&gt;.xml')
@app.route('/&lt;user&gt;/atom.xml')
def generatefeed(user):
    ...

    return feed.atom_str()</code></pre>
<p>That way, if you go to <code>http://myserver.com/{user}.xml</code>, you would get an RSS feed for that user's most recent 20 videos. There are a few other considerations to keep in mind (For example, I have a cache that only re-queries the YouTube API once per hour and otherwise re-serves the same feed. And better error handling.)</p>
<p>If you'd like to see the full source in all it's glory, it's available on GitHub: <a href="https://github.com/jpverkamp/yrss">jpverkamp/yrss</a>. You will have to supply an <code>API_KEY</code> as an environment variable to run it, but that should be relatively straight forward.</p>]]></content></entry><entry><title>Tupper's self-referential formula</title><link href="http://blog.jverkamp.com/2015/05/07/tuppers-self-referential-formula" /><id>urn:uuid:bf58a9a0-f27e-d83e-acd1-97ca1db67a9b</id><updated>2015-05-07T00:00:00Z</updated><summary type="html"><![CDATA[<p>Quick post today. Let's implement <a href="https://en.wikipedia.org/wiki/Tupper's_self-referential formula">Tupper's self-referential formula</a> in Racket!</p>
<div>$$ \frac{1}{2} < \left \lfloor mod \left ( \left \lfloor \frac{y}{17} 2^{-17 \lfloor x \rfloor - mod(\lfloor y \rfloor, 2)} \right \rfloor, 2 \right ) \right \rfloor $$</div>
<pre class="racket"><code> (tupper 960939379918958884971672962127852754715004339660129306651505519271702802395266424689642842174350718121267153782770623355993237280874144307891325963941337723487857735749823926629715517173716995165232890538221612403238855866184013235585136048828693337902491454229288667081096184496091705183454067827731551705405381627380967602565625016981482083418783163849115590225610003652351370343874461848378737238198224849863465033159410054974700593138339226497249461751545728366702369745461014655997933798537483143786841806593422227898388722980000748404719) </code></pre>
<p><a href="http://blog.jverkamp.com/2015/05/07/tuppers-self-referential-formula/tupper.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/05/07/tuppers-self-referential-formula/tupper.png" /></a></p>
<p>That's the result of graphing the above function at a point rather far away from the origin. Specifically, where <code>y</code> is around that crazy big number. Look familiar?</p>
]]></summary><content type="html"><![CDATA[<p>Quick post today. Let's implement <a href="https://en.wikipedia.org/wiki/Tupper's_self-referential formula">Tupper's self-referential formula</a> in Racket!</p>
<div>$$ \frac{1}{2} < \left \lfloor mod \left ( \left \lfloor \frac{y}{17} 2^{-17 \lfloor x \rfloor - mod(\lfloor y \rfloor, 2)} \right \rfloor, 2 \right ) \right \rfloor $$</div>
<pre class="racket"><code> (tupper 960939379918958884971672962127852754715004339660129306651505519271702802395266424689642842174350718121267153782770623355993237280874144307891325963941337723487857735749823926629715517173716995165232890538221612403238855866184013235585136048828693337902491454229288667081096184496091705183454067827731551705405381627380967602565625016981482083418783163849115590225610003652351370343874461848378737238198224849863465033159410054974700593138339226497249461751545728366702369745461014655997933798537483143786841806593422227898388722980000748404719) </code></pre>
<p><a href="http://blog.jverkamp.com/2015/05/07/tuppers-self-referential-formula/tupper.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/05/07/tuppers-self-referential-formula/tupper.png" /></a></p>
<p>That's the result of graphing the above function at a point rather far away from the origin. Specifically, where <code>y</code> is around that crazy big number. Look familiar?</p>
<!--more-->
<p>The basic idea behind the formula is that it can encode any arbitrary bitmap (so long as it's black and white and only 106x17 pixels). Essentially under the hood everything is in base 17. First, let's fairly directly translate the original formula into Racket:</p>
<pre class="racket"><code>; Tupper's "self-referential" formula
; Encodes a bitmap as an integer
(define (tupper k)
  (flomap-&gt;bitmap
   (build-flomap*
    1 106 17
    ( (x y)
      (set! y (+ y k))
      (set! x (- 105 x))
      (cond
        [(&lt; 1/2 (floor (mod (* (floor (/ y 17)) (expt 2 (- (* -17 (floor x)) (mod (floor y) 17)))) 2)))
         (vector 0)]
        [else
         (vector 1)])))))</code></pre>
<p>One amusing caveat that we have to deal with here is that <code>modulus</code> doesn't work on numbers this large. So instead, we're going to have to do it manually:</p>
<pre class="racket"><code>; Modulus that will work with really large numbers
(define (mod a b)
  (define q (floor (/ a b)))
  (define r (- a (* b q)))
  r)</code></pre>
<p>Whee!</p>
<p>Another neat trick I was playing with is "rendering" the image by adding one digit at a time (in base 10, so it's mostly noise):</p>
<pre class="racket"><code>(define (render-to target)
  (define str-target (number-&gt;string target))
  (define str-buffer (make-string (string-length str-target) #\0))

  (for/list ([i (in-range (sub1 (string-length str-target)) -1)])
    (string-set! str-buffer i (string-ref str-target i))
    (tupper (string-&gt;number str-buffer))))

&gt; (write-animated-gif
   (render-to 960939379918958884971672962127852754715004339660129306651505519271702802395266424689642842174350718121267153782770623355993237280874144307891325963941337723487857735749823926629715517173716995165232890538221612403238855866184013235585136048828693337902491454229288667081096184496091705183454067827731551705405381627380967602565625016981482083418783163849115590225610003652351370343874461848378737238198224849863465033159410054974700593138339226497249461751545728366702369745461014655997933798537483143786841806593422227898388722980000748404719)
   5
   "tupper.gif"
   #:last-frame-delay 50)</code></pre>
<p><a href="http://blog.jverkamp.com/2015/05/07/tuppers-self-referential-formula/tupper.gif" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/05/07/tuppers-self-referential-formula/tupper.gif" /></a></p>
<p>If you look carefully, you'll occasionally see flashes of the final image being rendered. This happens whenever the base-10 numbers that we're adding line up with the base-17 encoding.</p>
<p>I'm not sure it's particularly useful for anything, but I found it amusing.</p>
<p>Code: <a href="https://github.com/jpverkamp/small-projects/blob/master/blog/tupper.rkt">tupper.rkt</a></p>]]></content></entry><entry><title>It's all Greek to me</title><link href="http://blog.jverkamp.com/2015/04/17/its-all-greek-to-me" /><id>urn:uuid:b361ed4c-c170-609c-a31d-3fbc5642e648</id><updated>2015-04-17T00:00:00Z</updated><summary type="html"><![CDATA[<p>A few days ago an interesting article came across my RSS feeds: <a href="http://flowingdata.com/2015/04/14/its-all-greek-or-chinese-or-spanish-or-to-me/">Its All Greek (or Chinese or Spanish or) to Me</a>. Basically, in English, when you're confused, you'll often say 'It's all Greek to me'. It turns out that man (if not all) languages around the world have a similar saying, but the target varies. Luckily, Wikipedia has a lovely page about it: <a href="https://en.wikipedia.org/wiki/Greek_to me">Greek to me</a>.</p>
]]></summary><content type="html"><![CDATA[<p>A few days ago an interesting article came across my RSS feeds: <a href="http://flowingdata.com/2015/04/14/its-all-greek-or-chinese-or-spanish-or-to-me/">Its All Greek (or Chinese or Spanish or) to Me</a>. Basically, in English, when you're confused, you'll often say 'It's all Greek to me'. It turns out that man (if not all) languages around the world have a similar saying, but the target varies. Luckily, Wikipedia has a lovely page about it: <a href="https://en.wikipedia.org/wiki/Greek_to me">Greek to me</a>.</p>
<!--more-->
<p>When I posted the link to Facebook, I got a quick question: are there any cycles? While one could just scan through the document, it would be a lot more interesting (at least to me!) if you could do it automatically. Let's toss together a quick script to do it.</p>
<p>First thing we need: a way to get the content of the Wikipedia page. Python is great for this, with <a href="http://docs.python-requests.org/en/latest/">requests</a> to grab the page and <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> to process it:</p>
<pre class="python"><code>content = requests.get('https://en.wikipedia.org/wiki/Greek_to_me').text
soup = bs4.BeautifulSoup(content)

table = soup.find('table', {'class': 'wikitable sortable'})

pairs = collections.defaultdict(set)

for row in table.findAll('tr'):
    cols = row.findAll('td')
    if not cols:
        continue

    if len(cols) == 5:
        srcs = [src.strip() for src in cols[0].text.split(',')]

    dsts = [dst.strip() for dst in cols[-1].text.split(',')]
    for i, dst in enumerate(dsts):
        dsts[i] = re.sub(r'[\[(].*?[\])]', '', dst)

    for src in srcs:
        if ' ' in src: continue

        for dst in dsts:
            if ' ' in dst: continue

            pairs[src].add(dst)</code></pre>
<p>Basically, we download the page. Then we go through each of the rows (<code>tr</code>). Skip any rows without column elements (<code>td</code>) as that's probably the header, otherwise, pull them out. The first column (index <code>0</code>) is the language with the idiom (English in the example) while the last column (index <code>-1</code>) is the target (Greek). There's one caveat though, that sometimes the table uses a <code>rowspan</code> when one source can have multiple targets but is only listed once. We check that by only changing the <code>srcs</code> when there are 5 columns.</p>
<p>Parse through all of that and what do you have?</p>
<pre class="python"><code>&gt;&gt;&gt; import pprint
&gt;&gt;&gt; pprint.pprint(dict(pairs))
{u'': set([]),
 u'Afrikaans': set([u'Greek']),
 u'Albanian': set([u'Chinese']),
 u'Arabic': set([u'Chinese', u'Garshuni']),
 ...
 u'Vietnamese': set([u'Cambodian']),
 u'Volap\xfck': set([]),
 u'Yiddish': set([u'Aramaic'])}</code></pre>
<p>Exactly what I was looking for. Okay, next step. Find any cycles in the graph. This is straight forward enough by performing a <a href="https://en.wikipedia.org/wiki/depth_first search">depth first search</a>:</p>
<pre class="python"><code>def cycle(node, seen):

    for neighbor in pairs[node]:
        new_seen = seen + [neighbor]

        if neighbor in seen:
            yield new_seen[new_seen.index(neighbor):]
        else:
            for recur in cycle(neighbor, new_seen):
                yield recur</code></pre>
<p>The basic idea is to make a generator that returns each cycle as it finds it. It does so by search down each branch, maintaining a list of all nodes it has <code>seen</code>. If it sees the same node twice, that's a cycle. Otherwise, try all of the neighbors. We avoid infinite loops since there's a guaranteed base case to the recursion: <code>seen</code> is always one bigger on each step and it's maximum size is the number of nodes in the graph.</p>
<p>So how does it work?</p>
<pre class="python"><code>&gt;&gt;&gt; for result in cycle('English', ['English']):
...     print result
...
['English', u'Greek', u'Chinese', u'English']
['English', u'Greek', u'Turkish', u'Arabic', u'Chinese', u'English']
['English', u'Greek', u'Turkish', u'French', u'Chinese', u'English']
['English', u'Greek', u'Turkish', u'French', u'Hebrew', u'Chinese', u'English']
['English', u'Dutch', u'Chinese', u'English']</code></pre>
<p>Neat! We've already found 5 cycles that involve English alone. But how many cycles are there all together? For that, we need a way to determine if a cycle is actually unique. If you have the cycles <code>A -> B -> C -> A</code>, that's the same as <code>B -> C -> A -> B</code>. You can do this by putting the cycles in <a href="https://en.wikipedia.org/wiki/lexical_order">lexical order</a> (so that the 'smallest' element in the cycle is first).</p>
<pre class="python"><code>def reorder(cycle):
    if cycle[0] == cycle[-1]:
        cycle = cycle[1:]

    smallest = min(cycle)
    for el in list(cycle):
        if el == smallest:
            break
        else:
            cycle = cycle[1:] + [cycle[0]]

    return cycle</code></pre>
<p>It also is smart enough that if we pass it a list with the first and last node the same (as we will), it trims that off automatically.</p>
<pre class="python"><code>&gt;&gt;&gt; reorder(['A', 'B', 'C', 'A'])
['A', 'B', 'C']
&gt;&gt;&gt; reorder(['B', 'C', 'A', 'B'])
['A', 'B', 'C']</code></pre>
<p>Bam. So we use that and a <code>set</code> to keep track of what we've seen:</p>
<pre class="python"><code>&gt;&gt;&gt; seen = set()
&gt;&gt;&gt; for src in pairs.keys():
...     for result in cycle(src, [src]):
...         result = reorder(result)
...         if not str(result) in seen:
...             print(result)
...             seen.add(str(result))
...
[u'Chinese', u'English', u'Greek']
[u'Chinese', u'English', u'Dutch']
[u'Arabic', u'Chinese', u'English', u'Greek', u'Turkish']
[u'Chinese', u'English', u'Greek', u'Turkish', u'French']
[u'Chinese', u'English', u'Greek', u'Turkish', u'French', u'Hebrew']</code></pre>
<p>Huh. So they all go through English. I didn't actually expect that. :) Still, it's cool to be able to unify them like that.</p>
<p>Okay, one last trick. Let's visualize them. Luckily, there's a nice Python interface for <a href="https://pypi.python.org/pypi/graphviz">graphviz</a> that we can use:</p>
<pre class="python"><code># --- Render a nice graph ---

g = graphviz.Digraph()
for src in pairs.keys():
    for dst in pairs[src]:
        g.edge(src, dst)

g.graph_attr['overlap'] = 'false'
g.graph_attr['splines'] = 'true'

g.format = 'png'
g.engine = 'neato'

g.render('greek-to-me')</code></pre>
<p><a href="http://blog.jverkamp.com/2015/04/17/its-all-greek-to-me/greek-to-me.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/04/17/its-all-greek-to-me/greek-to-me.png" /></a></p>
<p>Awesome.</p>
<p>It's not the easiest thing in the world to read, but if you look carefully you can pick out a few interesting things. Let's tweak it a bit to color nodes if and only if they have both an inward edge and an outward one:</p>
<pre class="python"><code>for src in pairs.keys():
    # Does this node lead to another
    has_out = pairs[src]

    # Does any node lead to this one
    has_in = False
    for dst in pairs.keys():
        if src in pairs[dst]:
            has_in = True
            break

    # If both, color it
    if has_out and has_in:
        g.node(src, color = 'blue')
</code></pre>
<p><a href="http://blog.jverkamp.com/2015/04/17/its-all-greek-to-me/greek-to-me-color-nodes.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/04/17/its-all-greek-to-me/greek-to-me-color-nodes.png" /></a></p>
<p>That's a little better, all of the nodes in any cycle are in there. Let's go ahead and show all of the edges in any cycle:</p>
<pre class="python"><code># Get all edges that are part of a cycle
cycle_edges = set()
for cycle in cycles:
    for src, dst in zip(cycle, cycle[1:]):
        cycle_edges.add((src, dst))
    cycle_edges.add((cycle[-1], cycle[0]))

for src in pairs.keys():
    for dst in pairs[src]:
        if (src, dst) in cycle_edges:
            g.edge(src, dst, color = 'blue')
        else:
            g.edge(src, dst)</code></pre>
<p><a href="http://blog.jverkamp.com/2015/04/17/its-all-greek-to-me/greek-to-me-color.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/04/17/its-all-greek-to-me/greek-to-me-color.png" /></a></p>
<p>So they're all in that pocket. If I had a few more minutes, I could show all of the cycles as different colors, but that gets complicated in that many re-use the same paths. So it goes.</p>
<p>If you'd like to see / run the code, you can grab it from GitHub: <a href="https://github.com/jpverkamp/small-projects/blob/master/blog/greek-to-me.py">greek-to-me.py</a></p>]]></content></entry><entry><title>A Quick Look at RC4</title><link href="http://blog.jverkamp.com/2015/04/14/a-quick-look-at-rc4" /><id>urn:uuid:7faa7241-8deb-6230-5db2-37d89e5af00c</id><updated>2015-04-14T00:00:00Z</updated><summary type="html"><![CDATA[<p>In cryptography work, <a href="https://en.wikipedia.org/wiki/RC4">RC4</a> (Rivest Cipher 4) is well known as both one of the easiest to implement and fastest to run <a href="https://en.wikipedia.org/wiki/symmetric_encryption">symmetric encryption</a> algorithms. Unfortunately, over time there have been a number of attacks on RC4, both in poorly written protocols (such as in the case of <a href="https://en.wikipedia.org/wiki/WEP">WEP</a>) or statistical attacks against the protocol itself.</p>
<p>Still, for how well it formed, it's an amazingly simple algorithm, so I decided to try my hand at implementing it.</p>
]]></summary><content type="html"><![CDATA[<p>In cryptography work, <a href="https://en.wikipedia.org/wiki/RC4">RC4</a> (Rivest Cipher 4) is well known as both one of the easiest to implement and fastest to run <a href="https://en.wikipedia.org/wiki/symmetric_encryption">symmetric encryption</a> algorithms. Unfortunately, over time there have been a number of attacks on RC4, both in poorly written protocols (such as in the case of <a href="https://en.wikipedia.org/wiki/WEP">WEP</a>) or statistical attacks against the protocol itself.</p>
<p>Still, for how well it formed, it's an amazingly simple algorithm, so I decided to try my hand at implementing it.</p>
<!--more-->
<p>Basically, RC4 is what is known as a '<a href="https://en.wikipedia.org/wiki/stream_cipher">stream cipher</a>', implying that each byte in the input message is encrypted individually (generally taking into account feedback from previous bytes). This runs counter to the perhaps more well known <a href="https://en.wikipedia.org/wiki/block_ciphers">block ciphers</a> such as DES and AES, where bytes are instead encrypted together (although feedback between blocks is still of course possible).</p>
<p>The first step of the algorithm is to take your encryption key (a password or the like) and convert it into a sequence of bytes at least as long as your input. For RC4, this is done in two pieces. First, prepare the index:</p>
<pre class="python"><code>def rc4(key, msg):
    S = list(range(256))

    j = 0
    for i in range(256):
        j = (j + S[i] + key[i % len(key)]) % 256
        S[i], S[j] = S[j], S[i]

    ...</code></pre>
<p>Or in Racket:</p>
<pre class="racket"><code>(define (rc4 key msg)
  (define (mod256 n) (modulo n 256))

  (define permutation (make-bytes 256))
  (for ([i (in-range 256)])
    (bytes-set! permutation i i))

  (define (S i)
    (bytes-ref permutation i))

  (define (swap! i j)
    (let ([pi (bytes-ref permutation i)]
          [pj (bytes-ref permutation j)])
      (bytes-set! permutation i pj)
      (bytes-set! permutation j pi)))

  ; Key-scheduling algorithm
  (for/fold ([j 0]) ([i (in-range 256)])
    (let ([j (mod256 (+ j
                        (S i)
                        (bytes-ref key (modulo i (bytes-length key)))))])
      (swap! i j)
      j))

  ...)</code></pre>
<p>I made the Racket version a little more verbose with helper functions, since I know I'll both be indexing the permutation and swapping values again in the next step. That's one of the reasons that I'll sometimes go for Python over Racket in on off scripts.</p>
<p>Still, relatively simple in both cases.</p>
<p>The next step is to turn that into a stream, essentially creating an infinite number generator. Luckily, both Python and Racket have generators, which are perfectly suited for this sort of thing (assuming in both cases that <code>S</code> / <code>permutation</code> are in scope from above):</p>
<pre class="python"><code>def rc4(key, msg):
    ...

    def prga():
        i = j = 0
        while True:
            i = (i + 1) % 256
            j = (j + S[i]) % 256
            S[i], S[j] = S[j], S[i]
            yield S[(S[i] + S[j]) % 256]

    return prga # DEBUG</code></pre>
<pre class="racket"><code>(define (rc4 key msg)
  ...

  ; Pseudo-random generation algorithm
  (define prga
    (generator ()
      (let loop ([i 1] [j (S 1)])
        (swap! i j)
        (yield (S (mod256 (+ (S i) (S j)))))
        (loop (mod256 (+ i 1)) (mod256 (+ j (S (+ i 1))))))))

  prng) ; DEBUG</code></pre>
<p>Now that we have a stream, we can generate a few bytes and take a look if we wanted:</p>
<pre class="python"><code>&gt;&gt;&gt; import binascii, itertools
&gt;&gt;&gt; prga = rc4(b'Secret', b'Attack at dawn')
&gt;&gt;&gt; print(binascii.hexlify(bytes(itertools.islice(prga(), 10))))
b'04d46b053ca87b594172'</code></pre>
<pre class="racket"><code>(define (bytes-&gt;hex b*)
  (apply ~a (for/list ([b (in-bytes b*)])
        (~a (number-&gt;string (quotient b 16) 16)
            (number-&gt;string (modulo b 16) 16)))))

&gt; (define prga (rc4 "Secret" "Attack at dawn"))
&gt; (bytes-&gt;hex (apply bytes (for/list ([i (in-range 10)] [b (in-producer prga)]) b)))
"04d46b053ca87b594172"</code></pre>
<p>Both of them the same? Good sign. Both matching the example on the Wikipedia page? Even better!</p>
<p>So, we have an infinite stream of bytes. What next?</p>
<p>Well, this is actually the crazy part: You just <a href="https://en.wikipedia.org/wiki/xor">xor</a> them.</p>
<pre class="python"><code>def rc4(key, msg):
    ...

    return bytes(msgbyte ^ keybyte for msgbyte, keybyte in zip(msg, prga()))</code></pre>
<pre class="racket"><code>(define (rc4 key msg)
  ...

  ; Encryption
  (apply bytes
    (for/list ([input-byte (in-bytes msg)] [key-byte (in-producer prga)])
      (bitwise-xor input-byte key-byte))))</code></pre>
<p>And now we can encrypt!</p>
<pre class="python"><code>&gt;&gt;&gt; print(binascii.hexlify(rc4(b'Secret', b'Attack at dawn')))
b'45a01f645fc35b383552544b9bf5'</code></pre>
<pre class="racket"><code>&gt; (bytes-&gt;hex (rc4 "Secret" "Attack at dawn"))
"45a01f645fc35b383552544b9bf5"</code></pre>
<p>And decrypt!</p>
<pre class="python"><code>&gt;&gt;&gt; rc4(b'Secret', rc4(b'Secret', b'Attack at dawn'))
b'Attack at dawn'</code></pre>
<pre class="racket"><code>&gt; (rc4 "Secret" (rc4 "Secret" "Attack at dawn"))
#"Attack at dawn"</code></pre>
<p>Very cool. I'm really starting to see the appeal of RC4. A couple dozen lines of Python/Racket and you're encrypting. Bam. As mentioned, it's not really an algorithm you should use in encryption any more (the author has released a slightly more complicated algorithm called Spritz that works very similarly).</p>
<p>And that's it. If you'd like to see the entire code in one place (along with some fiddling in both cases to deal with Unicode keys/messages as well as pure bytes), it's on GitHub: <a href="https://github.com/jpverkamp/small-projects/blob/master/blog/rc4.py">rc4.py</a>, <a href="https://github.com/jpverkamp/small-projects/blob/master/blog/rc4.rkt">rc4.rkt</a>.</p>
<p><code>7b82 c5cf 12c4 e168 8a4a 5cbe 9300</code></p>
<p><img alt="smile" class="emoji" src="/emoji/smile.svg" /></p>]]></content></entry><entry><title>Generating perfect portmanteaus</title><link href="http://blog.jverkamp.com/2015/04/07/generating-perfect-portmanteaus" /><id>urn:uuid:0b6a158d-e441-c880-5fcf-4d4a1394e61d</id><updated>2015-04-07T00:00:00Z</updated><summary type="html"><![CDATA[<p>A quick programming post, since it's been a while, inspired by this video:</p>
<p><iframe width="560" height="315" src="//www.youtube.com/embed/QVn2PZGZxaI" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>I'm not going to go quite as far as that, but I thought it would be interesting to write up some quick code to generate portmanteaus<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>.</p>
]]></summary><content type="html"><![CDATA[<p>A quick programming post, since it's been a while, inspired by this video:</p>
<p><iframe width="560" height="315" src="//www.youtube.com/embed/QVn2PZGZxaI" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>I'm not going to go quite as far as that, but I thought it would be interesting to write up some quick code to generate portmanteaus<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>.</p>
<!--more-->
<p>Basically<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>, a portmanteau is a combination of two words, smooshing them together<span class="footnote"><sup><a href="#footnote-3">[3]</a></sup></span> and dropping some letters from each. In this case, what I'm specifically interested in is 'perfect' portmanteaus (I'm not sure if there is a better term for it), where the suffix of one word exactly matches the prefix of the other.</p>
<p>As an example, consider the words <code>hamster</code> and <code>termine</code>. The last three letters of the former, perfectly matches the first three of the latter, so let's overlap them. <code>hamstermite</code>. Bam.</p>
<p>So how do we do it?</p>
<pre class="racket"><code>(define current-minimum-overlap (make-parameter 3))

(define (portmanteau left right)
  (define maximum-overlap (- (min (string-length left) (string-length right)) 1))

  (for*/first ([overlap (in-range maximum-overlap (- (current-minimum-overlap) 1) -1)]
               #:when (equal? (substring left (- (string-length left) overlap))
                              (substring right 0 overlap)))
    (list left
          right
          (string-append
           (substring left 0 (- (string-length left) overlap))
           right))))</code></pre>
<p>Should be straight forward enough. Basically, we start with the longest possible overlap (1 less than the length of the shorter word, since we don't want to completely subsume a word), counting down until we reach some minimum overlap. For each possible sequence, we compare the prefix and suffix of the two words, only proceeding into the body of the loop when they match. That's the beauty of <code><a href="http://docs.racket-lang.org/search/index.html?q=for*/first">for*/first</a></code>, it will loop until it gets a valid value, returning when it does.</p>
<p>And that's really it. Try it out with the example from earlier:</p>
<pre class="racket"><code>&gt; (portmanteau "hamster" "termite")
'("hamster" "termite" "hamstermite")</code></pre>
<p>Since that was so quick, let's put some simple wrapper code around it in order to find all portmanteaus from a given word list. First, do the heavy lifting of finding portmanteaus:</p>
<pre class="racket"><code>(define (portmanteaus)
  (define words
    (for*/list ([raw-line (in-lines)]
                [line (in-value (string-trim (string-downcase raw-line)))]
                #:when (not (equal? "" line)))
      line))

  (for*/list ([left (in-list words)]
              [right (in-list words)]
              #:when (not (eq? left right))
              [portmanteau (in-value (portmanteau left right))]
              #:when portmanteau)
    portmanteau))</code></pre>
<p><code><a href="http://docs.racket-lang.org/search/index.html?q=in-value">in-value</a></code> is useful in <code><a href="http://docs.racket-lang.org/search/index.html?q=for*">for*</a></code> since it lets you bind a single value for future <code>#:when</code> blocks without having to recalculate anything.</p>
<p>After that, a wrapper to process some command line parameters and render output in a few different ways:</p>
<pre class="racket"><code>(define paths
  (command-line
   #:program "portmanteau"
   #:once-each
   [("--minimum-overlap")
    overlap
    "Specify the minimum necessary overlap (default = 3)"
    (cond
      [(string-&gt;number overlap) =&gt; current-minimum-overlap]
      [else (error '--minimum-overlap "must specify a number")])]
   #:once-any
   [("--verbose")
    "Print in verbose mode (default = false)"
    (verbose-mode #t)]
   [("--graph")
    "Print out a dotfile"
    (graph-mode #t)]
   #:args paths

   paths))

(when (null? paths)
  (set! paths '("-")))

(for ([path (in-list paths)])
  (define results
    (cond
      [(equal? path "-")
       (portmanteaus)]
      [else
       (with-input-from-file path portmanteaus)]))

  (define g (unweighted-graph/directed '()))

  (for ([result (in-list results)])
    (match-define (list left right portmanteau) result)
    (cond
      [(verbose-mode)
       (printf "~a + ~a = ~a\n" left right portmanteau)]
      [(graph-mode)
       (add-edge! g (~a "\"" left "\"") (~a "\"" right "\""))]
      [else
       (displayln portmanteau)]))

  (when (graph-mode)
    (displayln (graphviz g))))</code></pre>
<p>Now you can do some interesting things:</p>
<pre class="bash"><code>$ racket portmanteau.rkt animals.txt

brown recluse spider monkey
gila monstermite
grasshopperegrine falcon
hamstermite
leechidna
ottermite</code></pre>
<p>Just in case you cannot figure out what animals actually went into that list:</p>
<pre class="bash"><code>$ racket portmanteau.rkt --verbose animals.txt

brown recluse spider + spider monkey = brown recluse spider monkey
gila monster + termite = gila monstermite
grasshopper + peregrine falcon = grasshopperegrine falcon
hamster + termite = hamstermite
leech + echidna = leechidna
otter + termite = ottermite</code></pre>
<p>Or if you want to be a little more general, matching with only 2 characters rather than the default 3:</p>
<pre class="bash"><code>$ racket portmanteau.rkt --minimum-overlap 2 --verbose animals.txt

armadillo + loon = armadilloon
armadillo + lorikeet = armadillorikeet
armadillo + louse = armadillouse
black mamba + badger = black mambadger
brown bear + armadillo = brown bearmadillo
brown recluse spider + spider monkey = brown recluse spider monkey
chinchilaa + aardvark = chinchilaardvark
copperhead snake + kestrel = copperhead snakestrel
coyote + termite = coyotermite
crow + owl = crowl
eagle + leech = eagleech
eagle + leopard seal = eagleopard seal
echidna + narwhal = echidnarwhal
gecko + koala = geckoala
gila monster + termite = gila monstermite
grasshopper + peregrine falcon = grasshopperegrine falcon
hamster + termite = hamstermite
hyena + narwhal = hyenarwhal
jackal + albatross = jackalbatross
king cobra + rattlesnake = king cobrattlesnake
king cobra + raven = king cobraven
kingsnake + kestrel = kingsnakestrel
kiwi + wild boar = kiwild boar
leech + chinchilaa = leechinchilaa
leech + echidna = leechidna
leopard seal + albatross = leopard sealbatross
narwhal + albatross = narwhalbatross
ostrich + chinchilaa = ostrichinchilaa
otter + termite = ottermite
polar bear + armadillo = polar bearmadillo
rattlesnake + kestrel = rattlesnakestrel
sloth bear + armadillo = sloth bearmadillo
snapping turtle + leech = snapping turtleech
snapping turtle + leopard seal = snapping turtleopard seal
sparrow + owl = sparrowl
sperm whale + leech = sperm whaleech
sperm whale + leopard seal = sperm whaleopard seal
sponge + gecko = spongecko
swan + anaconda = swanaconda
wild boar + armadillo = wild boarmadillo</code></pre>
<p>Heh. Narwhalbatross. Wild boarmadillo. <img alt="smile" class="emoji" src="/emoji/smile.svg" /></p>
<p>And as a final bonus, using the <a href="https://github.com/stchang/graph/tree/master">graph</a> library I've used (and contributed to) before, we can render the structure of the thing):</p>
<pre class="bash"><code>$ racket portmanteau.rkt --graph --minimum-overlap 2 animals.txt \
    | sed "s/edge \[dir=none\];//g" \
    | fdp -Tpng &gt; animals.png \
    && open animals.png</code></pre>
<p><a href="http://blog.jverkamp.com/2015/04/07/generating-perfect-portmanteaus/animals.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/04/07/generating-perfect-portmanteaus/animals.png" /></a></p>
<p>Fun. :)</p>
<p>Think of the arrows as going from the stuck on word to where it's sticking rather than in the order the words would be written. It's easy enough to change though if you'd like, just swap the arguments in the <code>add-edge!</code> call above.</p>
<p>And... that's it. Not much more to do with this one, unless I want to duplicate the above video and portmanteau all the things! We'll see.</p>
<p>As with all my code, you can see the entire thing on GitHub: <a href="https://github.com/jpverkamp/small-projects/blob/master/blog/portmanteau.rkt">portmanteau.rkt</a></p>]]></content></entry><entry><title>Performance problems with Flask and Docker</title><link href="http://blog.jverkamp.com/2015/04/03/performance-problems-with-flask-and-docker" /><id>urn:uuid:8d7d8e5b-effe-34a6-b190-d0068d5879a0</id><updated>2015-04-03T00:00:00Z</updated><summary type="html"><![CDATA[<p>I had an interesting problem recently on a project I was working on. It's a simple <a href="http://flask.pocoo.org/">Flask</a>-based webapp, designed to be deployed to <a href="https://aws.amazon.com/">AWS</a> using <a href="https://www.docker.com/">Docker</a>. The application worked just fine when I was running it locally, but as soon as I pushed the docker container...</p>
<p>Latency spikes. Bad enough that the application was failing AWS's healthy host checks, cycling in and out of existence<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>:</p>
<p><a href="http://blog.jverkamp.com/2015/04/03/performance-problems-with-flask-and-docker/health-check.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/04/03/performance-problems-with-flask-and-docker/health-check.png" /></a></p>
]]></summary><content type="html"><![CDATA[<p>I had an interesting problem recently on a project I was working on. It's a simple <a href="http://flask.pocoo.org/">Flask</a>-based webapp, designed to be deployed to <a href="https://aws.amazon.com/">AWS</a> using <a href="https://www.docker.com/">Docker</a>. The application worked just fine when I was running it locally, but as soon as I pushed the docker container...</p>
<p>Latency spikes. Bad enough that the application was failing AWS's healthy host checks, cycling in and out of existence<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>:</p>
<p><a href="http://blog.jverkamp.com/2015/04/03/performance-problems-with-flask-and-docker/health-check.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/04/03/performance-problems-with-flask-and-docker/health-check.png" /></a></p>
<!--more-->
<p>At that time, the only traffic to the container was the health checks, every 30 seconds, as regular as clockwork. So it wasn't load that was making them fail. And it was exactly the same code each time<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span><span class="footnote"><sup><a href="#footnote-3">[3]</a></sup></span>:</p>
<pre class="python"><code>@app.route('/', methods = ['GET'])
def healthcheck():
    return "I'm a teapot"</code></pre>
<p>So not that either. So what in the world was going on?</p>
<p>Google to the rescue! <code><a href="https://www.google.com/search?q=flask application periodically slow">flask application periodically slow</a></code></p>
<p>The very first link is a response on StackOverflow:</p>
<blockquote>
    On operating systems that support ipv6 and have it configured such as modern Linux systems, OS X 10.4 or higher as well as Windows Vista some browsers can be painfully slow if accessing your local server. The reason for this is that sometimes localhost is configured to be available on both ipv4 and ipv6 socktes and some browsers will try to access ipv6 first and then ivp4. -- <a href="http://stackoverflow.com/questions/11150343/slow-requests-on-local-flask-server">Slow Requests on Local Flask Server</a>
</blockquote>
<p>Huh. Get a shell into my docker container, and what do you know:</p>
<pre class="bash"><code>$ cat /etc/hosts
172.17.1.112	27392a3e0fa5
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters</code></pre>
<p>Yup. <code>localhost</code> routes to both IPv4's <code>127.0.0.1</code> and IPv6's <code>::1</code>. Comment out the <code>::1</code> line and give it a shot... Yup. That did it. Waited ten minutes and the hosts weren't marked unhealthy once. All I should need to do is add it to the <code>Dockerfile</code> and we should be golden, yes?</p>
<pre class="text"><code>$ vi Dockerfile
...
RUN sed -i "s/::1.*//g"
...

$ docker build .
...
Step 9 : RUN sed -i "s/::1.*//g" /etc/hosts
 ---&gt; Running in 7c73dc473507
sed: cannot rename /etc/sedXZv0Yy: Device or resource busy</code></pre>
<p>What.</p>
<pre class="text"><code>$ vi Dockerfile
...
RUN sed "s/::1.*//g" /etc/hosts &gt; /etc/hosts-new && mv /etc/hosts-new /etc/hosts
...

$ docker build .
...
RUN sed "s/::1.*//g" /etc/hosts &gt; /etc/hosts-new && mv /etc/hosts-new /etc/hosts
 ---&gt; Running in d6b896f4fc9e
sed: cannot rename /etc/sedqYrfxO: Device or resource busy</code></pre>
<p>Double what.</p>
<p>Back to Google: <code><a href="https://www.google.com/search?q=docker edit hosts">docker edit hosts</a></code></p>
<p>Specifically: <a href="https://github.com/docker/docker/issues/1951">Unable to modify /etc/hosts file in a container #1951</a>. Looks like there was a fix that would let you edit <code>/etc/hosts</code> if you were in a container (that used to not be possible), but (because it's actually mounted rather than just a container file), it's non-trivial to edit it as part of a build.</p>
<p>All righty then.</p>
<p>That's about when I decided to listen to the Flask documentation:</p>
<blockquote>You can use the builtin server during development, but you should use a full deployment option for production applications. (Do not use the builtin development server in production.)</blockquote>
<p>All right. Not only is it what I'm actually supposed to be doing, but if I used CGI, I can avoid Flask trying to resolve <code>localhost</code> at all. I've worked with <a href="http://wiki.nginx.org/Main">nginx</a> before. Let's use that.</p>
<p>Picking some documentation from a hat, I decided to use <a href="https://uwsgi-docs.readthedocs.org/en/latest/">uWSGI</a> as the glue between nginx and Flask. Easy enough to install with pip (although I had to grab a C compiler from the apt package <code>build-essential</code>) and off we go.</p>
<p>First, a small <code>nginx</code> config:</p>
<pre class="nginx"><code>location / { try_files $uri @project; }
location @project {
    include uwsgi_params;
    uwsgi_pass unix:/tmp/uwsgi.sock;
}</code></pre>
<p>Then, to start it all up, a change to the <code>Dockerfile</code> <code>CMD</code>:</p>
<pre class="bash"><code>CMD uwsgi -s /tmp/uwsgi.sock -w project:app --chown-socket=www-data:www-data --enable-threads & \
    nginx -g 'daemon off;'</code></pre>
<p>That <code>--chown-socket</code> flag really drove me a bit batty. Basically, <code>uwsgi</code> was starting as the <code>root</code> user (within the Docker container). <code>nginx</code> was starting as <code>root</code>. But the <code>nginx</code> threads were not. They were starting as <code>www-data</code> and thus couldn't read the Unix socket between the two.</p>
<p>All righty then.</p>
<p>Let's go!</p>
<p>Starting successfully... And it's running. Not on the first try or even the 10th (I left out quite a bit of fumbling around tweaking flags), but eventually as was well in the world.</p>
<p>Push it out to AWS...</p>
<p>Health check passed.</p>
<p>Bam.</p>
<p>Awesome.</p>
<p>Now I not only have a neat little webapp, I have one that doesn't randomly decide to take forever on every other request or so.</p>
<p>If you're looking for the bare minimum <code>requirements.txt</code> and <code>Dockerfile</code> that I'm using (in addition to that <code>nginx</code> host configuration file above), here they are:</p>
<p><code>requirements.txt</code></p>
<pre class="text"><code>flask
flup6
uwsgi</code></pre>
<p><code>Dockerfile</code>:</p>
<pre class="text"><code>FROM ubuntu:14.04

RUN apt-get update && apt-get install -y build-essential nginx python3.4 python3.4-dev
RUN easy_install3 pip

WORKDIR /project

ADD requirements.txt /project/requirements.txt
RUN pip install -r requirements.txt

ADD . /project

ADD nginx /etc/nginx

CMD uwsgi -s /tmp/uwsgi.sock -w project:app --chown-socket=www-data:www-data --enable-threads & \
    nginx -g 'daemon off;'</code></pre>
<p>It's for moments like these that I do software. That little moment when everything comes together just right and it all just ... works.</p>]]></content></entry></feed>