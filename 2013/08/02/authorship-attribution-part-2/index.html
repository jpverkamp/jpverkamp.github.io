<!doctype html><html><head><title>Authorship attribution: Part 2 â€“ jverkamp.com</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><link rel=alternate type=application/atom+xml title="jverkamp.com (Atom 2.0)" href=//blog.jverkamp.com/feed/><script src=/jquery_5823201688106629450.min.8c7d803d89ebdecf2416d468f4ecb981a3acf645154a7a336f2e5d0190ea8163.js integrity="sha256-jH2APYnr3s8kFtRo9Oy5gaOs9kUVSnozby5dAZDqgWM=" defer></script>
<script src=/jquery.fancybox_6181813213021922412.min.921ca906a32e718ab61ac0b4da24e0eaa6bdd41912654a33b44bd3fc2f0c2a4d.js integrity="sha256-khypBqMucYq2GsC02iTg6qa91BkSZUoztEvT/C8MKk0=" defer></script>
<script src=/katex_12008035502722260518.min.1c3dce03daaf56a7d2fe0d0ec49bf35256837226f835adaf52c09502ef9bc5d1.js integrity="sha256-HD3OA9qvVqfS/g0OxJvzUlaDcib4Na2vUsCVAu+bxdE=" defer></script>
<script src=/bigfoot_8444447145154709333.min.5e80bd85ebaeb95607834d6777bfe7013d1d161f0f02a31b845e4bd765898316.js integrity="sha256-XoC9heuuuVYHg01nd7/nAT0dFh8PAqMbhF5L12WJgxY=" defer></script>
<script src=/mermaid_12365941879912544862.min.973b5415b3fa1835d620c0e58236f859d5f80891f447e240cbbb9eb825251ff0.js integrity="sha256-lztUFbP6GDXWIMDlgjb4WdX4CJH0R+JAy7ueuCUlH/A=" defer></script>
<script src=/main.min.982c2d7bb434b4cf8b19c2cf38ef7e7f7162ddbed610e5bdddbb937001e26574.js integrity="sha256-mCwte7Q0tM+LGcLPOO9+f3Fi3b7WEOW93buTcAHiZXQ=" defer></script>
<link rel=stylesheet href=/katex_14163593549783030822.min.16bacd59fb224ae6c97a749ab0ac1e708cb5e0c9ce5c9a08d7fd73dc8e69429f.css integrity="sha256-FrrNWfsiSubJenSasKwecIy14MnOXJoI1/1z3I5pQp8="><link rel=stylesheet href=/bigfoot-default_16482899066638414220.min.f15d4faa4519addb976f9d69b42bd9eb491b3596b6b2eda83aa3e9e1f48b8f14.css integrity="sha256-8V1PqkUZrduXb51ptCvZ60kbNZa2su2oOqPp4fSLjxQ="><link rel=stylesheet href=/jquery.fancybox_16222217791602823071.min.e28b5d7d9d89efacb5f708ac30cbd76b1b9a0f816dfd9da96631d1d09cbbdd76.css integrity="sha256-4otdfZ2J76y19wisMMvXaxuaD4Ft/Z2pZjHR0Jy73XY="><link rel=stylesheet href=/css_4780214198035419921.min.246b6f8e7620eeb717bca5e7b121037906e5dfaa05805f427fcc5a09b6c99f5c.css integrity="sha256-JGtvjnYg7rcXvKXnsSEDeQbl36oFgF9Cf8xaCbbJn1w="><link rel=stylesheet href=/main.min.e0e68b86dea32185ab89b0b9cc01649107cc6b0be3290c8c7b13c716bc0dabfa.css integrity="sha256-4OaLht6jIYWribC5zAFkkQfMawvjKQyMexPHFrwNq/o="></head><body><div id=wrapper><header id=page-header role=banner><h1><a href=/>JP's Blog</a></h1><ul id=page-header-links><li><a href=https://github.com/jpverkamp>GitHub</a> *
<a href=https://www.flickr.com/photos/jpverkamp>Flickr</a> *
<a href=/resume>Resume</a></li><li><form action=/search/ method=get class="navbar-form navbar-right" role=search _lpchecked=1><div class=form-group><input name=q type=text class=form-control placeholder=Search>
<button type=submit class="btn btn-default" value=Search>Search</button></div></form></li></ul><nav id=header-navigation role=navigation class=ribbon><ul class=main-navigation><li><a href=https://blog.jverkamp.com/programming/>Programming</a></li><li><a href=https://blog.jverkamp.com/reviews/>Reviews</a></li><li><a href=https://blog.jverkamp.com/photography/>Photography</a></li><li><a href=https://blog.jverkamp.com/maker/>Maker</a></li><li><a href=https://blog.jverkamp.com/writing/>Writing</a></li><li><a href=https://blog.jverkamp.com/research/>Research</a></li><li><a href=https://blog.jverkamp.com/search/>Search</a></li><li class=subscription data-subscription=rss><a href=/atom.xml rel=subscribe-rss title="subscribe via RSS">RSS</a></li></ul></nav></header><div id=page-content-wrapper><div id=page-content><article data-pagefind-body><header><h1 class=entry-title data-pagefind-meta=title>Authorship attribution: Part 2</h1><div class=entry-meta><span class=entry-date>2013-08-02</span></div><div class=entry-taxonomies><div class=entry-tags><ul class=taxonomy-keys><li><a class=taxonomy-key href=/programming/languages/>Languages</a><ul class=taxonomy-values><li><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/ class=previous-link></a><a class=taxonomy-value href=/programming/languages/racket>Racket</a><a href=https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/ class=next-link></a></li><li><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/ class=previous-link></a><a class=taxonomy-value href=/programming/languages/scheme>Scheme</a><a href=https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/ class=next-link></a></li></ul></li><li><a class=taxonomy-key href=/programming/topics/>Topics</a><ul class=taxonomy-values><li><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/ class=previous-link></a><a class=taxonomy-value href=/programming/topics/computational-linguistics>Computational Linguistics</a><a href=https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/ class=next-link></a></li><li><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/ class=previous-link></a><a class=taxonomy-value href=/programming/topics/mathematics>Mathematics</a><a href=https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/ class=next-link></a></li><li><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/ class=previous-link></a><a class=taxonomy-value href=/programming/topics/research>Research</a><a href=https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/ class=next-link></a></li><li><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/ class=previous-link></a><a class=taxonomy-value href=/programming/topics/vectors>Vectors</a><a href=https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/ class=next-link></a></li></ul></li><li><a class=taxonomy-key href=/programming>programming</a><ul><li><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/ class=previous-link>Prev</a>
<a href=https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/ class=next-link>Next</a></ul></li><li><a class=taxonomy-key href=/>All Posts</a><ul><li><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/ class=previous-link>Prev</a>
<a href=https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/ class=next-link>Next</a></ul></li></ul></div></div></header><div class=entry-content><p><a href=https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/>Last time</a>, we used word rank to try to figure out who could possibly have written Cuckoo&rsquo;s calling. It didn&rsquo;t work out so well, but we at least have a nice framework in place. So perhaps we can try a few more ways of turning entire novels into a few numbers.</p><p>Rather than word rank, how about <a href=https://en.wikipedia.org/wiki/stop%20word>stop word</a> frequency. Essentially, stop words are small words such as articles and prepositions that don&rsquo;t always carry much weight for a sentence&rsquo;s meaning. On the other hand though, those are exactly the same words that appear most commonly, so perhaps the frequencies will tell us something more.</p><p>The code is actually rather similar. To start out with, we want to load in a set of stop words. There are dozens of lists out there; any of them will work.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scheme data-lang=scheme><span style=display:flex><span><span style=color:#75715e>; Load the stop words</span>
</span></span><span style=display:flex><span>(<span style=color:#66d9ef>define </span>stop-words
</span></span><span style=display:flex><span>  (with-input-from-file <span style=color:#e6db74>&#34;stop-words.txt&#34;</span>
</span></span><span style=display:flex><span>    (<span style=color:#66d9ef>lambda </span>()
</span></span><span style=display:flex><span>      (<span style=color:#a6e22e>for/set</span> ([line (<span style=color:#a6e22e>in-lines</span>)]) (<span style=color:#a6e22e>fix-word</span> line)))))
</span></span></code></pre></div><p>With that, we just need to count the occurances of each word and then normalize. This will help when some books have more or less stop words overall.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scheme data-lang=scheme><span style=display:flex><span><span style=color:#75715e>; Calculate the relative frequencies of stop words in a text</span>
</span></span><span style=display:flex><span>(<span style=color:#66d9ef>define </span>(<span style=color:#a6e22e>stop-word-frequency</span> [in (<span style=color:#a6e22e>current-input-port</span>)])
</span></span><span style=display:flex><span>  <span style=color:#75715e>; Store the frequency and word count</span>
</span></span><span style=display:flex><span>  (<span style=color:#66d9ef>define </span>counts (<span style=color:#a6e22e>make-hash</span>))
</span></span><span style=display:flex><span>  (<span style=color:#66d9ef>define </span>total (<span style=color:#a6e22e>make-parameter</span> <span style=color:#ae81ff>0.0</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>; Loop across the input text</span>
</span></span><span style=display:flex><span>  (<span style=color:#a6e22e>for*</span> ([line (<span style=color:#a6e22e>in-lines</span> in)]
</span></span><span style=display:flex><span>         [word (<span style=color:#a6e22e>in-list</span> (<span style=color:#a6e22e>string-split</span> line))])
</span></span><span style=display:flex><span>    (<span style=color:#66d9ef>define </span>fixed (<span style=color:#a6e22e>fix-word</span> word))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    (<span style=color:#a6e22e>when</span> (<span style=color:#a6e22e>set-member?</span> stop-words fixed)
</span></span><span style=display:flex><span>      (<span style=color:#a6e22e>total</span> (+ (<span style=color:#a6e22e>total</span>) <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>      (<span style=color:#a6e22e>hash-set!</span> counts word (<span style=color:#a6e22e>add1</span> (<span style=color:#a6e22e>hash-ref</span> counts word <span style=color:#ae81ff>0</span>)))))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>; Normalize and return frequencies</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>; Use the order in the stop words file</span>
</span></span><span style=display:flex><span>  (<span style=color:#a6e22e>for/vector</span> ([word (<span style=color:#a6e22e>in-set</span> stop-words)])
</span></span><span style=display:flex><span>    (/ (<span style=color:#a6e22e>hash-ref</span> counts word <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>       (<span style=color:#a6e22e>total</span>))))
</span></span></code></pre></div><p>Using Cuckoo&rsquo;s Calling and a particular list with 50 words, we have:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scheme data-lang=scheme><span style=display:flex><span>&gt; (with-input-from-file <span style=color:#e6db74>&#34;../target.txt&#34;</span> stop-word-frequency)
</span></span><span style=display:flex><span><span style=color:#f92672>&#39;#</span>(<span style=color:#ae81ff>0.043</span> <span style=color:#ae81ff>0.003</span> <span style=color:#ae81ff>0.000</span> <span style=color:#ae81ff>0.002</span> <span style=color:#ae81ff>0.026</span>
</span></span><span style=display:flex><span>   <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>   <span style=color:#ae81ff>0.001</span> <span style=color:#ae81ff>0.000</span> <span style=color:#ae81ff>0.000</span> <span style=color:#ae81ff>0.000</span> <span style=color:#ae81ff>0.000</span>)
</span></span></code></pre></div><p>Well that doesn&rsquo;t mean much to us. Hopefully it means more to the computer. ðŸ˜„</p><p>So how similar does this one say Cuckoo&rsquo;s Calling and the Deathly Hallows are?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scheme data-lang=scheme><span style=display:flex><span>&gt; (<span style=color:#66d9ef>let </span>([a (with-input-from-file <span style=color:#e6db74>&#34;Cuckoo&#39;s Calling.txt&#34;</span> stop-word-frequency)]
</span></span><span style=display:flex><span>        [b (with-input-from-file <span style=color:#e6db74>&#34;Deathly Hallows.txt&#34;</span> stop-word-frequency)])
</span></span><span style=display:flex><span>    (<span style=color:#a6e22e>cosine-similarity</span> a b))
</span></span><span style=display:flex><span><span style=color:#ae81ff>0.877</span>
</span></span></code></pre></div><p>That&rsquo;s a lot higher! Unfortunately, that doesn&rsquo;t really mean that they&rsquo;re more similar than the other test. For all we know, everything could be more similar. So let&rsquo;s try the entire library again:</p><table><thead><tr><th>1</th><th>0.896</th><th>Stephen, King</th><th>Wizard and Glass</th></tr></thead><tbody><tr><td>2</td><td>0.896</td><td>Rowling, J.K.</td><td>Harry Potter and the Order of the Phoenix</td></tr><tr><td>3</td><td>0.895</td><td>Riordan, Rick</td><td>The Mark of Athena</td></tr><tr><td>4</td><td>0.891</td><td>Jordan, Robert</td><td>Knife of Dreams</td></tr><tr><td>5</td><td>0.891</td><td>Riordan, Rick</td><td>The Lost Hero</td></tr><tr><td>6</td><td>0.888</td><td>Jordan, Robert</td><td>A Crown of Swords</td></tr><tr><td>7</td><td>0.888</td><td>Riordan, Rick</td><td>The Son of Neptune</td></tr><tr><td>8</td><td>0.887</td><td>Croggon, Alison</td><td>The Singing</td></tr><tr><td>9</td><td>0.887</td><td>Stephen, King</td><td>The Drawing of the Three</td></tr><tr><td>10</td><td>0.884</td><td>Jordan, Robert</td><td>Crossroads of Twilight</td></tr></tbody></table><p>Well, that&rsquo;s good and bad. It&rsquo;s unfortunate that it&rsquo;s not first, but we actually have a Harry Potter book in the top 10! The rest aren&rsquo;t that low down either, mostly appearing in the top 25. That should help with the author averages:</p><table><thead><tr><th>1</th><th>0.876</th><th>Jordan, Robert</th></tr></thead><tbody><tr><td>2</td><td>0.876</td><td>Rowling, J.K.</td></tr><tr><td>3</td><td>0.873</td><td>Stephen, King</td></tr><tr><td>4</td><td>0.865</td><td>Martin, George R. R.</td></tr><tr><td>5</td><td>0.851</td><td>Riordan, Rick</td></tr></tbody></table><p>None too shabby! It&rsquo;s a bit surprising that Robert Jordan is up at the top, but if we only consider authors that were actually around to write Cuckoo&rsquo;s Calling, JK Rowling is actually at the top of the list.</p><p>Still, can we do better?</p><p>Here&rsquo;s another idea (that I used in my &lt;a href="//blog.jverkamp.com"/category/programming/anngram/">previous work</a>): <a href=https://en.wikipedia.org/wiki/n-grams>n-grams</a>. Essentially, take constant sized slices of text, completely ignoring the content. So if you were dealing with the text &lsquo;THE DUCK QUACKS&rsquo; and 4-grams, you would have these:</p><pre tabindex=0><code>&#39;THE &#39;  &#39;HE D&#39;  &#39;E DU&#39;  &#39; DUC&#39;  &#39;DUCK&#39;
&#39;UCK &#39;  &#39;CK Q&#39;  &#39;K QU&#39;  &#39; QUA&#39;  &#39;QUAC&#39;
&#39;UACK&#39;
</code></pre><p>How does this help us? Well, in addition to keeping track of the most common words, n-grams will capture the relationships between words. Theoretically, this extra information might help out. So how do we measure it?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scheme data-lang=scheme><span style=display:flex><span><span style=color:#75715e>; Calculate n gram frequencies</span>
</span></span><span style=display:flex><span>(<span style=color:#66d9ef>define </span>(<span style=color:#a6e22e>n-gram-frequency</span> [in (<span style=color:#a6e22e>current-input-port</span>)] <span style=color:#f92672>#</span>:n [n <span style=color:#ae81ff>4</span>] <span style=color:#f92672>#</span>:limit [limit <span style=color:#ae81ff>100</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>; Store counts and total to do frequency later</span>
</span></span><span style=display:flex><span>  (<span style=color:#66d9ef>define </span>counts (<span style=color:#a6e22e>make-hash</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>; Keep a circular buffer of text, read char by char</span>
</span></span><span style=display:flex><span>  (<span style=color:#66d9ef>define </span>n-gram (make-string <span style=color:#ae81ff>4</span> <span style=color:#e6db74>#\n</span>ul))
</span></span><span style=display:flex><span>  (<span style=color:#a6e22e>for</span> ([c (<span style=color:#a6e22e>in-port</span> read-char in)])
</span></span><span style=display:flex><span>    (<span style=color:#66d9ef>set! </span>n-gram (substring (string-append n-gram (string c)) <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    (<span style=color:#a6e22e>hash-set!</span> counts n-gram (<span style=color:#a6e22e>add1</span> (<span style=color:#a6e22e>hash-ref</span> counts n-gram <span style=color:#ae81ff>0</span>))))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>; Find the top limit many values</span>
</span></span><span style=display:flex><span>  (<span style=color:#66d9ef>define </span>top-n 
</span></span><span style=display:flex><span>    (<span style=color:#a6e22e>take</span> 
</span></span><span style=display:flex><span>     (<span style=color:#a6e22e>sort</span>
</span></span><span style=display:flex><span>      (<span style=color:#a6e22e>for/list</span> ([(<span style=color:#a6e22e>key</span> val) (<span style=color:#a6e22e>in-hash</span> counts)])
</span></span><span style=display:flex><span>        (list val key))
</span></span><span style=display:flex><span>      (<span style=color:#66d9ef>lambda </span>(<span style=color:#a6e22e>a</span> b) (&gt; (car a) (car b))))
</span></span><span style=display:flex><span>     limit))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>; Cacluate the frequency of just those</span>
</span></span><span style=display:flex><span>  (<span style=color:#66d9ef>define </span>total (* <span style=color:#ae81ff>1.0</span> (<span style=color:#a6e22e>for/sum</span> ([vk (<span style=color:#a6e22e>in-list</span> top-n)]) (car vk))))
</span></span><span style=display:flex><span>  (<span style=color:#a6e22e>for/hash</span> ([vk (<span style=color:#a6e22e>in-list</span> top-n)])
</span></span><span style=display:flex><span>    (values (cadr vk) (/ (car vk) total))))
</span></span></code></pre></div><p>It&rsquo;s pretty much the same as the previous code. The only difference is the code to read the n-grams rather than the words, but that should be pretty straight forward. It&rsquo;s certainly not the most efficient, but it&rsquo;s fast enough. It can churn through a few hundred books in a few minutes. Good enough for me.</p><p>How does it perform though?</p><table><thead><tr><th>1</th><th>0.777</th><th>Stephen,</th><th>Wizard and Glass</th></tr></thead><tbody><tr><td>2</td><td>0.764</td><td>Jordan, Robert</td><td>The Gathering Storm</td></tr><tr><td>3</td><td>0.757</td><td>Card, Orson Scott</td><td>Heart Fire</td></tr><tr><td>4</td><td>0.757</td><td>Card, Orson Scott</td><td>Children of the Mind</td></tr><tr><td>5</td><td>0.756</td><td>Stephen,</td><td>Song of Susannah</td></tr><tr><td>6</td><td>0.756</td><td>Stephen,</td><td>The Dark Tower</td></tr><tr><td>7</td><td>0.753</td><td>Butcher, Jim</td><td>White Night</td></tr><tr><td>8</td><td>0.751</td><td>Butcher, Jim</td><td>Turn Coat</td></tr><tr><td>9</td><td>0.746</td><td>Butcher, Jim</td><td>Captain&rsquo;s Fury</td></tr><tr><td>10</td><td>0.746</td><td>Butcher, Jim</td><td>Side Jobs</td></tr></tbody></table><p>That&rsquo;s not so good. How about the averages?</p><table><thead><tr><th>1</th><th>0.731</th><th>Stephen, King,</th></tr></thead><tbody><tr><td>2</td><td>0.724</td><td>Martin, George R. R.</td></tr><tr><td>3</td><td>0.715</td><td>Jordan, Robert</td></tr><tr><td>4</td><td>0.708</td><td>Butcher, Jim</td></tr><tr><td>5</td><td>0.698</td><td>Robinson, Kim Stanley</td></tr></tbody></table><p>It turns out that JK Rowling is actually second from the bottom. Honestly, I&rsquo;m not sure what this says. Did I mess up the algorithm? Well then why are Steven King, Robert Jordan, and Jim Butcher still so high up?</p><p>I still have a few more ideas though. Next week it is!</p></div></article></div></div><footer id=page-footer role=contentinfo><nav id=footer-navigation role=navigation class=ribbon><ul class=main-navigation><li><a href=/archive-by-date/>All posts: By Date</a></li><li><a href=/archive-by-tag/>All posts: By Tag</a></li><li><a href=/atom.xml>RSS: All <sup><svg xmlns="http://www.w3.org/2000/svg" width="8" height="8" viewBox="0 0 24 24"><path fill="#fff" d="M6.503 20.752C6.503 22.546 5.047 24 3.252 24c-1.796.0-3.252-1.454-3.252-3.248s1.456-3.248 3.252-3.248c1.795.001 3.251 1.454 3.251 3.248zM0 8.18v4.811c6.05.062 10.96 4.966 11.022 11.009h4.817C15.777 15.29 8.721 8.242.0 8.18zm0-3.368C10.58 4.858 19.152 13.406 19.183 24H24c-.03-13.231-10.755-23.954-24-24v4.812z"/></svg></sup></a></li><li><a href=/programming/atom.xml>RSS: programming<sup><svg xmlns="http://www.w3.org/2000/svg" width="8" height="8" viewBox="0 0 24 24"><path fill="#fff" d="M6.503 20.752C6.503 22.546 5.047 24 3.252 24c-1.796.0-3.252-1.454-3.252-3.248s1.456-3.248 3.252-3.248c1.795.001 3.251 1.454 3.251 3.248zM0 8.18v4.811c6.05.062 10.96 4.966 11.022 11.009h4.817C15.777 15.29 8.721 8.242.0 8.18zm0-3.368C10.58 4.858 19.152 13.406 19.183 24H24c-.03-13.231-10.755-23.954-24-24v4.812z"/></svg></sup></a></li></ul></nav><div id=page-footer-content data-pagefind-ignore=all><div class=legal><p>All posts unless otherwise mentioned are licensed under
<a rel=license href=//creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US><img alt="Creative Commons License" style=border-width:0 src=//i.creativecommons.org/l/by-nc-sa/3.0/80x15.png></a></p><p>Any source code unless otherwise mentioned is licensed under the <a href=//directory.fsf.org/wiki/License:BSD_3Clause>3 clause BSD license</a></p></div></div></footer></div></body></html>