<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>jverkamp.com</title><link href="http://blog.jverkamp.com" /><link rel="self" href="http://blog.jverkamp.com/feed/" /><updated>2015-08-24T00:00:00Z</updated><author><name>JP Verkamp</name></author><id>urn:uuid:f148b655-ada3-c720-0c01-ca384ab68088</id><entry><title>Adjacency Matrix Generator</title><link href="http://blog.jverkamp.com/2015/08/24/adjacency-matrix-generator" /><id>urn:uuid:136b3438-1761-5142-adc5-95a8b5586bca</id><updated>2015-08-24T00:00:00Z</updated><summary type="html"><![CDATA[<p>Been a while since I've actually tackled one of the <a href="http://blog.jverkamp.com/category/programming/by-source/daily-programmer">Daily Programmer</a> challenges, so let's try one out. From <a href="https://www.reddit.com/r/dailyprogrammer/comments/3h0uki/20150814_challenge_227_hard_adjacency_matrix/)">a week and a half ago</a>, we are challeneged to make an adjacency matrix generator, turning a graphical representation of a graph into an <a href="https://en.wikipedia.org/wiki/adjacency_matrix">adjacency matrix</a>.</p>
<p>Input:</p>
<pre class="text"><code>a-----b
|\   / \
| \ /   \
|  /     e
| / \   /
|/   \ /
c-----d</code></pre>
<p>Output:</p>
<pre class="text"><code>01110
10101
11010
10101
01010</code></pre>
]]></summary><content type="html"><![CDATA[<p>Been a while since I've actually tackled one of the <a href="http://blog.jverkamp.com/category/programming/by-source/daily-programmer">Daily Programmer</a> challenges, so let's try one out. From <a href="https://www.reddit.com/r/dailyprogrammer/comments/3h0uki/20150814_challenge_227_hard_adjacency_matrix/)">a week and a half ago</a>, we are challeneged to make an adjacency matrix generator, turning a graphical representation of a graph into an <a href="https://en.wikipedia.org/wiki/adjacency_matrix">adjacency matrix</a>.</p>
<p>Input:</p>
<pre class="text"><code>a-----b
|\   / \
| \ /   \
|  /     e
| / \   /
|/   \ /
c-----d</code></pre>
<p>Output:</p>
<pre class="text"><code>01110
10101
11010
10101
01010</code></pre>
<!--more-->
<p>Specifically, we're working under a few conditions:</p>
<ul>
    <li>Nodes<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span> will be represented by a lowercase letter <code>a</code> to <code>z</code> (there will never be more than 26 nodes)</li>
    <li>Edges are either horizontally, vertically, or diagonally at 45 degrees and will be represented by <code>|-/\</code></li>
    <li>All edges will have a node at each end</li>
    <li>Any two nodes have at most one edge between them</li>
    <li>If edges need to bend, a virtual node marked by a <code>#</code> will be inserted (see examples later)</li>
    <li>Edges can overlap; there will always be at least one directed edge adjacent to each node</li>
</ul>
<p>My general idea for a solution takes place in three parts:</p>
<ol>
    <li>Load the grid into a quickly accessible form</li>
    <li>From each node, try each of the 8 cardinal directions looking for an edge; on an edge follow it until you hit either:
        <ul>
            <li>Another node: record the edge</li>
            <li>A virtual node: recursively check that edge instead</li>
        </ul>
    </li>
    <li>Print out the edges found above</li>
</ol>
<p>Let's do it. First, read in the edges:</p>
<pre class="python"><code>from collections import defaultdict as ddict

# Points will be stored as (row, col) tuples
# The grid is a mapping of points to the character in that location
# Nodes nodes store their point for fast iteration
# The final solution will be a mapping of nodes to a set of other nodes adjacent to them
grid = ddict(lambda : ' ')
nodes = {}
adjacency = ddict(set)

# Read the rest of the grid
for row, line in enumerate(sys.stdin):
    for col, char in enumerate(line):

        pt = (row, col)
        grid[pt] = char

        if is_node(char):
            nodes[char] = (row, col)</code></pre>
<p>Straight forward. I love <code>defaultdict</code>, since it means we don't have to worry about looking for points off the edge of the grid. If it's out of bounds, it will just return empty space.</p>
<p>Next, the 'seeking' function, that will follow an edge until a node (either real or virtual):</p>
<pre class="python"><code># List of possible edges, ordered row, col, edge type
possible_edges = (
    (-1, -1, '\\'), (-1, 0, '|'), (-1, 1, '/'),
    ( 0, -1, '-'),                ( 0, 1, '-'),
    ( 1, -1, '/'),  ( 1, 0, '|'), ( 1, 1, '\\')
)

def neighbors(src_pt, previous_deltas = None):
    '''Given a point, yield any neighboring nodes'''

    src = grid[src_pt]
    row, col = src_pt

    for row_delta, col_delta, edge_type in possible_edges:

        dst_pt = (row + row_delta, col + col_delta)
        dst = grid[dst_pt]

        # Don't go back the way we came
        if (-row_delta, -col_delta) == previous_deltas:
            continue

        # A valid leaving edge, follow it until a node or a #
        elif dst == edge_type:
            for i in naturals(2):
                dst_pt = (row + i * row_delta, col + i * col_delta)
                dst = grid[dst_pt]

                # Found the target node, add
                if is_node(dst):
                    yield dst
                    break

                # Found a connector, continue on the other exit point
                elif dst == '#':
                    yield from neighbors(dst_pt, (row_delta, col_delta))
                    break</code></pre>
<p>Basically, we have eight <code>possible_edges</code>, each of which has a delta along the row and column, along with the character that has to start and end it (the rest could be under something else, we don't really care, since we're making the assumption that the input is well formed).</p>
<p>I did use a couple of helper functions here. They should be fairly obvious:</p>
<pre class="python"><code>def is_node(c):
    return c in 'abcdefghijklmnopqrstuvwxyz'

def is_edge(c):
    return c in '|-/\\'

def naturals(i = 0):
    while True:
        yield i
            i += 1</code></pre>
<p>That includes one trick I've picked up from my <a href="http://blog.jverkamp.com/category/programming/by-language/racket">Racket</a> posts. <code>naturals</code> is an infinite list (a generator) of all natural numbers, starting at the given point. It's useful in this case since we don't know how far we're going to run along the edge, just that it will end eventually.</p>
<p>Finally, we need functions to iterate over all of the nodes as starting points and then to print out the results:</p>
<pre class="python"><code># Start at each node and expand all edges
# Note: This will find each edge twice, so it goes
for (src, pt) in nodes.items():
    for dst in neighbors(pt):
        adjacency[src].add(dst)
        adjacency[dst].add(src)

# Print an adjacency matrix in sorted node order
for src in sorted(nodes):
    for dst in sorted(nodes):
        if dst in adjacency[src]:
            sys.stdout.write('1')
        else:
            sys.stdout.write('0')
    sys.stdout.write('\n')</code></pre>
<p>As noted, this will find each edge twice, but so it goes. To account for that, we would have to use up a bit more memory storing the state of where we've been. As it is, we have a mostly functional solution, which I like.</p>
<p>And that's it. Let's see a few of the test cases from the <a href="https://www.reddit.com/r/dailyprogrammer/comments/3h0uki/20150814_challenge_227_hard_adjacency_matrix/)">original post</a>:</p>
<pre class="bash"><code>$ cat 1.matrix; python3 matrix-reader.py &lt; 1.matrix

7
a-----b
|\   / \
| \ /   \
|  /     e
| / \   /
|/   \ /
c-----d

01110
10101
11010
10101
01010

$ cat 3.matrix; python3 matrix-reader.py &lt; 3.matrix

7
a  b--c
|    /
|   /
d  e--f
 \    |
  \   |
g--h--#

00010000
00100000
01001000
10000001
00100100
00001001
00000001
00010110

9
   #--#
   | /        #
   |a--------/-\-#
  #--\-c----d   /
   \  \|     \ / \
   |\  b      #   #
   | #  \        /
   |/    #------#
   #

0111
1011
1101
1110</code></pre>
<p>Shiny!</p>
<p>I wonder how hard it would be to program the inverse: take an adjacency matrix as input and generate one of these graphical matrices as output? Even better, generate an 'optimal' graphic, with the smallest possible area.</p>
<p>We'll see.</p>
<p>The full code is available on GitHub: <a href="https://github.com/jpverkamp/small-projects/blob/master/blog/matrix-reader.py">matrix-reader.py</a></p>]]></content></entry><entry><title>Setting up Postfix and OpenDKIM</title><link href="http://blog.jverkamp.com/2015/08/10/setting-up-postfix-and-opendkim" /><id>urn:uuid:65ecefd0-9ca8-1ba1-d32a-ae3857191285</id><updated>2015-08-10T00:00:00Z</updated><summary type="html"><![CDATA[<p>Last week, I was presented with a fairly interesting challenge: add DKIM (via <a href="http://www.opendkim.org/">OpenDKIM</a>) support to our mail servers (running <a href="http://www.postfix.org/">Postfix</a>). Given that I've never actually worked on a mail server before, it sounded fun. <img alt="smile" class="emoji" src="/emoji/smile.svg" /></p>
]]></summary><content type="html"><![CDATA[<p>Last week, I was presented with a fairly interesting challenge: add DKIM (via <a href="http://www.opendkim.org/">OpenDKIM</a>) support to our mail servers (running <a href="http://www.postfix.org/">Postfix</a>). Given that I've never actually worked on a mail server before, it sounded fun. <img alt="smile" class="emoji" src="/emoji/smile.svg" /></p>
<!--more-->
<p>First, a bit of background on what exactly DKIM is:</p>
<blockquote>
    DomainKeys Identified Mail (DKIM) is an email validation system designed to detect email spoofing by providing a mechanism to allow receiving mail exchangers to check that incoming mail from a domain is authorized by that domain's administrators and that the email (including attachments) has not been modified during transport. A digital signature included with the message can be validated by the recipient using the signer's public key published in the DNS.
    -- Wikipedia: <a href="https://en.wikipedia.org/wiki/DKIM">DKIM</a>
</blockquote>
<p>Sounds nice. So what in the world does that mean?</p>
<h2>A bit of background</h2>
<p>Starting in the details, we have <a href="https://en.wikipedia.org/wiki/public_key cryptography">public key cryptography</a>. The basic idea of public key cryptography is that you have some sort of algorithm with two keys: one of which can be used to encrypt things and can be made public and another separate piece of information which can be used to decrypt things and should remain private. That way, you can publish your public key, well, publically. Then anyone that wants to send you a message can do so, knowing that only you (since only you possess the private key) can read it.</p>
<p>If we take this a step further, we can swap the roles of the public and private key<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>. Instead of encrypting with the public key, we will use the private key, requiring the <em>public</em> key for decryption. This sounds mad, since the public key is, by definition public. So what's the point of a message that only you can write but anyone can read?</p>
<p>Well, that's exactly the point: <em>only you</em> can write it. This is what's called a digital signature. Since only your private key could have encoded the message and since only you have the private key, this allows anyone to read your message and be safe in the knowledge that you wrote it.</p>
<p>This is exactly what DKIM does.</p>
<p>By creating an public/private key pair, publishing the public key to a DNS record, and using the private key to sign messages, you are allowing any receiver to verify that <em>you</em> were person who sent them. Don't get me wrong, there are still a few problems with this approach (we'll get to them later), but it's a cool idea.</p>
<h2>Implementation</h2>
<p>So how do you actually implement it in practice?</p>
<p>Given that I don't have any particular previous experience with mail servers, the answer involves a lot of Google. Here are the steps that worked for me.</p>
<h4>1 - Have a previously configured server correctly delivering mail via <a href="http://www.postfix.org/">Postfix</a></h4>
<h4>2 - Install <a href="http://www.opendkim.org/">OpenDKIM</a></h4>
<pre class="bash"><code>sudo apt-get install -y opendkim opendkim-tools</code></pre>
<h4>3 - Generate a new keypair</h4>
<pre class="bash"><code>sudo opendkim-genkey -s mail -d example.com</code></pre>
<p>The <code>-s</code> argument specifies a selector, which allows us to have multiple keypairs specified on the same domain (if we wanted to), while the <code>-d</code> is the domain we are signing for. In this case, we are generating a record for <code>mail._domainkey.example.com</code>.</p>
<p>This will generate two files. The first is <code>mail.private</code>, which we will move to <code>/etc/opendkim/keys/example.com.private</code> (to match the <code>KeyFile</code> later). Also, we need to make sure that the file has correctly narrow Unix permissions, or OpenDKIM will refuse to use it (for our own safety):</p>
<pre class="bash"><code>sudo chmod 0400 /etc/opendkim/keys/example.com.private
sudo chown opendkim:opendkim /etc/opendkim/keys/example.com.private
sudo adduser postfix opendkim</code></pre>
<p>The second file is <code>mail.txt</code>, which is a DNS record for the above domain. Set it up so this returns successfully:</p>
<pre class="bash"><code>$ dig mail._domainkey.example.com TXT

...
;; ANSWER SECTION:
mail._domainkey.example.com. 599	IN TXT "v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQC5N3lnvvrYgPCRSoqn+awTpE+iGYcKBPpo8HHbcFfCIIV10Hwo4PhCoGZSaKVHOjDm4yefKXhQjM7iKzEPuBatE7O47hAx1CJpNuIdLxhILSbEmbMxJrJAG0HZVn8z6EAoOHZNaPHmK2h4UUrjOG8zA5BHfzJf7tGwI+K619fFUwIDAQAB" ; ----- DKIM key mail for example.com
...</code></pre>
<p>That's (almost) all we have to do to configure the public half of the key pair. (We still have to use the same selector later in the <code>KeyFile</code>, but that's it.)</p>
<h4>4 - Create an OpenDKIM configuration file (<code>/etc/opendkim.conf</code>)</h4>
<pre class="bash"><code>Canonicalization        relaxed/relaxed
ExternalIgnoreList      refile:/etc/opendkim/TrustedHosts
InternalHosts           refile:/etc/opendkim/TrustedHosts
KeyTable                refile:/etc/opendkim/KeyTable
LogWhy                  Yes
MinimumKeyBits          1024
Mode                    sv
PidFile                 /var/run/opendkim/opendkim.pid
SigningTable            refile:/etc/opendkim/SigningTable
Socket                  inet:8891@localhost
Syslog                  Yes
SyslogSuccess           Yes
UMask                   022
UserID                  opendkim:opendkim</code></pre>
<p>What in the world do those all mean? Well, (based somewhat on documentation and somewhat on expirimentation):</p>
<ul>
    <li><code>Canonicalization</code> - controls whether further mail servers can edit the message contents with destroying the signature. <code>relaxed</code> allows more modification, <code>simple</code> is more strict. <code>relaxed</code> seems to be the more common option.</li>
    <li><code>ExternalIgnoreList</code> - hosts that we trust to send us mail; do not validate their DKIM signatures (if present). Specifying <code>refile</code> means that we can use wildcards.</li>
    <li><code>InternalHosts</code> - hosts that will relay mail through us; if we see an unsigned message coming from one of them, sign it before forwarding it along (This is important! I'll get to why in a bit)</li>
    <li><code>KeyTable</code> - a list of private keys we can use to sign messages (included later)</li>
    <li><code>LogWhy</code> - log errors to <code>/var/log/mail.log</code> (useful for debugging)</li>
    <li><code>MinimumKeyBits</code> - flag an error if we try to specify a private key shorter (less secure) than this</li>
    <li><code>Mode</code> - various options, <code>sv</code> is fairly common and means <code>s</code>ign outgoing messages and <code>v</code>erify incoming ones</li>
    <li><code>PidFile</code> - where to store the <a href="https://en.wikipedia.org/wiki/PID_file">PID file</a></li>
    <li><code>SigningTable</code> - specify which key from the <code>KeyTable</code> should be used for a given message</li>
    <li><code>Socket</code> - how Postfix and OpenDKIM communicate</li>
    <li><code>Syslog</code>, <code>SyslogSuccess</code> - log to <a href="https://en.wikipedia.org/wiki/syslog">syslog</a> as well as <code>mail.log</code>; log successes as well as failures</li>
    <li><code>UMask</code> - allows other Linux users (such as Postfix's) to talk to OpenDKIM</li>
    <li><code>UserID</code> - the user that OpenDKIM runs as</li>
</ul>
<p>Fairly straight forward.</p>
<h4>5 - Next, another copy of the socket definition in <code>/etc/default/opendkim</code> (I'm actually not sure why this one is necessary)</h4>
<pre class="bash"><code>SOCKET="inet:8891@localhost"</code></pre>
<p>Next, specify our keys in <code>/etc/opendkim/KeyTable</code>:</p>
<pre class="bash"><code>example.com example.com:mail:/etc/opendkim/keys/example.com.private</code></pre>
<p>The first entry is a name for the key. It can be anything and doesn't necessary have to match the domain, just so long as the corresponding entry in <code>SigningTable</code> matches. After that, you have the domain where your public key is hosted, the selector on that domain, and where the private key is locally located. So in the above example, <code>example.com:mail</code> corresponds to a key at <code>mail._domainkey.example.com</code>.</p>
<p>And what emails to sign in <code>/etc/opendkim/SigningTable</code>:</p>
<pre class="bash"><code>*@example.com example.com
*@*.example.com example.com
*@*.example.org example.com</code></pre>
<p>This one is a bit more complicated, since for my particular case, I had emails coming from two different domains, along with subdomains in both cases (the first field). They can (and in this case) all use the same key, since the second entry matches the value specified in <code>KeyTable</code>.</p>
<p>Note again: Since we're using wildcards here, we have to specify <code>refile</code> up above in <code>opendkim.conf</code>.</p>
<h4>6 - Next, <code>/etc/opendkim/TrustedHosts</code></h4>
<p>This file will be used to specify both incoming message we trust and outgoing message we will sign (although it doesn't have to do both).</p>
<pre class="bash"><code>127.0.0.1
10.77.0.0/16
example.com
*.example.com
*.example.org
*.*.example.org</code></pre>
<p>Entries here can be either IP addresses, <a href="https://en.wikipedia.org/wiki/CIDR">CIDR</a> style IP ranges, or hostnames (including wildcards, since this is a <code>refile</code>), all of which I use above. This actually took a bit to figure out, since originally I was signing email directly from the box (successfully), but when I attempted to actually send a signed email from the product, it didn't work (since the frontends relay mail to the mail servers).</p>
<h4>7 - Okay, that's enough to configure OpenDKIM. Next, we need to tell Postfix to talk to it</h4>
<p>This one is relatively straight forward as well. Just add a few lines to the bottom of <code>/etc/postfix/main.cf</code>:</p>
<pre class="bash"><code># DKIM
milter_default_action = accept
milter_protocol = 2
smtpd_milters = inet:localhost:8891
non_smtpd_milters = inet:localhost:8891</code></pre>
<p>Essentially, milter is a protocol Postfix uses for plugins. We want to configure all mail traffic (both from smptd and not) to go to OpenDKIM, via the port we specified (twice) earlier. Shiny.</p>
<h4>8 - Finally, restart both OpenDKIM and Postfix, so they can take advantage of their new settings</h4>
<pre class="bash"><code>sudo service opendkim restart
sudo service postfix restart</code></pre>
<p>This should only take a few seconds.</p>
<p>And that's it. We can send a test email:</p>
<pre class="bash"><code>echo "test email" | mail -s &lt;code&gt;hostname&lt;/code&gt; me@example.com</code></pre>
<p>That will send an email with the <code>hostname</code> of the current machine to the specified address, useful if you're working with multiple different machines and need to know which are up to date.</p>
<p>Check the message headers and you should see a block that looks like this:</p>
<pre class="bash"><code>DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=example.com; s=mail;
        t=1439063676; bh=RnRlt9pNo9HfghopMAD1V157IZOFVrE6piv9xdXYFNs=;
        h=To:Subject:Reply-To:From:Date;
        b=LekoRRQgOX97WUHP/ELtl/yhMzZsiCPr8kqaYUpZER5sYZ1dyAwgOKKvuI3mL3IMo
         4Y90NDGv+CHm2ZmAaGmFOgGnDPsDxLE+ptleVBP/cQny9grftwA8Emc3MKS6aJ/w5P
         E1bh2wFE8LiRTl/wcof6JL0MyeoR8R63FCKMgnA8=</code></pre>
<p>Bam.</p>
<h2>A few caveats</h2>
<p>So, what is DKIM actually used for?</p>
<p>The original claim is that it's an <em>email validation system designed to detect email spoofing</em>. Which is all well and good, but there's one big problem: adaptation.</p>
<p>Numbers are a little hard to come by, but one site that I found is builtWith trends: <a href="https://trends.builtwith.com/mx/DKIM">DKIM Usage Statistics</a>. Their reported coverage notes:</p>
<ul>
    <li>Quantcast Top 10k - 14 of 10,000</li>
    <li>Quantcast Top 100k - 104 of 100,000</li>
    <li>Quantcast Top Million - 585 of 865,105</li>
    <li>Entire Internet - 24,064 of 328,844,222</li>
</ul>
<p>That's less than 0.1% in any category. Not so great.</p>
<p>This is a problem, not because it means that mail servers aren't using it, but rather because there is little reason for mail <em>clients</em> to support it. It's easy enough to verify a DKIM signature if present, but they're so rarely present, that most will not go through that effort.</p>
<p>Furthmore, the header does not sign the message headers and (unless you are using SMTP over TLS), it is trivial to remove. If the DKIM header is not present, the message is trivial to modify with the recipient none the wiser. This could be offset--for some business models--by requiring messages to contain a DKIM header and rejecting those that don't.</p>
<p>Still, it's an interesting technology and there's no particular harm (other than a small amount of extra CPU effort to do the signing) in implementing it.</p>]]></content></entry><entry><title>Laundry Files</title><link href="http://blog.jverkamp.com/2015/07/28/laundry-files" /><id>urn:uuid:be3e3e1e-8aa3-4d98-27ca-8ac00ecce18d</id><updated>2015-07-28T00:00:00Z</updated><summary type="html"><![CDATA[<p><a href="https://www.goodreads.com/book/show/101869.The_Atrocity_Archives"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/1-the-atrocity-archives.jpg" /></a> <a href="https://www.goodreads.com/book/show/14150.The_Jennifer_Morgue"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/2-the-jennifer-morgue.jpg" /></a> <a href="https://www.goodreads.com/book/show/7149287-the-fuller-memorandum"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/3-the-fuller-memorandum.jpg" /></a> <a href="https://www.goodreads.com/book/show/12393566-the-apocalypse-codex"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/4-the-apocalypse-codex.jpg" /></a> <a href="https://www.goodreads.com/book/show/18211295-the-rhesus-chart"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/5-the-rhesus-chart.jpg" /></a> <a href="https://www.goodreads.com/book/show/23154785-the-annihilation-score"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/6-the-annihilation-score.jpg" /></a></p>
<p>Next up on my <a href="http://blog.jverkamp.com/2015/01/01/2015-reading-list">2015 Reading List</a>: <a href="https://www.goodreads.com/series/50764-laundry-files">The Laundry Files</a> by <a href="https://www.goodreads.com/author/show/8794.Charles_Stross">Charles Stross</a>. Even though this wasn't originally on my list, I ended up just reading this rather than reading two series at once. So it goes.</p>
]]></summary><content type="html"><![CDATA[<p><a href="https://www.goodreads.com/book/show/101869.The_Atrocity_Archives"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/1-the-atrocity-archives.jpg" /></a> <a href="https://www.goodreads.com/book/show/14150.The_Jennifer_Morgue"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/2-the-jennifer-morgue.jpg" /></a> <a href="https://www.goodreads.com/book/show/7149287-the-fuller-memorandum"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/3-the-fuller-memorandum.jpg" /></a> <a href="https://www.goodreads.com/book/show/12393566-the-apocalypse-codex"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/4-the-apocalypse-codex.jpg" /></a> <a href="https://www.goodreads.com/book/show/18211295-the-rhesus-chart"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/5-the-rhesus-chart.jpg" /></a> <a href="https://www.goodreads.com/book/show/23154785-the-annihilation-score"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/6-the-annihilation-score.jpg" /></a></p>
<p>Next up on my <a href="http://blog.jverkamp.com/2015/01/01/2015-reading-list">2015 Reading List</a>: <a href="https://www.goodreads.com/series/50764-laundry-files">The Laundry Files</a> by <a href="https://www.goodreads.com/author/show/8794.Charles_Stross">Charles Stross</a>. Even though this wasn't originally on my list, I ended up just reading this rather than reading two series at once. So it goes.</p>
<!--more-->
<p>Basic premise: Cthulhian beasties live at the bottom of fractals. Higher mathematics is roughly equivalent to magic. Computers make math/magic far easier to perform.</p>
<p>It's a really cool premise and I'm glad that the series was recommended to me. I've greatly enjoyed following the exploits of Bob, computer programmer turned computational demonologist turned necromancer and his wife Mo: combat violinist.</p>
<p>Fair warning though: Stross takes tropes (such as computer programming, spy stuff, or the <a href="https://en.wikipedia.org/wiki/Scrum_(software development)">scrum method</a>) and turns them up to 11. From time to time, the terminology gets a bit annoying (especially if you're not as familiar with the various fields he's writing about), but personally, I love it.</p>
<p>Now, the same as <a href="http://blog.jverkamp.com/2015/06/21/rama">Rama</a>, my individual reviews as I read each book:</p>
<hr />
<p><a href="https://www.goodreads.com/book/show/101869.The_Atrocity_Archives"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/1-the-atrocity-archives.jpg" /></a></p>
<p>That... was intense.</p>
<p>To really get the most out of this book, I think you'd have to have a certain combination of things in your head: a knowledge of a lot of the more esoteric bits of computer science theory, physics, pop culture, and cosmic horrors. Pretty much right up my alley. Even then, I will admit to having no idea exactly where he was going with a few of those.</p>
<p>It's entirely possible that you'll still like it, even without the same base knowledge. It's still a quick enough read, with enough action to keep you turning the page, even if some of the technical info dumps are probably going to get skipped.</p>
<p>The last bit spent in the other world was fascinating. From a purely academic / world building standpoint, I would love to be able to see how such an alternate universe would deal with the sun going out and the air seeping away. How long would people survive? How would they deal with the beastie making it all happen?</p>
<p>After this, I am definitely going to read the rest of the Laundry Files.</p>
<hr />
<p><a href="https://www.goodreads.com/book/show/14150.The_Jennifer_Morgue"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/2-the-jennifer-morgue.jpg" /></a></p>
<p>This is an excellent follow up to The Atrocity Archives.</p>
<p>On the plus side, Stross managed to tone down the technobabble from ridiculous to only mildly over the top. There's still more than enough references to enjoy and enough neat world building on the (non Euclidean) edges between technology and demonology, but it's not quite hitting you over the head like the first one did.</p>
<p>On the other hand, what he gave up in technobabble, he replaced with spybabble. There are are strong hints of spy novel trappings all over the book. Let's just say though, they are there for a good reason.</p>
<p>One neat addition to this book (although I don't know if she'll be back) is the character of Ramona Random. (view spoiler) (which leads to some more distinctly R rated scenes than the first book had), but I thought she was a really interesting character. Despite--or perhaps because of--her rather problematic backstory, I really started to root for her throughout the book.</p>
<p>Also Mo. She's something of a badass now, taking on the bigwigs both in the Laundry and in the real world. Her violin is an especially interesting bit of world building. I especially love her part towards the end even if I'll admit that I didn't see the twist coming as soon as I could have. I do hope they do more with her in the latter books.</p>
<hr />
<p><a href="https://www.goodreads.com/book/show/7149287-the-fuller-memorandum"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/3-the-fuller-memorandum.jpg" /></a></p>
<p>(Take 2, the internet ate my first review)</p>
<p>This is another solid entry in the Laundry Files and perhaps my favorite yet. We're really starting to get into some of the more horrific corners of the world. In particular, we learn a bit more about Mo's violin; we get another glimpse into a far flung world (view spoiler); and more about Angleton than I bet Bob ever wanted to know (and I really is explored more in the next books).</p>
<p>On the down side, Bob and Mo continue to have to do some fairly terrible things. There's some fallout from that in this book, but I feel like they just sort of skate by it without really dealing with what's going on. That's either going to come up in a later book or it's going to continue to get weirder when it doesn't.</p>
<p>The writing still pulls you along kicking and screaming. From the halfway point on, and in particular the ending, I couldn't put it down. There are more odd first/third person switches (Bob speaks in first person, everyone else is third person) and in particular a few chapters where I assume the intention is to hide exactly who is speaking. All together, it requires a bit more mental bandwidth to read than the previous two, but it's still worth it.</p>
<p>Looking forward, there are still two novels with Bob as the main character. There are a few things that I really want to see. Fingers crossed.</p>
<hr />
<p><a href="https://www.goodreads.com/book/show/12393566-the-apocalypse-codex"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/4-the-apocalypse-codex.jpg" /></a></p>
<p>The first chunk of this book was a little bit weak, in particularly the introduction of the new character Ms. Hazard. She represents a different style to the magic in this universe, which is interesting, but she just seems too good at her job. In the same manner as Superman, the more powerful someone becomes, the harder it is to make them interesting.</p>
<p>That does get better in the later half of the book, especially in the climax. I'm still not sure what to think about the mixed first/third person writing style, but it is interesting to see some of the scenes from two very different points of view. I miss Mo though.</p>
<p>So far as world building goes, Stross continues to build out the more interesting parts of the world, building towards an eventual apocalypse (or at least rather bad day). The visuals of the Sleeper and the Pyramid are still some of my favorite parts of these books, especially the idea that the wall from the previous books is designed to keep people out, not in. I hadn't gotten that impression before.</p>
<hr />
<p><a href="https://www.goodreads.com/book/show/18211295-the-rhesus-chart"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/5-the-rhesus-chart.jpg" /></a></p>
<p>Vampires don't exist.</p>
<p>Or at least that's what everyone in the Laundry believes. Turns out... there's a good reason for that.</p>
<p>From the perspective of someone in software development, the maddening extent at which the Scrum operates is amusing and all too close to home. After twisting first computer science and spy stuff into this world, it was neat to see the new topic.</p>
<p>My one real complaint with this book is that I think it's out of order. It's significantly smaller in scope than the previous two books where I was hoping for even more escalation towards CASE NIGHTMARE GREEN, rather than something somewhat more down to home. Although on the other hand, a few of the events in this book need the previous books to make sense. Still it's weird.</p>
<p>Now, sadly, I've finished the Bob focused Laundry Files novels released to date. So it goes. I look forward to the next.</p>
<hr />
<p><a href="https://www.goodreads.com/book/show/23154785-the-annihilation-score"><img src="http://blog.jverkamp.com/2015/07/28/laundry-files/6-the-annihilation-score.jpg" /></a></p>
<p>The Annihilation Score takes a bit of a different take from the previous five books, shifting the viewpoint from the previous hero* Bob to his wife Mo.</p>
<p>Previously, Mo had been one of my favorite characters in the series. She's just mysterious enough that she's interesting, coming in to save the day with a truly terrifying violin. Unfortunately, the more I know about her, the less I care.</p>
<p>The basic idea is interesting enough. Basically, as the world careens towards madness, magic is becoming more prevalent. But people don't believe in magic anymore, they believe in superheroes. One thing leads to another and Mo ends up leading a mostly publicly known super-hero task force. Much as The Rhesus Chart, despite the incoming danger, this feels like a de-escalation from the first four books.</p>
<p>One problem that annoyed me throughout the book was the relationship between Bob and Mo. I get it, they've been through Hell (literally) and they're each dealing with their own demons (again, literally), but I don't really understand their reactions to it. It feels more like Stross needed to get Bob out of the picture. So it goes.</p>
<p>Overall, I miss Bob. I miss Mo as a secondary character. I still enjoyed this more than enough to continue the series when the next comes out.</p>
<p>* For some definitions of hero.</p>
<hr />
<p>Amusingly, I haven't actually read that many ongoing series this year. If/when the next book comes out, I'll probably update this post.</p>
<p>Up next (for real this time): <a href="https://www.goodreads.com/book/show/337048.The_Engines_of_God">The Engines of God</a> by <a href="https://www.goodreads.com/author/show/73812.Jack_McDevitt">Jack McDevitt</a>. Onwards!</p>]]></content></entry><entry><title>365</title><link href="http://blog.jverkamp.com/2015/07/27/365" /><id>urn:uuid:46af348c-3551-4c68-0861-7b6cb3b7952d</id><updated>2015-07-27T00:00:00Z</updated><summary type="html"><![CDATA[<p>I more or less randomly decided to start uploading a picture a day. We'll see how it goes. <img alt="smile" class="emoji" src="/emoji/smile.svg" /></p>
<div><div class="flickr-gallery" data-set-id="72157654714216078" data-per-page="30"></div><p><a href="https://flickr.com/photos/jpverkamp/sets/72157654714216078">View on Flickr</a></p></div>
<p>This post should update automatically as I add more pictures to this set.</p>]]></summary><content type="html"><![CDATA[<p>I more or less randomly decided to start uploading a picture a day. We'll see how it goes. <img alt="smile" class="emoji" src="/emoji/smile.svg" /></p>
<div><div class="flickr-gallery" data-set-id="72157654714216078" data-per-page="30"></div><p><a href="https://flickr.com/photos/jpverkamp/sets/72157654714216078">View on Flickr</a></p></div>
<p>This post should update automatically as I add more pictures to this set.</p>]]></content></entry><entry><title>Finding AWS IAM users by access key</title><link href="http://blog.jverkamp.com/2015/07/22/finding-aws-iam-users-by-access-key" /><id>urn:uuid:4b3770fb-d5b2-cf75-6c83-24a7895c99fa</id><updated>2015-07-22T00:00:00Z</updated><summary type="html"><![CDATA[<p>Every once in a while<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>, I find myself with an <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSGettingStartedGuide/AWSCredentials.html">AWS access key</a> and need to figure out who in the world it belongs to. Unfortunately, so far as I've been able to find, there's no way to directly do this in either the <a href="https://aws.amazon.com/console/">AWS console</a> or with the <a href="https://aws.amazon.com/cli/">AWS api</a>.</p>
]]></summary><content type="html"><![CDATA[<p>Every once in a while<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span>, I find myself with an <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSGettingStartedGuide/AWSCredentials.html">AWS access key</a> and need to figure out who in the world it belongs to. Unfortunately, so far as I've been able to find, there's no way to directly do this in either the <a href="https://aws.amazon.com/console/">AWS console</a> or with the <a href="https://aws.amazon.com/cli/">AWS api</a>.</p>
<!--more-->
<p>Luckily, <a href="https://aws.amazon.com/cli/">boto</a>:</p>
<pre class="python"><code>#!/usr/bin/env python3

import boto.iam.connection
import pprint
import sys

if len(sys.argv) == 1:
    print('Usage: who-iam [access-key ...]')
    sys.exit(0)

conn = boto.iam.connection.IAMConnection()
users = conn.get_all_users()

for user in users['list_users_response']['list_users_result']['users']:
    keys = conn.get_all_access_keys(user['user_name'])
    for key in keys['list_access_keys_response']['list_access_keys_result']['access_key_metadata']:
        for target in sys.argv[1:]:
            if key['access_key_id'] == target:
                print(key['access_key_id'], user['user_name'])</code></pre>
<p>Check it out (keys changed on the off chance that actually matters):</p>
<pre class="bash"><code>$ who-iam AKIAIOSWISKB6EXAMPLE AKIAIOSGWISN7EXAMPLE
AKIAIOSWISKB6EXAMPLE luke
AKIAIOSGWISN7EXAMPLE han</code></pre>
<p>It's rather slow (since it has to make <code>O(n)</code> requests and doesn't short circuit), but this should be something you do rarely enough that it doesn't matter.</p>
<p>If you'd like to download the script, it's available in my <a href="https://github.com/jpverkamp/dotfiles">dotfiles</a>: <a href="https://github.com/jpverkamp/dotfiles/blob/master/bin/who-iam">who-iam</a></p>]]></content></entry><entry><title>Configuring Websockets behind an AWS ELB</title><link href="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb" /><id>urn:uuid:7dcd0d64-5bf4-e7c8-1c43-b21d96bd35e0</id><updated>2015-07-20T00:00:00Z</updated><summary type="html"><![CDATA[<p>Recently at work, we were trying to get an application that uses <a href="https://en.wikipedia.org/wiki/websockets">websockets</a> working on an <a href="https://aws.amazon.com/">AWS</a> instance behind an <a href="https://aws.amazon.com/elasticloadbalancing/">ELB (load balancer)</a> and <a href="http://nginx.org/">nginx</a> on the instance.</p>
<p>If you're either not using a secure connection or handling the cryptography on the instance (either in nginx or Flask), it works right out of the box. But if you want the ELB to handle TLS termination it doesn't work nearly as well... Luckily, after a bit of fiddling, I got it working.</p>
]]></summary><content type="html"><![CDATA[<p>Recently at work, we were trying to get an application that uses <a href="https://en.wikipedia.org/wiki/websockets">websockets</a> working on an <a href="https://aws.amazon.com/">AWS</a> instance behind an <a href="https://aws.amazon.com/elasticloadbalancing/">ELB (load balancer)</a> and <a href="http://nginx.org/">nginx</a> on the instance.</p>
<p>If you're either not using a secure connection or handling the cryptography on the instance (either in nginx or Flask), it works right out of the box. But if you want the ELB to handle TLS termination it doesn't work nearly as well... Luckily, after a bit of fiddling, I got it working.</p>
<!--more-->
<p>First, we have a basic application. For my purposes, I wrote a quick Websocket chat app: <a href="https://github.com/jpverkamp/ws-chat">ws-chat</a>. The particular implementation details aren't as important. We'll start with the nginx config file:</p>
<pre class="nginx"><code>upstream webserver {
    server 127.0.0.1:8000;
}

upstream wsserver {
    server 127.0.0.1:9000;
}

server {
    listen 80 proxy_protocol;

    location / {
        if ($http_x_forwarded_proto = "http") {
            return 301 https://$host$request_uri;
        }

        proxy_pass http://webserver;
    }

    location /ws/ {
        proxy_pass http://wsserver;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}</code></pre>
<p>Straight forward enough. We have two backend services: a <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/web-server.py">web server</a> running on port 8000 (a simple Flask server that just servers a single <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/templates/index.html">HTML page</a>) and the <a href="https://github.com/jpverkamp/ws-chat/blob/master/app/ws-server.py">websocket backend</a> running on port 9000. Alternatively, these could be the same codebase. The important parts are that you allow the Websocket <code>upgrade</code> header to pass through to establish the connection and that you tell nginx to listen using the <code>proxy_protocol</code>, an extra header that passes through extra information:</p>
<pre class="text"><code>PROXY_STRING + single space + INET_PROTOCOL + single space + CLIENT_IP + single space + PROXY_IP + single space + CLIENT_PORT + single space + PROXY_PORT + "\r\n"</code></pre>
<p>This seems like it wouldn't be necessary, except that without <code>proxy_protocol</code> AWS ELBs seem to strip something important to the connection.</p>
<p>Next, we need to configure the load balancer. One complication here is that telling the load balancer to forward HTTPS traffic to HTTP will not work for the websockets. Instead, you have to configure it to forward TCP (SSL) to TCP. This will still work for HTTP/HTTPS traffic (as HTTP is just a specific protocol over TCP and HTTPS is just HTTP with a TLS layer), but it will also allow the non-HTTP websocket traffic to pass through successfully. Something like this:</p>
<p><a href="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb/configure-elb.png" data-toggle="lightbox"><img src="http://blog.jverkamp.com/2015/07/20/configuring-websockets-behind-an-aws-elb/configure-elb.png" /></a></p>
<p>(Don't forget to set the certificate :))</p>
<p>Finally, you have to configure the ELB also to speak proxy protocol. This part is slightly more annoying, since (at least now), there's no way to configure this through the AWS console. You have to use the <a href="https://aws.amazon.com/cli/">AWS CLI</a>.</p>
<p>First, create the new policy (assuming you have an environment variable <code>ELB_NAME</code> defined):</p>
<pre class="bash"><code>aws elb create-load-balancer-policy \
    --load-balancer-name $ELB_NAME \
    --policy-name $ELB_NAME-proxy-protocol \
    --policy-type-name ProxyProtocolPolicyType \\
    --policy-attributes AttributeName=ProxyProtocol,AttributeValue=True</code></pre>
<p>Then, attach it to the load balancer. You will have to run this once for each port that the instance is listening on:</p>
<pre class="bash"><code>aws elb set-load-balancer-policies-for-backend-server \
    --load-balancer-name $ELB_NAME \
    --instance-port 80 \
    --policy-names $ELB_NAME-proxy-protocol</code></pre>
<p>Make sure that you're using <code>https://</code> for the web traffic and <code>wss://</code> for the websocket and you're golden. Encrypted websockets behind an AWS ELB. Now if only they would expose the proxy protocol options in the console...</p>]]></content></entry><entry><title>Pacifica II</title><link href="http://blog.jverkamp.com/2015/07/19/pacifica-ii" /><id>urn:uuid:383c47d0-4a3c-abcc-e80d-8b6da43ee4d0</id><updated>2015-07-19T00:00:00Z</updated><summary type="html"><![CDATA[<p>Another trip to Pacifica towards the end of the day. I had a bit of fun with a polarizing filter and HDR shots. Take a wild guess which shots those were. :)</p>
<div><div class="flickr-gallery" data-set-id="72157655694180549" data-per-page="30"></div><p><a href="https://flickr.com/photos/jpverkamp/sets/72157655694180549">View on Flickr</a></p></div>]]></summary><content type="html"><![CDATA[<p>Another trip to Pacifica towards the end of the day. I had a bit of fun with a polarizing filter and HDR shots. Take a wild guess which shots those were. :)</p>
<div><div class="flickr-gallery" data-set-id="72157655694180549" data-per-page="30"></div><p><a href="https://flickr.com/photos/jpverkamp/sets/72157655694180549">View on Flickr</a></p></div>]]></content></entry><entry><title>Automagically storing Python objects in Redis</title><link href="http://blog.jverkamp.com/2015/07/16/automagically-storing-python-objects-in-redis" /><id>urn:uuid:e83d21ba-8555-bfba-b05f-ceefc0808723</id><updated>2015-07-16T00:00:00Z</updated><summary type="html"><![CDATA[<p>When you're starting out on a simple web application, eventually<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span> you will reach the point where you need to store some form of persistant data. Basically, you have three options<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>:</p>
<ul>
    <li>Store the information in flat files on the file system</li>
    <li>Store the information in a database (<a href="https://www.mysql.com/">MySQL</a>, <a href="https://www.sqlite.org/">SQLite</a> etc)</li>
    <li>Store the information in a key/value store (<a href="https://www.mongodb.org/">mongoDB</a>, <a href="http://redis.io/">reddis</a>)</li>
</ul>
<p>There are all manner of pros and cons to each, in particular how easy they are to get started in, how well they fit the data you are using, and how well they will scale horizontally (adding more machines rather than bigger ones).</p>
]]></summary><content type="html"><![CDATA[<p>When you're starting out on a simple web application, eventually<span class="footnote"><sup><a href="#footnote-1">[1]</a></sup></span> you will reach the point where you need to store some form of persistant data. Basically, you have three options<span class="footnote"><sup><a href="#footnote-2">[2]</a></sup></span>:</p>
<ul>
    <li>Store the information in flat files on the file system</li>
    <li>Store the information in a database (<a href="https://www.mysql.com/">MySQL</a>, <a href="https://www.sqlite.org/">SQLite</a> etc)</li>
    <li>Store the information in a key/value store (<a href="https://www.mongodb.org/">mongoDB</a>, <a href="http://redis.io/">reddis</a>)</li>
</ul>
<p>There are all manner of pros and cons to each, in particular how easy they are to get started in, how well they fit the data you are using, and how well they will scale horizontally (adding more machines rather than bigger ones).</p>
<!--more-->
<p>For the project that I was working on (I'll post about it eventually), I didn't have terribly many different kinds of data to store, so it would be easy enough to start with anything. I started with a simple file system backing, with one json file per object that I was storing. That worked well enough, but I didn't particularly care for having to write all of the code myself to join / find child objects. I wanted something a little more powerful.</p>
<p>Next, I considered using a database with an <a href="https://en.wikipedia.org/wiki/Object-relational_mapping">ORM</a> layer. That would let me define everything as Python objects and let the library handle all of the mappings to the actual database. That way, I could write a bare minimum of code for my actual models. Unfortunately, the data wasn't particularly well structured for a relational database, being more hierarchical in struture. It's entirely possible to represent hierarchical data in a relational database, it's just not what they are best suited for.</p>
<p>Finally, I came to Redis. I've used Redis in a few projects at work and come to really like it. It works great as a simple key/value store and even better when you start taking advatage of some of their other data structures. In particular, Redis hashes and lists map nicely to Python dicts and lists. So that's what I ended up doing: Writing a pair of base classes (<code>RedisDict</code> and <code>RedisList</code>) which to the programmer act just like a Python dict or list, but actually store all of their data transparently in Redis.</p>
<p>Let's get to it.</p>
<p>First, there is a bit of shared code that both <code>RedisDict</code> and <code>RedisList</code> share, which we can factor out into a base class for the two of them: <code>RedisObject</code>.</p>
<pre class="python"><code>import base64
import json
import redis
import os

class RedisObject(object):
    '''
    A base object backed by redis.
    Genrally, use RedisDict or RedisList rather than this directly.
    '''

    def __init__(self, id = None):
        '''Create or load a RedisObject.'''

        self.redis = redis.StrictRedis(host = 'redis', decode_responses = True)

        if id:
            self.id = id
        else:
            self.id = base64.urlsafe_b64encode(os.urandom(9)).decode('utf-8')

        if ':' not in self.id:
            self.id = self.__class__.__name__ + ':' + self.id

    def __bool__(self):
        '''Test if an object currently exists'''

        return self.redis.exists(self.id)

    def __eq__(self, other):
        '''Tests if two redis objects are equal (they have the same key)'''

        return self.id == other.id

    def __str__(self):
        '''Return this object as a string for testing purposes.'''

        return self.id

    def delete(self):
        '''Delete this object from redis'''

        self.redis.delete(self.id)

    @staticmethod
    def decode_value(type, value):
        '''Decode a value if it is non-None, otherwise, decode with no arguments.'''

        if value == None:
            return type()
        else:
            return type(value)

    @staticmethod
    def encode_value(value):
        '''Encode a value using json.dumps, with default = str'''

        return str(value)</code></pre>
<p>Most of that should be pretty straight forward. The basic idea is that any <code>RedisObject</code>, be it a <code>RedisDict</code> or a <code>RedisList</code> has an ID. This will be used as the key that Redis stores the object under. In particular, I've used a neat (in my opinion) trick to generate random alphanumeric identifiers:</p>
<pre class="python"><code>&gt;&gt;&gt; base64.urlsafe_b64encode(os.urandom(9)).decode('utf-8')
u'UQTfwq8XLZr6'</code></pre>
<p>Neat. Alternatively, if you want to use a specific value for an ID (such as a user's email address), you can just specify that instead. Next, the <code>__bool__</code> method will make a <code>RedisObject</code> 'truthy'. You can use Python's <code>if</code> to tell if an object acutally exists or not. Finally, <code>delete</code>. I wanted to use <code>__del__</code> originally, but that actually gets called when an object is garbage collected, which doesn't quite work for this usage<span class="footnote"><sup><a href="#footnote-3">[3]</a></sup></span></p>
<p>Finally, static helper functions <code>decode_value</code> and <code>encode_value</code>. These will be used in a bit, since Redis only stores strings. Thus a <code>RedisObject</code> stores the type of each value and needs to know how to read/write that in a systematic way. For that, I'm using Python's <code>json</code> encoding, falling back to <code>str</code> (and thus the <code>__str__</code> magic function on objects). This will deal nicely with most default Python objects and can be easily extended for all manner of more interesting ones if you'd like (I've done it for <code>RedisObject</code>s).</p>
<p>One oddity that you've probably noticed is that I've hard coded the Reids IP to connect to. I'm using <a href="https://github.com/docker/compose">docker-compose</a> to run my project, which sets up hostnames automagically within the various containers.</p>
<p>Next, <code>RedisDict</code>:</p>
<pre class="python"><code>import json
import redis

from lib.RedisObject import RedisObject

class RedisDict(RedisObject):
    '''An equivalent to dict where all keys/values are stored in Redis.'''

    def __init__(self, id = None, fields = {}, defaults = None):
        '''
        Create a new RedisObject
        id: If specified, use this as the redis ID, otherwise generate a random ID.
        fields: A map of field name to construtor used to read values from redis.
            Objects will be written with json.dumps with default = str, so override __str__ for custom objects.
            This should generally be set by the subobject's constructor.
        defaults: A map of field name to values to store when constructing the object.
        '''

        RedisObject.__init__(self, id)

        self.fields = fields

        if defaults:
            for key, val in defaults.items():
                self[key] = val

    def __getitem__(self, key):
        '''
        Load a field from this redis object.
        Keys that were not specified in self.fields will raise an exception.
        Keys that have not been set (either in defaults or __setitem__) will return the default for their type (if set)
        '''

        if key == 'id':
            return self.id

        if not key in self.fields:
            raise KeyError('{} not found in {}'.format(key, self))

        return RedisObject.decode_value(self.fields[key], self.redis.hget(self.id, key))

    def __setitem__(self, key, val):
        '''
        Store a value in this redis object.
        Keys that were not specified in self.fields will raise an exception.
        Keys will be stored with json.dumps with a default of str, so override __str__ for custom objects.
        '''

        if not key in self.fields:
            raise KeyError('{} not found in {}'.format(key, self))

        self.redis.hset(self.id, key, RedisObject.encode_value(val))

    def __iter__(self):
        '''Return (key, val) pairs for all values stored in this RedisDict.'''

        yield ('id', self.id.rsplit(':', 1)[-1])

        for key in self.fields:</code></pre>
<p>Basically, there are three interesting parts to this code: <code>__init__</code> stores the fields that this object has (and should be set by the constructors in subclasses) and can also be used as as a constructor for new objects. <code>__get/setitem__</code> will load/store items via Redis. Given the <code>encode/decode_value</code> functions in <code>RedisObject</code>, this is actually really straight forward.</p>
<p>So how would you use something like this?</p>
<p>Here's is most of the <code>User</code> class from the project I am working on:</p>
<pre class="python"><code>import bcrypt

import lib
import models
import utils

class User(lib.RedisDict):
    '''A user. Duh.'''

    def __init__(self, id = None, email = None, **defaults):

        # Use email as id, if specified
        if email:
            id = email
            defaults['email'] = email

        lib.RedisDict.__init__(
            self,
            id = email,
            fields = {
                'name': str,
                'email': str,
                'password': str,
                'friends': lib.RedisList.as_child(self, 'friends', models.User),
            },
            defaults = defaults
        )

    def __setitem__(self, key, val):
        '''Override the behavior if user is trying to change the password'''

        if key == 'password':
            val = bcrypt.hashpw(
                val.encode('utf-8'),
                bcrypt.gensalt()
            ).decode('utf-8')

        lib.RedisDict.__setitem__(self, key, val)

    def verifyPassword(self, testPassword):
        '''Verify if a given password is correct'''

        hashedTestPassword = bcrypt.hashpw(
            testPassword.encode('utf-8'),
            self['password'].encode('utf-8')
        ).decode('utf-8')

        return hashedTestPassword == self['password']</code></pre>
<p>A <code>User</code> will have four fields: a <code>name</code>, an <code>email</code>, a <code>password</code>, and a list of <code>friends</code> (we'll get to how that works in a bit). Then, I've added some custom code to automatically store passwords using <a href="https://en.wikipedia.org/wiki/bcrypt">bcrypt</a><span class="footnote"><sup><a href="#footnote-4">[4]</a></sup></span>. You can use it just like you would a <code>dict</code>:</p>
<pre class="python"><code>&gt;&gt;&gt; han = User(
...     name = 'Luke Skywalker',
...     email = 'luke@rebel-alliance.io',
...     password = 'TheForce',
... )
...

&gt;&gt;&gt; print(luke['name'])
Luke Skywalker

&gt;&gt;&gt; luke.verifyPassword('password')
False

&gt;&gt;&gt; luke.verifyPassword('TheForce')
True

&gt;&gt;&gt; han = User(
...     name = 'Han Solo',
...     email = 'han@rebel-alliance.io',
...     password = 'LetTheWookieWin',
... )
...

&gt;&gt;&gt; luke['friends'].append(han)

&gt;&gt;&gt; han['friends'].append(luke)

&gt;&gt;&gt; print(luke['friends'][0]['name'])
'Han Solo'</code></pre>
<p>Then we can go into the <code>redis-cli</code> to verify that everything saved correctly:</p>
<pre class="bash"><code>127.0.0.1:6379&gt; keys *
1) "User:han@rebel-alliance.io:friends"
2) "User:han@rebel-alliance.io"
3) "User:luke@rebel-alliance.io:friends"
4) "User:luke@rebel-alliance.io"

127.0.0.1:6379&gt; hgetall User:luke@rebel-alliance.io
1) "name"
2) "Luke Skywalker"
3) "email"
4) "luke@rebel-alliance.io"
5) "password"
6) "$2b$12$XQ1zDvl5PLS6g.K64H27xewPQMnkELa3LvzFSyay8p9kz0XXHVOFq"

127.0.0.1:6379&gt; lrange User:luke@rebel-alliance.io:friends 0 -1
1) "User:han@rebel-alliance.io"</code></pre>
<p>There are two entires for each, since technically the <code>friends</code> list is a <code>RedisList</code>. Originally, I was storing these as JSON encoded lists, but as they got larger, this started to get a little unweildy.</p>
<p>Another plus is that since all of the objects are backed by Redis, you get automatic persistance. Stop Python completely, start it back up, and you can just load the same objects again (remember, for these objects, I'm using the <code>email</code> as the ID):</p>
<pre class="python"><code>&gt;&gt;&gt; luke = User('luke@rebel-alliance.io')

&gt;&gt;&gt; print(luke['friends'][0]['name'])
'Han Solo'</code></pre>
<p>Very cool.</p>
<p>So, speaking of <code>RedisList</code>, how does that work? Mostly the same as <code>RedisDict</code> (although I had a few more functions to implement):</p>
<pre class="python"><code>import json
import redis

from lib.RedisObject import RedisObject

class RedisList(RedisObject):
    '''An equivalent to list where all items are stored in Redis.'''

    def __init__(self, id = None, item_type = str, items = None):
        '''
        Create a new RedisList
        id: If specified, use this as the redis ID, otherwise generate a random ID.
        item_type: The constructor to use when reading items from redis.
        values: Default values to store during construction.
        '''

        RedisObject.__init__(self, id)

        self.item_type = item_type

        if items:
            for item in items:
                self.append(item)

    @classmethod
    def as_child(cls, parent, tag, item_type):
        '''Alternative callable constructor that instead defines this as a child object'''

        def helper(_ = None):
            return cls(parent.id + ':' + tag, item_type)

        return helper

    def __getitem__(self, index):
        '''
        Load an item by index where index is either an int or a slice
        Warning: this is O(n))
        '''

        if isinstance(index, slice):
            if slice.step != 1:
                raise NotImplemented('Cannot specify a step to a RedisObject slice')

            return [
                RedisObject.decode_value(self.item_type, el)
                for el in self.redis.lrange(self.id, slice.start, slice.end)
            ]
        else:
            return RedisObject.decode_value(self.item_type, self.redis.lindex(self.id, index))

    def __setitem__(self, index, val):
        '''Update an item by index
        Warning: this is O(n)
        '''

        self.redis.lset(self.id, index, RedisObject.encode_value(val))

    def __len__(self):
        '''Return the size of the list.'''

        return self.redis.llen(self.id)

    def __delitem__(self, index):
        '''Delete an item from a RedisList by index. (warning: this is O(n))'''

        self.redis.lset(self.id, index, '__DELETED__')
        self.redis.lrem(self.id, 1, '__DELETED__')

    def __iter__(self):
        '''Iterate over all items in this list.'''

        for el in self.redis.lrange(self.id, 0, -1):
            yield RedisObject.decode_value(self.item_type, el)

    def lpop(self):
        '''Remove and return a value from the left (low) end of the list.'''

        return RedisObject.decode_value(self.item_type, self.redis.lpop(self.id))

    def rpop(self):
        '''Remove a value from the right (high) end of the list.'''

        return RedisObject.decode_value(self.item_type, self.redis.rpop(self.id))

    def lpush(self, val):
        '''Add an item to the left (low) end of the list.'''

        self.redis.lpush(self.id, RedisObject.encode_value(val))

    def rpush(self, val):
        '''Add an item to the right (high) end of the list.'''

        self.redis.rpush(self.id, RedisObject.encode_value(val))

    def append(self, val):
        self.rpush(val)</code></pre>
<p>Basically, I'm mapping a lot of the default Python <code>list</code> functionality to Redis lists and vice versa<span class="footnote"><sup><a href="#footnote-5">[5]</a></sup></span>. It's a little odd and some things aren't as efficient as I'd like (you only get <code>O(1)</code> access to the beginning and end of the list), but so it goes. It works great, as you saw in the <code>friends</code> example above.</p>
<p>The one interesting function is <code>as_child</code>. Since either a <code>RedisDict</code> or a <code>RedisList</code> expect a 'constructor-like' function as the data type, I need something that will correctly store a <code>RedisList</code> inside of a <code>RedisDict</code> while generating a human readable ID (with <code>:friends</code> appended in the example above). I love <a href="https://en.wikipedia.org/wiki/higher_order functions">higher order functions</a>.</p>
<p>And... that's it. Eventually, I think I'll look into publishing this as a library to <code>pip</code> or the like. But since I've never done that before and this post is already a little on the long sice, we'll leave that for another day. All of the code is included in the post, so you can copy and paste it into your project if you'd like to try it out before I publish it. Once I have, I'll edit this post.</p>]]></content></entry><entry><title>Backing up Moves Data</title><link href="http://blog.jverkamp.com/2015/07/10/backing-up-moves-data" /><id>urn:uuid:2eed2118-1b61-a7ab-1b50-35fe8d9c2baa</id><updated>2015-07-10T00:00:00Z</updated><summary type="html"><![CDATA[<p>Another <a href="http://blog.jverkamp.com/category/programming/by-topic/backups">backup post</a>, this time I'm going to back up my data from the <a href="https://www.moves-app.com/">Moves App</a> (step counter + GPS tracker). Theoretically, it should be possible to get this same data from the app as part of my <a href="http://blog.jverkamp.com/category/programming/by-project/ios-backup">iOS Backup</a> series, but the data there is in a strange binary format. Much easier to use their API.</p>
]]></summary><content type="html"><![CDATA[<p>Another <a href="http://blog.jverkamp.com/category/programming/by-topic/backups">backup post</a>, this time I'm going to back up my data from the <a href="https://www.moves-app.com/">Moves App</a> (step counter + GPS tracker). Theoretically, it should be possible to get this same data from the app as part of my <a href="http://blog.jverkamp.com/category/programming/by-project/ios-backup">iOS Backup</a> series, but the data there is in a strange binary format. Much easier to use their API.</p>
<!--more-->
<p>The first step will be to make a few helper methods. As I often do with web scripts, I'll be using <a href="http://blog.jverkamp.com/category/programming/by-language/python">Python</a> and the excellent <a href="http://docs.python-requests.org/en/latest/">Requests</a> library. First things first, we have to get an <code>access_token</code> using an <a href="https://en.wikipedia.org/wiki/OAuth">OAuth</a> handshake. It's a little complicated since our app is designed to run from the command line, yet needs to interact with the user on initial set up, but luckily that only has to be done once:</p>
<pre class="python"><code># Request a new access token

if not 'access_token' in config:
    url = 'https://api.moves-app.com/oauth/v1/authorize?response_type=code&client_id={client_id}&scope={scope}'.format(
        client_id = config['client_id'],
        scope = 'activity location'
    )
    print('Opening URL in browser...')
    webbrowser.open(url)
    code = raw_input('Please follow prompts and enter code: ')

    response = requests.post('https://api.moves-app.com/oauth/v1/access_token?grant_type=authorization_code&code={code}&client_id={client_id}&client_secret={client_secret}&redirect_uri={redirect_uri}'.format(
        code = code,
        client_id = config['client_id'],
        client_secret = config['client_secret'],
        redirect_uri = 'http://localhost/',
    ))
    js = response.json()
    print(js)

    config['access_token'] = js['access_token']
    config['refresh_token'] = js['refresh_token']
    config['user_id'] = js['user_id']

    with open('config.yaml', 'w') as fout:
        yaml.safe_dump(config, fout, default_flow_style=False)</code></pre>
<p>Basically, we have to have two values to start the handshake: <code>client_id</code> and <code>client_secret</code>. I've put those in a separate file (<code>config.yaml</code>) so that we don't have secrets in a repository. From there, we make a request to a given endpoint (see above), which opens in a browser. The user then gets an eight digit code which they enter in the app on the phone, prompting the web browser in turn to redirect with a <code>code</code> parameter. This part is a little ugly and I could make it much nicer by running a temporary single endpoint server, but since this only needs to be done once, I didn't bother.</p>
<p>After that, we take the <code>code</code> we just got, along with the <code>client_id</code> and <code>client_secret</code> and get the initial <code>access_token</code> and a <code>refresh_token</code> we can periodically use to prove we're still the same person.</p>
<p>Next, a little bit of framework. We'll wrap the default <code>requests</code> object to automatically provide an <code>access_token</code> to any <code>GET</code> or <code>POST</code> requests I want to make to the API, now that I've gotten one:</p>
<pre class="python"><code>def makeMethod(f):
    def run(url, **kwargs):

        if 'access_token' in config:
            headers = {'Authorization': 'Bearer {access_token}'.format(access_token = config['access_token'])}
        else:
            headers = {}

        url = 'https://api.moves-app.com/api/1.1' + url.format(**kwargs)

        if 'data' in kwargs:
            return f(url, data = kwargs['data'], headers = headers)
        else:
            return f(url, headers = headers)

    return run

get = makeMethod(requests.get)
post = makeMethod(requests.post)</code></pre>
<p>With that, we can just always use that <code>refresh_token</code> we got above every time we run the script. This is definitely over kill, but it saves a little bit of logic telling when we have to refresh the code or not and doesn't really cost anything more than a single extra request:</p>
<pre class="python"><code># Perform a refresh on the access token just as a matter of course

response = requests.post('https://api.moves-app.com/oauth/v1/access_token', data = {
    'grant_type': 'refresh_token',
    'refresh_token': config['refresh_token'],
    'client_id': config['client_id'],
    'client_secret': config['client_secret']
})
js = response.json()

config['access_token'] = js['access_token']
config['refresh_token'] = js['refresh_token']
config['user_id'] = js['user_id']

with open('config.yaml', 'w') as fout:
    yaml.safe_dump(config, fout, default_flow_style=False)</code></pre>
<p>Next, fetch my user profile:</p>
<pre class="python"><code># Load the user profile to see how far back data goes

user_profile = get('/user/profile').json()</code></pre>
<p>The most interesting bit of information here is <code>.profile.firstDate</code>, which tells us when we first started using Moves. We can then loop from that date forward in time, grabbing any days we are missing. Since sometimes previous days aren't completely done processing the next morning, I'll also always re-download the last week's worth of data no matter what.</p>
<pre class="python"><code># Loop through all missing files, or force load anything less than a week ago

date = datetime.datetime.strptime(user_profile['profile']['firstDate'], '%Y%m%d')
today = datetime.datetime.now()
oneWeekAgo = today - datetime.timedelta(days = 7)

while date &lt; today:
    dir = os.path.join('data', date.strftime('%Y'), date.strftime('%m'))
    filename = os.path.join(dir, date.strftime('%d') + '.json')

    if not date &gt; oneWeekAgo and os.path.exists(filename):
        date += datetime.timedelta(days = 1)
        continue

    if not os.path.exists(dir):
        os.makedirs(dir)

    print(filename)

    response = get('/user/storyline/daily/{date}?trackPoints=true', date = date.strftime('%Y%m%d'))

    if response.status_code != 200:
        print('Bad response, stopping')
        print(response.text)
        sys.exit(0)

    if int(response.headers['x-ratelimit-minuteremaining']) &lt; 1:
        print('Rate limited, waiting one minute before continuing')
        time.sleep(60)

    if int(response.headers['x-ratelimit-hourremaining']) &lt; 1:
        print('Rate limited, wait one hour and try again')
        time.sleep(3600)

    with codecs.open(filename, 'w', 'utf-8') as fout:
        fout.write(response.text)

    date += datetime.timedelta(days = 1)</code></pre>
<p>There is a neat bit in there with the <code>x-ratelimit-minuteremaining</code> and <code>x-ratelimit-hourremaining</code>. If we're downloading the entire history for the first time, you're going to get rate limited. So in this case, we'll wait a minute or an hour until the rate limit has expired.</p>
<p>And that's it. In the end, I end up with a pile of files, one for each day, each with exactly where I was on that day. I can use that data for all sorts of interesting analytics, like how far I walk in the average week, what my area of influence is, or even to combine with my <a href="http://blog.jverkamp.com/category/photography">photography</a> so that I can geotag my pictures. It's a lot of fun.</p>
<p>So, yes. I am something of a digital hoarder. But on the flip side, storage space is cheap and data is interesting. Perhaps I'll get a post or two out of making pretty pretty pictures out of where all I've been!</p>
<p>If you'd like to see / download the entire script for my Moves backup (or any of my other non-iOS backups, those are <a href="http://blog.jverkamp.com/category/programming/by-project/ios-backup">here</a>), you can do so here: <a href="https://github.com/jpverkamp/backup">jpverkamp/backup on GitHub</a></p>]]></content></entry><entry><title>Muir Woods</title><link href="http://blog.jverkamp.com/2015/07/03/muir-woods" /><id>urn:uuid:c635c0f1-c2f0-1d7c-9141-7ae3bdcca075</id><updated>2015-07-03T00:00:00Z</updated><summary type="html"><![CDATA[<p>Finally made it to Muir Woods. A lovely place, even if entirely too busy. Also, my camera's battery ran out, so half of these were actually taken with my phone--which actually works pretty well.</p>
<div><div class="flickr-gallery" data-set-id="72157653090636294" data-per-page="30"></div><p><a href="https://flickr.com/photos/jpverkamp/sets/72157653090636294">View on Flickr</a></p></div>]]></summary><content type="html"><![CDATA[<p>Finally made it to Muir Woods. A lovely place, even if entirely too busy. Also, my camera's battery ran out, so half of these were actually taken with my phone--which actually works pretty well.</p>
<div><div class="flickr-gallery" data-set-id="72157653090636294" data-per-page="30"></div><p><a href="https://flickr.com/photos/jpverkamp/sets/72157653090636294">View on Flickr</a></p></div>]]></content></entry></feed>