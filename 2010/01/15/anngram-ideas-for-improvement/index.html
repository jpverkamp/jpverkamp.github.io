<!doctype html><html><head><title>AnnGram - Ideas for improvement – jverkamp.com</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><link rel=alternate type=application/atom+xml title="jverkamp.com (Atom 2.0)" href=//blog.jverkamp.com/feed/><script src=/jquery_17422284542669262002.min.834c1e2313951f0c25b90152fe8b62250eab4a2e8cbd2560fa1a1cdc91c71733.js integrity="sha256-g0weIxOVHwwluQFS/otiJQ6rSi6MvSVg+hoc3JHHFzM=" defer></script><script src=/jquery.fancybox_16245765822111608191.min.ef050764ff69f3287c89ff655825479dd8304dcd9bc653d1ddd4772a701ad445.js integrity="sha256-7wUHZP9p8yh8if9lWCVHndgwTc2bxlPR3dR3KnAa1EU=" defer></script><script src=/katex_17296078054267651618.min.deed0e78a77391517d530a00324ce7b760ac204134992ef22d36f583615ae498.js integrity="sha256-3u0OeKdzkVF9UwoAMkznt2CsIEE0mS7yLTb1g2Fa5Jg=" defer></script><script src=/auto-render_14944144240389301023.min.e694d9d5eae2917984179683ead27b998784a81398e8836c369373d2c67fc32a.js integrity="sha256-5pTZ1erikXmEF5aD6tJ7mYeEqBOY6INsNpNz0sZ/wyo=" defer></script><script src=/bigfoot_28293813221957978.min.af671f08986f0a2267c5a0cb2748b005489e47fa25f55479b200e2a563d23022.js integrity="sha256-r2cfCJhvCiJnxaDLJ0iwBUieR/ol9VR5sgDipWPSMCI=" defer></script><script src=/mermaid_9520146763733687737.min.bfe50b47c0387e3c3bf97ec5f338bba94f7ccb02f962a4aec8cf0b4d68c8434d.js integrity="sha256-v+ULR8A4fjw7+X7F8zi7qU98ywL5YqSuyM8LTWjIQ00=" defer></script><script src=/main.min.d60298c89fc4f1e938aceb45c60926efee8b03efc8b50683082e669a100da643.js integrity="sha256-1gKYyJ/E8ek4rOtFxgkm7+6LA+/ItQaDCC5mmhANpkM=" defer></script><link rel=stylesheet href=/katex_13658330645258633971.min.64e42891d651aee0b8cd02ec9227ff271d37bcf06dae985d1acf0eba1623e850.css integrity="sha256-ZOQokdZRruC4zQLskif/Jx03vPBtrphdGs8OuhYj6FA="><link rel=stylesheet href=/bigfoot-default_8781527669040159104.min.0d2b289fa3451447692959fcee5676b846532fe7ab50ddc4660824c42d4adcd7.css integrity="sha256-DSson6NFFEdpKVn87lZ2uEZTL+erUN3EZggkxC1K3Nc="><link rel=stylesheet href=/jquery.fancybox_5330465509389191777.min.67505d77381ebd82623ab9296c1989c44ec828d867c00296e80e52ef860cac37.css integrity="sha256-Z1BddzgevYJiOrkpbBmJxE7IKNhnwAKW6A5S74YMrDc="><link rel=stylesheet href=/css_1846377409604050217.min.fffe842bc000dd1fe8661ac6427a392a08faa95e8edab1ea7fb98c5f8dac6a6f.css integrity="sha256-//6EK8AA3R/oZhrGQno5Kgj6qV6O2rHqf7mMX42sam8="><link rel=stylesheet href=/main.min.b60cba31b8c292392a2d336408f8f344e261df78516eb17bcf029173b24a42b5.css integrity="sha256-tgy6MbjCkjkqLTNkCPjzROJh33hRbrF7zwKRc7JKQrU="></head><body><div id=wrapper><header id=page-header role=banner><h1><a href=/>JP's Blog</a></h1><ul id=page-header-links><li><a href=https://github.com/jpverkamp>GitHub</a> *
<a href=https://www.flickr.com/photos/jpverkamp>Flickr</a> *
<a href=/resume>Resume</a></li><li><form action=/search/ method=get class="navbar-form navbar-right" role=search _lpchecked=1><div class=form-group><input name=q type=text class=form-control placeholder=Search>
<button type=submit class="btn btn-default" value=Search>Search</button></div></form></li></ul><nav id=header-navigation role=navigation class=ribbon><ul class=main-navigation><li><a href=https://blog.jverkamp.com/photography/>Photography</a></li><li><a href=https://blog.jverkamp.com/reviews/>Reviews</a></li><li><a href=https://blog.jverkamp.com/programming/>Programming</a></li><li><a href=https://blog.jverkamp.com/home-automation/>Home Automation</a></li><li><a href=https://blog.jverkamp.com/maker/>Maker</a></li><li><a href=https://blog.jverkamp.com/writing/>Writing</a></li><li><a href=https://blog.jverkamp.com/research/>Research</a></li><li><a href=https://blog.jverkamp.com/search/>Search</a></li><li class=subscription data-subscription=rss><a href=/atom.xml rel=subscribe-rss title="subscribe via RSS">RSS</a></li></ul></nav></header><div id=page-content-wrapper><div id=page-content><article data-pagefind-body><header><h1 class=entry-title data-pagefind-meta=title>AnnGram - Ideas for improvement</h1><div class=entry-meta><span class=entry-date>2010-01-15</span></div><div class=entry-taxonomies><div class=entry-tags><ul class=taxonomy-keys><li><a class=taxonomy-key href=/programming/languages/>Languages</a><ul class=taxonomy-values><li><a href=https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/ class=previous-link></a><a class=taxonomy-value href=/programming/languages/.net>.NET</a><a href=https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/ class=next-link></a></li><li><a href=https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/ class=previous-link></a><a class=taxonomy-value href=/programming/languages/c>C#</a><a href=https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/ class=next-link></a></li></ul></li><li><a class=taxonomy-key href=/programming/topics/>Topics</a><ul class=taxonomy-values><li><a href=https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/ class=previous-link></a><a class=taxonomy-value href=/programming/topics/computational-linguistics>Computational Linguistics</a><a href=https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/ class=next-link></a></li><li><a href=https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/ class=previous-link></a><a class=taxonomy-value href=/programming/topics/natural-language-processing>Natural Language Processing</a><a href=https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/ class=next-link></a></li><li><a href=https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/ class=previous-link></a><a class=taxonomy-value href=/programming/topics/neural-networks>Neural Networks</a><a href=https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/ class=next-link></a></li><li><a href=https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/ class=previous-link></a><a class=taxonomy-value href=/programming/topics/research>Research</a><a href=https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/ class=next-link></a></li></ul></li><li><a class=taxonomy-key href=/research>research</a><ul><li><a href=https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/ class=previous-link>Prev</a>
<a href=https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/ class=next-link>Next</a></ul></li><li><a class=taxonomy-key href=/>All Posts</a><ul><li><a href=https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/ class=previous-link>Prev</a>
<a href=https://blog.jverkamp.com/2010/01/15/%E3%83%AD%E3%83%9C%E3%83%83%E3%83%88%E3%81%A8%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD/ class=next-link>Next</a></ul></li></ul></div></div></header><div class=entry-content><p>After my meeting yesterday with my thesis advisers, I have a number of new ideas to try to improve the efficiency of the neural networks.  The most promising of those are described below.</p><p><strong>Sliding window</strong></p><p>The first idea was to replace the idea of applying the most common frequencies directly with a sliding window (almost a directly analogue to the nGrams themselves).  The best way that we could come up to implent this would be to give the neural networks some sort of memory which brought up recurring networks (see below).</p><p><strong><a href=https://en.wikipedia.org/wiki/NETtalk%20%28artificial%20neural%20network%29>NETtalk</a></strong></p><p>From Wikipedia:</p><p><em>NETtalk is perhaps the best known artificial neural network. It is the result of research carried out in the mid 1980s by Terrence Sejnowski and Charles Rosenberg. The intent behind NETtalk was to construct simplified models that might shed light on the complexity of learning human level cognitive tasks, and their implementation as a connectionist model that could also learn to perform a comparable task.</em></p><p>I&rsquo;ve looked into NETtalk and, while it is a really neat concept, I do not know how directly related it is to the project at hand.  Essentially, NETtalk was designed to apply neural networks to mimicking human speech.  It&rsquo;s actually very interesting to listen to the neural network progressing from what is essentially gibberish to intelligible speech.  I&rsquo;ve included a link to the audio below:</p><p>From <a href=http://www.cnl.salk.edu/ParallelNetsPronounce/index.php>Salk Institute</a>:</p><p><a href=http://www.cnl.salk.edu/ParallelNetsPronounce/nettalk.mp3>NETtalk</a></p><p>If you are interested, you also can <a href=http://www.cnl.salk.edu/ParallelNetsPronounce/ParallelNetsPronounce-TJSejnowski.pdf>read the original report</a>.</p><p><strong><a href=https://en.wikipedia.org/wiki/Self-organizing%20map>Self-ordering maps</a></strong></p><p>From Wikipedia:</p><p><em>A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map. Self-organizing maps are different from other artificial neural networks in the sense that they use a neighborhood function to preserve the topological properties of the input space.</em></p><p>Self-ordering maps are interesting because they are already cropping up in more than one of the frameworks that I was looking through.  The basic idea is that you start with a field of essentially random seeming data and apply the neural network to it.  As the neural network learns, the data organizes into cohesive patterns.  I&rsquo;m not exactly sure how the idea could be applied directly, but I did have one idea.  Perhaps the same concept from earlier (the vector of common nGram frequencies) for a great number of documents could be set out in a multi-dimensional space and a self-ordering map could be applied directly.  It&rsquo;s something that I&rsquo;ll have to look into.</p><p><strong><a href=https://en.wikipedia.org/wiki/Recurrent%20neural%20network>Recurring networks</a></strong></p><p>From Wikipedia:</p><p><em>A recurrent neural network (RNN) is a class of neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior.</em></p><p><em>Recurrent neural networks must be approached differently from feedforward neural networks, both when analyzing their behavior and training them. Recurrent neural networks can also behave chaotically. Usually, dynamical systems theory is used to model and analyze them. While a feedforward network propagates data linearly from input to output, recurrent networks (RN) also propagate data from later processing stages to earlier stages.</em></p><p>This seems to be the most useful of all of the links that I&rsquo;ve checked so far.  As mentioned earlier under the idea of a sliding window, neural networks with memory would be a radically different alternative to the most common frequency vectors.  Essentially, the neural network receives input as the document is being scanned and some of the nodes will loop back.  In that way, the network can perceive ordering information as well as frequency information and improve its efficiency.  It&rsquo;s definitely something worth looking into if my current branch of exploration proves not to be optimal.</p><p><a href=http://www.cs.waikato.ac.nz/ml/weka/>** Weka**</a></p><p>Weka is a collection of data mining software written in Java and released under the <a href=http://www.gnu.org/licenses/gpl.html>GPL</a>.  While data mining and document classification are closely related fields, I do not see anything that directly relates to AnnGram here.  Perhaps later in the project, I will take a closer look.
<strong>NETtalk</strong> is perhaps the best known <a href=https://en.wikipedia.org/wiki/artificial%20neural%20network>artificial neural network</a>. It is the result of research carried out in the mid 1980s by <a href=https://en.wikipedia.org/wiki/Terrence%20Sejnowski>Terrence Sejnowski</a> and Charles Rosenberg. The intent behind NETtalk was to construct simplified models that might shed light on the complexity of learning human level cognitive tasks, and their implementation as a connectionist model that could also learn to perform a comparable task.</p></div></article></div></div><footer id=page-footer role=contentinfo><nav id=footer-navigation role=navigation class=ribbon><ul class=main-navigation><li><a href=/archive-by-date/>All posts: By Date</a></li><li><a href=/archive-by-tag/>All posts: By Tag</a></li><li><a href=/atom.xml>RSS: All <sup><svg width="8" height="8" viewBox="0 0 24 24"><path fill="#fff" d="M6.503 20.752C6.503 22.546 5.047 24 3.252 24c-1.796.0-3.252-1.454-3.252-3.248s1.456-3.248 3.252-3.248c1.795.001 3.251 1.454 3.251 3.248zM0 8.18v4.811c6.05.062 10.96 4.966 11.022 11.009h4.817C15.777 15.29 8.721 8.242.0 8.18zm0-3.368C10.58 4.858 19.152 13.406 19.183 24H24c-.03-13.231-10.755-23.954-24-24v4.812z"/></svg></sup></a></li><li><a href=/research/atom.xml>RSS: research<sup><svg width="8" height="8" viewBox="0 0 24 24"><path fill="#fff" d="M6.503 20.752C6.503 22.546 5.047 24 3.252 24c-1.796.0-3.252-1.454-3.252-3.248s1.456-3.248 3.252-3.248c1.795.001 3.251 1.454 3.251 3.248zM0 8.18v4.811c6.05.062 10.96 4.966 11.022 11.009h4.817C15.777 15.29 8.721 8.242.0 8.18zm0-3.368C10.58 4.858 19.152 13.406 19.183 24H24c-.03-13.231-10.755-23.954-24-24v4.812z"/></svg></sup></a></li></ul></nav><div id=page-footer-content data-pagefind-ignore=all><div class=legal><p>All posts unless otherwise mentioned are licensed under
<a rel=license href=//creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US><img alt="Creative Commons License" style=border-width:0 src=//i.creativecommons.org/l/by-nc-sa/3.0/80x15.png></a></p><p>Any source code unless otherwise mentioned is licensed under the <a href=//directory.fsf.org/wiki/License:BSD_3Clause>3 clause BSD license</a></p></div></div></footer></div></body></html>