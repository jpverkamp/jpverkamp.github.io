<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Small Scripts on jverkamp.com</title><link>https://blog.jverkamp.com/programming/sources/small-scripts/</link><description>Recent content in Small Scripts on jverkamp.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://blog.jverkamp.com/programming/sources/small-scripts/atom.xml" rel="self" type="application/rss+xml"/><item><title>Go is faster than Python? (an example parsing huge JSON logs)</title><link>https://blog.jverkamp.com/2022/02/11/go-is-faster-than-python-an-example-parsing-huge-json-logs/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/02/11/go-is-faster-than-python-an-example-parsing-huge-json-logs/</guid><description>&lt;p>Recently at work I came across a problem where I had to go through a year&amp;rsquo;s worth of logs and corelate two different fields across all of our requests. On the good side, we have the logs stored as JSON objects (archived from Datadog which collects them). On the down side&amp;hellip; it&amp;rsquo;s kind of a huge amount of data. Not as much as I&amp;rsquo;ve dealt with at previous jobs/in some academic problems, but we&amp;rsquo;re still talking on the order of terabytes.&lt;/p>
&lt;p>On one hand, write up a quick Python script, fire and forget. It takes maybe ten minutes to write the code and (for this specific example) half an hour to run it on the specific cloud instance the logs lived on. So we&amp;rsquo;ll start with that. But then I got thinking&amp;hellip; Python is supposed to be super slow right? Can I do better?&lt;/p>
&lt;p>(Note: This problem is mostly disk bound. So Python actually for the most part does just fine.)&lt;/p></description></item><item><title>A CLI Tool for Bulk Processing Github Dependabot Alerts (with GraphQL!)</title><link>https://blog.jverkamp.com/2022/02/03/a-cli-tool-for-bulk-processing-github-dependabot-alerts-with-graphql/</link><pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/02/03/a-cli-tool-for-bulk-processing-github-dependabot-alerts-with-graphql/</guid><description>&lt;p>Dependabot is &amp;hellip; &lt;em>somewhat useful&lt;/em>. When it comes to letting you know that there are critical issues in your dependencies that can be fixed simply by upgrading the package (they did all the work for you*). The biggest problem is that it can just be &lt;em>insanely&lt;/em> noisy. In a busy repo with multiple Node.JS codebases (especially), you can get dozens to even hundreds of reports a week. And for each one, you optimally would update the code&amp;hellip; but sometimes it&amp;rsquo;s just not practical. So you have to decide which updates you actually apply.&lt;/p>
&lt;p>So. How do we do it?&lt;/p>
&lt;p>Well the traditional rest based Github APIs don&amp;rsquo;t expose the dependabot data, &lt;em>but&lt;/em> the newer GraphQL one does! I&amp;rsquo;ll admit, I haven&amp;rsquo;t used as much GraphQL as I probably should, it&amp;rsquo;s&amp;hellip; a bit more complicated than REST. But it does expose what I need.&lt;/p></description></item><item><title>A simple Flask Logging/Echo Server</title><link>https://blog.jverkamp.com/2022/02/01/a-simple-flask-logging/echo-server/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/02/01/a-simple-flask-logging/echo-server/</guid><description>&lt;p>A very simple server that can be used to catch all incoming HTTP requests and just echo them back + log their contents. I needed it to test what a webhook actually returned to me, but I&amp;rsquo;m sure that there are a number of other things it could be dropped in for.&lt;/p>
&lt;p>It will take in any GET/POST/PATCH/DELETE HTTP request with any path/params/data (optionally JSON), pack that data into a JSON object, and both log that to a file (with a UUID1 based name) plus return this object to the request.&lt;/p>
&lt;p>Warning: Off hand, there is already a potential security problem in this regarding DoS. It will happily try to log anything you throw at it, no matter how big and will store those in memory first. So long running requests / large requests / many requests will quickly eat up your RAM/disk. So&amp;hellip; don&amp;rsquo;t leave this running unattended? At least not without additional configuration.&lt;/p>
&lt;p>That&amp;rsquo;s it! Hope it&amp;rsquo;s helpful.&lt;/p></description></item><item><title>Pulling more than 5000 logs from datadog</title><link>https://blog.jverkamp.com/2022/01/25/pulling-more-than-5000-logs-from-datadog/</link><pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/01/25/pulling-more-than-5000-logs-from-datadog/</guid><description>&lt;p>&lt;a href="https://www.datadoghq.com/" target="_blank" rel="noopener">Datadog&lt;/a> is pretty awesome. I wish I had it at my previous job, but better late than never. In particular, I&amp;rsquo;ve used it a lot for digging through recent logs to try to construct various events for various (security related) reasons.&lt;/p>
&lt;p>One of the problems I&amp;rsquo;ve come into though is that eventually you&amp;rsquo;re going to hit the limits of what datadog can do. In particular, I was trying to reconstruct user&amp;rsquo;s sessions and then check if they made one specific sequence of calls or another one. So far as I know, that isn&amp;rsquo;t directly possible, so instead, I wanted to download a subset of the datadog logs and process them locally.&lt;/p>
&lt;p>Easy enough, yes? Well: &lt;a href="https://stackoverflow.com/questions/67281698/datadog-export-logs-more-than-5-000" target="_blank" rel="noopener">https://stackoverflow.com/questions/67281698/datadog-export-logs-more-than-5-000&lt;/a>&lt;/p>
&lt;p>Turns out, you just can&amp;rsquo;t export more than 5000 logs directly. &lt;em>But&lt;/em>&amp;hellip; they have an API with pagination!&lt;/p></description></item><item><title>Solving Snakebird</title><link>https://blog.jverkamp.com/2021/08/18/solving-snakebird/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2021/08/18/solving-snakebird/</guid><description>&lt;p>&lt;a href="https://store.steampowered.com/app/357300/Snakebird/" target="_blank" rel="noopener">Snakebird!&lt;/a>&lt;/p>
&lt;figure>&lt;img src="https://blog.jverkamp.com/embeds/2021/snakebird-0.png"/>
&lt;/figure>
&lt;p>A cute little puzzle game, where you move around snake(birds). Move any number of snakes around the level, eating fruit, and getting to the exit. The main gotchas are that you have gravity to content with&amp;ndash;your snake will easily fall off the edge of the world&amp;ndash;and each time you eat a fruit, your snake gets bigger. This can help get longer to get into hard to reach places or it can cause trouble when you trap yourself in corners.&lt;/p>
&lt;p>Let&amp;rsquo;s use the new &lt;a href="2021-08-17-immutable.js-solvers">immutable.js solver&lt;/a> to solve these problems!&lt;/p></description></item><item><title>Immutable.js Solvers</title><link>https://blog.jverkamp.com/2021/08/17/immutable.js-solvers/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2021/08/17/immutable.js-solvers/</guid><description>&lt;p>A bit ago I wrote about writing a &lt;a href="https://blog.jverkamp.com/2021/06/25/a-generic-brute-force-backtracking-solver/">generic brute force solver&lt;/a> (wow, was that really two months ago?). It got &amp;hellip; complicate. Mostly, because every time I wrote a &lt;code>step&lt;/code> function, I had to be careful to undo the same. Wouldn&amp;rsquo;t it be nice if we could just write a step function and get backtracking for &amp;lsquo;free&amp;rsquo;?&lt;/p>
&lt;p>Well, with immutability you can!&lt;/p></description></item></channel></rss>