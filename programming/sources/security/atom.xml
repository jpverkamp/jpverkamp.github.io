<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Security on jverkamp.com</title><link>https://blog.jverkamp.com/programming/sources/security/</link><description>Recent content in Security on jverkamp.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://blog.jverkamp.com/programming/sources/security/atom.xml" rel="self" type="application/rss+xml"/><item><title>A CLI Tool for Bulk Processing Github Dependabot Alerts (with GraphQL!)</title><link>https://blog.jverkamp.com/2022/02/03/a-cli-tool-for-bulk-processing-github-dependabot-alerts-with-graphql/</link><pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/02/03/a-cli-tool-for-bulk-processing-github-dependabot-alerts-with-graphql/</guid><description>&lt;p>Dependabot is &amp;hellip; &lt;em>somewhat useful&lt;/em>. When it comes to letting you know that there are critical issues in your dependencies that can be fixed simply by upgrading the package (they did all the work for you*). The biggest problem is that it can just be &lt;em>insanely&lt;/em> noisy. In a busy repo with multiple Node.JS codebases (especially), you can get dozens to even hundreds of reports a week. And for each one, you optimally would update the code&amp;hellip; but sometimes it&amp;rsquo;s just not practical. So you have to decide which updates you actually apply.&lt;/p>
&lt;p>So. How do we do it?&lt;/p>
&lt;p>Well the traditional rest based Github APIs don&amp;rsquo;t expose the dependabot data, &lt;em>but&lt;/em> the newer GraphQL one does! I&amp;rsquo;ll admit, I haven&amp;rsquo;t used as much GraphQL as I probably should, it&amp;rsquo;s&amp;hellip; a bit more complicated than REST. But it does expose what I need.&lt;/p></description></item><item><title>A simple Flask Logging/Echo Server</title><link>https://blog.jverkamp.com/2022/02/01/a-simple-flask-logging/echo-server/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/02/01/a-simple-flask-logging/echo-server/</guid><description>&lt;p>A very simple server that can be used to catch all incoming HTTP requests and just echo them back + log their contents. I needed it to test what a webhook actually returned to me, but I&amp;rsquo;m sure that there are a number of other things it could be dropped in for.&lt;/p>
&lt;p>It will take in any GET/POST/PATCH/DELETE HTTP request with any path/params/data (optionally JSON), pack that data into a JSON object, and both log that to a file (with a UUID1 based name) plus return this object to the request.&lt;/p>
&lt;p>Warning: Off hand, there is already a potential security problem in this regarding DoS. It will happily try to log anything you throw at it, no matter how big and will store those in memory first. So long running requests / large requests / many requests will quickly eat up your RAM/disk. So&amp;hellip; don&amp;rsquo;t leave this running unattended? At least not without additional configuration.&lt;/p>
&lt;p>That&amp;rsquo;s it! Hope it&amp;rsquo;s helpful.&lt;/p></description></item><item><title>Pulling more than 5000 logs from datadog</title><link>https://blog.jverkamp.com/2022/01/25/pulling-more-than-5000-logs-from-datadog/</link><pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/01/25/pulling-more-than-5000-logs-from-datadog/</guid><description>&lt;p>&lt;a href="https://www.datadoghq.com/">Datadog&lt;/a> is pretty awesome. I wish I had it at my previous job, but better late than never. In particular, I&amp;rsquo;ve used it a lot for digging through recent logs to try to construct various events for various (security related) reasons.&lt;/p>
&lt;p>One of the problems I&amp;rsquo;ve come into though is that eventually you&amp;rsquo;re going to hit the limits of what datadog can do. In particular, I was trying to reconstruct user&amp;rsquo;s sessions and then check if they made one specific sequence of calls or another one. So far as I know, that isn&amp;rsquo;t directly possible, so instead, I wanted to download a subset of the datadog logs and process them locally.&lt;/p>
&lt;p>Easy enough, yes? Well: &lt;a href="https://stackoverflow.com/questions/67281698/datadog-export-logs-more-than-5-000">https://stackoverflow.com/questions/67281698/datadog-export-logs-more-than-5-000&lt;/a>&lt;/p>
&lt;p>Turns out, you just can&amp;rsquo;t export more than 5000 logs directly. &lt;em>But&lt;/em>&amp;hellip; they have an API with pagination!&lt;/p></description></item></channel></rss>