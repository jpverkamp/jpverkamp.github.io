<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Research on jverkamp.com</title><link>https://blog.jverkamp.com/programming/topics/research/</link><description>Recent content in Research on jverkamp.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://blog.jverkamp.com/programming/topics/research/atom.xml" rel="self" type="application/rss+xml"/><item><title>Authorship attribution: Part 3</title><link>https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/</link><pubDate>Tue, 06 Aug 2013 14:00:53 +0000</pubDate><guid>https://blog.jverkamp.com/2013/08/06/authorship-attribution-part-3/</guid><description>&lt;p>So far, we&amp;rsquo;ve had three different ideas for figuring out the author of an unknown paper (top n word ordering in &lt;a href="https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/">Part 1&lt;/a> and stop word frequency / 4-grams in &lt;a href="https://blog.jverkamp.com/2013/08/02/authorship-attribution-part-2/">Part 2&lt;/a>). Here&amp;rsquo;s something interesting though from the comments on the &lt;a title="JK Rowling" href="http://programmingpraxis.com/2013/07/19/j-k-rowling/">Programming Praxis post&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>&lt;cite>Globules&lt;/cite> said
&lt;small>July 19, 2013 at 12:29 PM&lt;/small>
Patrick Juola has a &lt;a href="http://languagelog.ldc.upenn.edu/nll/?p=5315" rel="nofollow">guest post on Language Log&lt;/a> describing the approach he took.&lt;/p>
&lt;/blockquote></description></item><item><title>Authorship attribution: Part 2</title><link>https://blog.jverkamp.com/2013/08/02/authorship-attribution-part-2/</link><pubDate>Fri, 02 Aug 2013 14:00:52 +0000</pubDate><guid>https://blog.jverkamp.com/2013/08/02/authorship-attribution-part-2/</guid><description>&lt;p>&lt;a href="https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/">Last time&lt;/a>, we used word rank to try to figure out who could possibly have written Cuckoo&amp;rsquo;s calling. It didn&amp;rsquo;t work out so well, but we at least have a nice framework in place. So perhaps we can try a few more ways of turning entire novels into a few numbers.&lt;/p></description></item><item><title>Authorship attribution: Part 1</title><link>https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/</link><pubDate>Tue, 30 Jul 2013 14:00:55 +0000</pubDate><guid>https://blog.jverkamp.com/2013/07/30/authorship-attribution-part-1/</guid><description>&lt;p>About two weeks ago, the new crime fiction novel &lt;a href="http://www.amazon.com/gp/product/0316206849/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=1789&amp;amp;creative=390957&amp;amp;creativeASIN=0316206849&amp;amp;linkCode=as2&amp;amp;tag=jverkampcom-20">Cuckoo&amp;rsquo;s Calling&lt;/a> was revealed to have actually been written by J.K. Rowling under the pseudonym Robert Galbraith. What&amp;rsquo;s interesting is exactly how they came to that conclusion. Here&amp;rsquo;s a quote from &lt;a title="J.K. Rowling’s Secret: A Forensic Linguist Explains How He Figured It Out" href="http://entertainment.time.com/2013/07/15/j-k-rowlings-secret-a-forensic-linguist-explains-how-he-figured-it-out/">Time&lt;/a> magazine (via &lt;a title="J K Rowling" href="http://programmingpraxis.com/2013/07/19/j-k-rowling/">Programming Praxis&lt;/a>):&lt;/p></description></item><item><title>FLAIRS 2010 - Augmenting n-gram Based Authorship Attribution With Neural Networks</title><link>https://blog.jverkamp.com/2010/05/21/flairs-2010-augmenting-n-gram-based-authorship-attribution-with-neural-networks/</link><pubDate>Fri, 21 May 2010 14:00:23 +0000</pubDate><guid>https://blog.jverkamp.com/2010/05/21/flairs-2010-augmenting-n-gram-based-authorship-attribution-with-neural-networks/</guid><description>Co-authors: Michael Wollowski, and Maki Hirotani
Abstract: While using statistical methods to determine authorship attribution is not a new idea and neural networks have been applied to a number of statistical problems, the two have not often been used together. We show that the use of articial neural networks, specically self-organizing maps, combined with n-grams provides a success rate on the order of previous work with purely statistical methods. Using a collection of documents including the works of Shakespeare, William Blake, and the King James Version of the Bible, we were able to demonstrate classication of documents into individual groups.</description></item><item><title>AnnGram - nGrams vs Words</title><link>https://blog.jverkamp.com/2010/03/17/anngram-ngrams-vs-words/</link><pubDate>Wed, 17 Mar 2010 05:05:37 +0000</pubDate><guid>https://blog.jverkamp.com/2010/03/17/anngram-ngrams-vs-words/</guid><description>&lt;p>&lt;strong>Overview&lt;/strong>&lt;/p>
&lt;p>For another comparison, I&amp;rsquo;ve been looking for a way to replace the nGrams with another way of turning a document into a vector.  Based on word frequency instead of nGrams, I&amp;rsquo;ve run a number of tests to see how the accuracy and speed of the algorithm compares for the two.&lt;/p>
&lt;p>&lt;strong>nGrams&lt;/strong>&lt;/p>
&lt;figure>&lt;img src="https://blog.jverkamp.com/embeds/2010/SOM-ngram.png"/>
&lt;/figure>
&lt;p>I still intend to look into why the Tragedy of Macbeth does not stay with the rest of Shakespeare&amp;rsquo;s plays.  I still believe that it is because portions of it were possible written by another author.&lt;/p></description></item><item><title>AnnGram vs k-means</title><link>https://blog.jverkamp.com/2010/02/10/anngram-vs-k-means/</link><pubDate>Wed, 10 Feb 2010 04:05:32 +0000</pubDate><guid>https://blog.jverkamp.com/2010/02/10/anngram-vs-k-means/</guid><description>Overview
As a set of benchmarks to test whether or not the new AnnGram algorithm is actually working correctly, I&amp;rsquo;ve been trying to come up with different yet similar methods to compare it too. Primarily, there are two possibilities:
Replace the nGram vectors with another form Process the nGrams using something other than Self-Organizing Maps I&amp;rsquo;m still looking through the related literature to decide if there is some way to use something other than the nGrams to feed into the SOM; however, I haven&amp;rsquo;t been having any luck.</description></item><item><title>AnnGram - Self-Organizing Map GUI</title><link>https://blog.jverkamp.com/2010/02/03/anngram-self-organizing-map-gui/</link><pubDate>Wed, 03 Feb 2010 05:05:20 +0000</pubDate><guid>https://blog.jverkamp.com/2010/02/03/anngram-self-organizing-map-gui/</guid><description>&lt;p>They say a picture is worth a thousand words:&lt;/p>
&lt;p>&lt;strong>One Thousand Words&lt;/strong>&lt;/p>
&lt;figure>&lt;img src="https://blog.jverkamp.com/embeds/2010/som-results-11.png"/>
&lt;/figure></description></item><item><title>AnnGram - New GUI</title><link>https://blog.jverkamp.com/2010/01/28/anngram-new-gui/</link><pubDate>Thu, 28 Jan 2010 05:05:36 +0000</pubDate><guid>https://blog.jverkamp.com/2010/01/28/anngram-new-gui/</guid><description>&lt;p>The old GUI framework just wasn&amp;rsquo;t working out (so far as adding new features went).  So, long story short, I&amp;rsquo;ve switched GUI layout.&lt;/p></description></item><item><title>AnnGram - Neural Network Progress</title><link>https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/</link><pubDate>Thu, 21 Jan 2010 05:05:19 +0000</pubDate><guid>https://blog.jverkamp.com/2010/01/21/anngram-neural-network-progress/</guid><description>&lt;p>&lt;a href="https://blog.jverkamp.com/2010/01/12/anngram-neuralnetwork-library/">As expected&lt;/a>, I&amp;rsquo;ve decided to change libraries from the** **&lt;a href="https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/">poor results with the original tests&lt;/a> may have been a direct results of a misunderstanding with the code base.  I think that the layers were not being hooked up correctly, resulting in low/random values.&lt;/p></description></item><item><title>AnnGram - Ideas for improvement</title><link>https://blog.jverkamp.com/2010/01/15/anngram-ideas-for-improvement/</link><pubDate>Fri, 15 Jan 2010 05:05:30 +0000</pubDate><guid>https://blog.jverkamp.com/2010/01/15/anngram-ideas-for-improvement/</guid><description>&lt;p>After my meeting yesterday with my thesis advisers, I have a number of new ideas to try to improve the efficiency of the neural networks.  The most promising of those are described below.&lt;/p>
&lt;p>&lt;strong>Sliding window&lt;/strong>&lt;/p>
&lt;p>The first idea was to replace the idea of applying the most common frequencies directly with a sliding window (almost a directly analogue to the nGrams themselves).  The best way that we could come up to implent this would be to give the neural networks some sort of memory which brought up recurring networks (see below).&lt;/p></description></item><item><title>AnnGram - Initial ANN Results</title><link>https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/</link><pubDate>Wed, 13 Jan 2010 05:05:14 +0000</pubDate><guid>https://blog.jverkamp.com/2010/01/13/anngram-initial-ann-results/</guid><description>&lt;p>&lt;strong>Overview&lt;/strong>&lt;/p>
&lt;p>For now, I&amp;rsquo;ve chosen to work with &lt;a href="http://franck.fleurey.free.fr/NeuralNetwork/">C# Neural network&lt;/a> library.  It was the easiest to get off the ground and running, so it seemed like a good place to start.&lt;/p></description></item><item><title>AnnGram - NeuralNetwork Library</title><link>https://blog.jverkamp.com/2010/01/12/anngram-neuralnetwork-library/</link><pubDate>Tue, 12 Jan 2010 05:05:54 +0000</pubDate><guid>https://blog.jverkamp.com/2010/01/12/anngram-neuralnetwork-library/</guid><description>&lt;p>I&amp;rsquo;ve been looking for a good Neural Network library to use with the AnnGram project and so far I&amp;rsquo;ve come across a couple of possibilities:&lt;/p>
&lt;p>&lt;a href="http://franck.fleurey.free.fr/NeuralNetwork/">&lt;strong>C# Neural network library&lt;/strong>&lt;/a>&lt;/p>
&lt;p>The top link on Google was an aptly named C# Neural network library.  Overall, it looks clean and easy to use and is licensed under the &lt;a href="http://www.gnu.org/licenses/gpl.html">GPL&lt;/a>, so should work well for my needs.   The framework has two types of training methods: genetic algorithms and backward propagation.  In addition, there are at least three different activation functions included: linear, signmoid, and heaviside functions.  The main problem with this framework is the spare documentation.  The only that I&amp;rsquo;ve been able to find so far is a generated API reference and a few examples (using their included GUI framework).&lt;/p></description></item><item><title>AnnGram - Initial GUI</title><link>https://blog.jverkamp.com/2010/01/05/anngram-initial-gui/</link><pubDate>Tue, 05 Jan 2010 05:05:55 +0000</pubDate><guid>https://blog.jverkamp.com/2010/01/05/anngram-initial-gui/</guid><description>&lt;p>&lt;strong>Overview&lt;/strong>&lt;/p>
&lt;p>Basically, I got tired of modifying the command line every time I wanted to test new values.  To that end, I spent a small bit of time coding up a GUI to make further experiments easier.&lt;/p></description></item><item><title>AnnGram - Cosine Distance</title><link>https://blog.jverkamp.com/2010/01/01/anngram-cosine-distance/</link><pubDate>Fri, 01 Jan 2010 05:05:00 +0000</pubDate><guid>https://blog.jverkamp.com/2010/01/01/anngram-cosine-distance/</guid><description>&lt;p>&lt;strong>Overview&lt;/strong>&lt;/p>
&lt;p>The first algorithm that I&amp;rsquo;ve chosen to implement is a simple cosine difference between the n-gram vectors.  This was the first method used in multiple of the papers that I&amp;rsquo;ve read and it seems like a good benchmark.&lt;/p>
&lt;p>Essentially, this method gives the similarity of two n-gram documents (either Documents or Authors) as an angle ranging from 0 (identical documents) to &lt;span class="latex-inline">\pi/2&lt;/span>
(completely different documents).  Documents written by the same author should have the lowest values.&lt;/p></description></item><item><title>AnnGram - Framework</title><link>https://blog.jverkamp.com/2009/12/29/anngram-framework/</link><pubDate>Tue, 29 Dec 2009 05:05:28 +0000</pubDate><guid>https://blog.jverkamp.com/2009/12/29/anngram-framework/</guid><description>&lt;p>&lt;strong>Document Framework&lt;/strong>&lt;/p>
&lt;p>The first portion of the framework that it was necessary to code was the ability to load documents.  To reduce the load on the processor when first loading the document, only a minimal amount of computation is done.  Further computation is pushed off until necessary.&lt;/p>
&lt;p>To avoid duplicating work, the n-grams are stored using &lt;a href="https://en.wikipedia.org/wiki/memoization">memoization&lt;/a>.  The basic idea is that when a function (in this case, a particular length of n-gram) is first requested, the calculation is done and the result is stored in memory.  During any future calls, the cached result is directly returned, greatly increasing speed at the cost of memory.  Luckily, modern computers have more than sufficient memory for the task at hand.&lt;/p></description></item><item><title>AnnGram - Overview</title><link>https://blog.jverkamp.com/2009/12/21/anngram-overview/</link><pubDate>Mon, 21 Dec 2009 05:05:34 +0000</pubDate><guid>https://blog.jverkamp.com/2009/12/21/anngram-overview/</guid><description>&lt;p>&lt;strong>Basic Premise&lt;/strong>&lt;/p>
&lt;p>For my senior thesis at Rose-Hulman Institute of Technology, I am attempting to combine the fields of Computational Linguistics and Artificial Intelligence in a new and useful manner.  Specifically, I am planning on making use of Artificial Neural Networks to enhance the performance of n-gram based document classification.  Over the next few months, I will be updating this category with background and information and further progress.&lt;/p>
&lt;p>First, I&amp;rsquo;ll start with some basic background information.&lt;/p></description></item></channel></rss>