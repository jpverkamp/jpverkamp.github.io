<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>JSON on jverkamp.com</title><link>https://blog.jverkamp.com/programming/topics/json/</link><description>Recent content in JSON on jverkamp.com</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 16 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.jverkamp.com/programming/topics/json/atom.xml" rel="self" type="application/rss+xml"/><item><title>API Tricks: Wikipedia Table JSON API</title><link>https://blog.jverkamp.com/2025/08/16/api-tricks-wikipedia-table-json-api/</link><pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2025/08/16/api-tricks-wikipedia-table-json-api/</guid><description>&lt;p>Quick, what is the order of the (as of now) 63 released Walk Disney Animation Studios films?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ api&lt;span style="color:#f92672">=&lt;/span>https://www.wikitable2json.com/api; &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> page&lt;span style="color:#f92672">=&lt;/span>List_of_Walt_Disney_Animation_Studios_films; &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> curl -s &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$api&lt;span style="color:#e6db74">/&lt;/span>$page&lt;span style="color:#e6db74">?table=0&amp;amp;keyRows=1&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> | jq &lt;span style="color:#e6db74">&amp;#39;.[0][].Film&amp;#39;&lt;/span> -rc &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> | egrep -v &lt;span style="color:#e6db74">&amp;#39;^as&amp;#39;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> | nl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 1	Snow White and the Seven Dwarfs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 2	Pinocchio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 3	Fantasia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 61	Strange World
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 62	Wish
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 63	Moana &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is a list on Wikipedia: &lt;a href="https://en.wikipedia.org/wiki/List%20of%20Walt%20Disney%20Animation%20Studios%20films">List of Walt Disney Animation Studios films&lt;/a>, but the tables there are &amp;hellip; a bit of a pain to copy paste. I could very well just manually do that, but where&amp;rsquo;s the fun in that?&lt;/p>
&lt;p>Luckily, &lt;a href="https://github.com/atye/wikitable2json/" target="_blank" rel="noopener">someone&lt;/a> went through the work of providing a wrapper around Wikipedia that will extract all (or selected) tables from a Wikipedia page!&lt;/p>
&lt;p>To break down the command:&lt;/p>
&lt;ul>
&lt;li>&lt;code>curl -s https://{...}?table=0&amp;amp;keyRows&lt;/code> - Download the first (&lt;a href="https://en.wikipedia.org/wiki/zero%20based%20indexing">zero based indexing&lt;/a>) table on the page; use the first row as column names (&lt;code>-s&lt;/code> for &amp;lsquo;silent&amp;rsquo; mode)&lt;/li>
&lt;li>&lt;code>jq '.[0][].Film&lt;/code> - Extract the first table in the response (&lt;code>.[0]&lt;/code>), for each row in that table &lt;code>[]&lt;/code> extract the film name &lt;code>.Film&lt;/code>&lt;/li>
&lt;li>&lt;code>egrep -v '^as&lt;/code> - Remove rows starting with &amp;lsquo;as &amp;hellip;&amp;rsquo;; these are extra rows when the studio was renamed&lt;/li>
&lt;/ul>
&lt;p>And that&amp;rsquo;s it!&lt;/p></description></item><item><title>Go is faster than Python? (an example parsing huge JSON logs)</title><link>https://blog.jverkamp.com/2022/02/11/go-is-faster-than-python-an-example-parsing-huge-json-logs/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/02/11/go-is-faster-than-python-an-example-parsing-huge-json-logs/</guid><description>&lt;p>Recently at work I came across a problem where I had to go through a year&amp;rsquo;s worth of logs and corelate two different fields across all of our requests. On the good side, we have the logs stored as JSON objects (archived from Datadog which collects them). On the down side&amp;hellip; it&amp;rsquo;s kind of a huge amount of data. Not as much as I&amp;rsquo;ve dealt with at previous jobs/in some academic problems, but we&amp;rsquo;re still talking on the order of terabytes.&lt;/p>
&lt;p>On one hand, write up a quick Python script, fire and forget. It takes maybe ten minutes to write the code and (for this specific example) half an hour to run it on the specific cloud instance the logs lived on. So we&amp;rsquo;ll start with that. But then I got thinking&amp;hellip; Python is supposed to be super slow right? Can I do better?&lt;/p>
&lt;p>(Note: This problem is mostly disk bound. So Python actually for the most part does just fine.)&lt;/p></description></item><item><title>A simple Flask Logging/Echo Server</title><link>https://blog.jverkamp.com/2022/02/01/a-simple-flask-logging/echo-server/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.jverkamp.com/2022/02/01/a-simple-flask-logging/echo-server/</guid><description>&lt;p>A very simple server that can be used to catch all incoming HTTP requests and just echo them back + log their contents. I needed it to test what a webhook actually returned to me, but I&amp;rsquo;m sure that there are a number of other things it could be dropped in for.&lt;/p>
&lt;p>It will take in any GET/POST/PATCH/DELETE HTTP request with any path/params/data (optionally JSON), pack that data into a JSON object, and both log that to a file (with a UUID1 based name) plus return this object to the request.&lt;/p>
&lt;p>Warning: Off hand, there is already a potential security problem in this regarding DoS. It will happily try to log anything you throw at it, no matter how big and will store those in memory first. So long running requests / large requests / many requests will quickly eat up your RAM/disk. So&amp;hellip; don&amp;rsquo;t leave this running unattended? At least not without additional configuration.&lt;/p>
&lt;p>That&amp;rsquo;s it! Hope it&amp;rsquo;s helpful.&lt;/p></description></item><item><title>CSV to JSON</title><link>https://blog.jverkamp.com/2015/12/11/csv-to-json/</link><pubDate>Fri, 11 Dec 2015 00:05:00 +0000</pubDate><guid>https://blog.jverkamp.com/2015/12/11/csv-to-json/</guid><description>&lt;p>Today at work, I had to process a bunch of CSV data. Realizing that I don&amp;rsquo;t have any particularly nice tools to work with streaming CSV data (although I did write about &lt;a href="https://blog.jverkamp.com/2012/10/04/querying-csv-files-with-sql/">querying CSV files with SQL&lt;/a>), I decided to write one:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ cat users.csv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#34;user_id&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;email&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;password&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;Luke Skywalker&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;luke@rebel-alliance.io&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$2&lt;span style="color:#e6db74">b&lt;/span>$12$XQ1zDvl5PLS6g&lt;span style="color:#e6db74">.K64H27xewPQMnkELa3LvzFSyay8p9kz0XXHVOFq&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;Han Solo&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;han@rebel-alliance.io&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$2&lt;span style="color:#e6db74">b&lt;/span>$12$eKJGP&lt;span style="color:#e6db74">.tt9u77PeXgMMFmlOyFWSuRZBUZLvmzuLlrum3vWPoRYgr92&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cat users.csv | csv2json | jq &lt;span style="color:#e6db74">&amp;#39;.&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;password&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$2&lt;span style="color:#e6db74">b&lt;/span>$12$XQ1zDvl5PLS6g&lt;span style="color:#e6db74">.K64H27xewPQMnkELa3LvzFSyay8p9kz0XXHVOFq&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Luke Skywalker&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;user_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;email&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;luke@rebel-alliance.io&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;password&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$2&lt;span style="color:#e6db74">b&lt;/span>$12$eKJGP&lt;span style="color:#e6db74">.tt9u77PeXgMMFmlOyFWSuRZBUZLvmzuLlrum3vWPoRYgr92&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Han Solo&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;user_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;email&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;han@rebel-alliance.io&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Decoding escaped Unicode strings</title><link>https://blog.jverkamp.com/2013/01/17/decoding-escaped-unicode-strings/</link><pubDate>Thu, 17 Jan 2013 14:00:56 +0000</pubDate><guid>https://blog.jverkamp.com/2013/01/17/decoding-escaped-unicode-strings/</guid><description>&lt;p>In one of my current research projects involving large amounts of &lt;a href="https://twitter.com/" title="Twitter">Twitter&lt;/a> data from a variety of countries, I came across an interesting problem. The Twitter stream is encoded as a series of &lt;a href="https://en.wikipedia.org/wiki/JSON">JSON&lt;/a> objects&amp;ndash;each of which has been written out using &lt;a href="https://en.wikipedia.org/wiki/ASCII">ASCII&lt;/a> characters. But not all of the Tweets (or even a majority in this case) can be represented with only ASCII. So what happens?&lt;/p>
&lt;p>Well, it turns out that they encode the data as JSON strings with &lt;a href="https://en.wikipedia.org/wiki/Unicode">Unicode&lt;/a> &lt;a href="https://en.wikipedia.org/wiki/escape%20characters">escape characters&lt;/a>. So if we had the Russian hashtag #победазанами (victory is ours), that would be encoded as such:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#34;#&lt;/span>&lt;span style="color:#ae81ff">\u043f\u043e\u0431\u0435\u0434\u0430\u0437\u0430\u043d\u0430\u043c\u0438&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item></channel></rss>